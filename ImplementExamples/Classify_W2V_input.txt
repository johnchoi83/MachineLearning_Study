[{"abstract": "This report summarizes the tutorial presented by the author at NIPS 2016 on\ngenerative adversarial networks (GANs). The tutorial describes: (1) Why\ngenerative modeling is a topic worth studying, (2) how generative models work,\nand how GANs compare to other generative models, (3) the details of how GANs\nwork, (4) research frontiers in GANs, and (5) state-of-the-art image models\nthat combine GANs with other methods. Finally, the tutorial contains three\nexercises for readers to complete, and the solutions to these exercises.", "authors": ["Ian Goodfellow"], "category": "cs.LG", "comment": "v2-v4 are all typo fixes. No substantive changes relative to v1", "img": "/static/thumbs/1701.00160v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.00160v4", "num_discussion": 0, "originally_published_time": "12/31/2016", "pid": "1701.00160v4", "published_time": "4/3/2017", "rawpid": "1701.00160", "tags": ["cs.LG"], "title": "NIPS 2016 Tutorial: Generative Adversarial Networks"}, {"abstract": "The dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks in an encoder-decoder configuration. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer, based\nsolely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to be\nsuperior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\nEnglish-to-German translation task, improving over the existing best results,\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\ntranslation task, our model establishes a new single-model state-of-the-art\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\nof the training costs of the best models from the literature. We show that the\nTransformer generalizes well to other tasks by applying it successfully to\nEnglish constituency parsing both with large and limited training data.", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "category": "cs.CL", "comment": "15 pages, 5 figures", "img": "/static/thumbs/1706.03762v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.03762v5", "num_discussion": 2, "originally_published_time": "6/12/2017", "pid": "1706.03762v5", "published_time": "12/6/2017", "rawpid": "1706.03762", "tags": ["cs.CL", "cs.LG"], "title": "Attention Is All You Need"}, {"abstract": "We present a conceptually simple, flexible, and general framework for object\ninstance segmentation. Our approach efficiently detects objects in an image\nwhile simultaneously generating a high-quality segmentation mask for each\ninstance. The method, called Mask R-CNN, extends Faster R-CNN by adding a\nbranch for predicting an object mask in parallel with the existing branch for\nbounding box recognition. Mask R-CNN is simple to train and adds only a small\noverhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to\ngeneralize to other tasks, e.g., allowing us to estimate human poses in the\nsame framework. We show top results in all three tracks of the COCO suite of\nchallenges, including instance segmentation, bounding-box object detection, and\nperson keypoint detection. Without bells and whistles, Mask R-CNN outperforms\nall existing, single-model entries on every task, including the COCO 2016\nchallenge winners. We hope our simple and effective approach will serve as a\nsolid baseline and help ease future research in instance-level recognition.\nCode has been made available at: https://github.com/facebookresearch/Detectron", "authors": ["Kaiming He", "Georgia Gkioxari", "Piotr Doll\u00e1r", "Ross Girshick"], "category": "cs.CV", "comment": "open source; appendix on more results", "img": "/static/thumbs/1703.06870v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.06870v3", "num_discussion": 2, "originally_published_time": "3/20/2017", "pid": "1703.06870v3", "published_time": "1/24/2018", "rawpid": "1703.06870", "tags": ["cs.CV"], "title": "Mask R-CNN"}, {"abstract": "A capsule is a group of neurons whose activity vector represents the\ninstantiation parameters of a specific type of entity such as an object or an\nobject part. We use the length of the activity vector to represent the\nprobability that the entity exists and its orientation to represent the\ninstantiation parameters. Active capsules at one level make predictions, via\ntransformation matrices, for the instantiation parameters of higher-level\ncapsules. When multiple predictions agree, a higher level capsule becomes\nactive. We show that a discrimininatively trained, multi-layer capsule system\nachieves state-of-the-art performance on MNIST and is considerably better than\na convolutional net at recognizing highly overlapping digits. To achieve these\nresults we use an iterative routing-by-agreement mechanism: A lower-level\ncapsule prefers to send its output to higher level capsules whose activity\nvectors have a big scalar product with the prediction coming from the\nlower-level capsule.", "authors": ["Sara Sabour", "Nicholas Frosst", "Geoffrey E Hinton"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1710.09829v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.09829v2", "num_discussion": 0, "originally_published_time": "10/26/2017", "pid": "1710.09829v2", "published_time": "11/7/2017", "rawpid": "1710.09829", "tags": ["cs.CV"], "title": "Dynamic Routing Between Capsules"}, {"abstract": "Very deep convolutional networks have been central to the largest advances in\nimage recognition performance in recent years. One example is the Inception\narchitecture that has been shown to achieve very good performance at relatively\nlow computational cost. Recently, the introduction of residual connections in\nconjunction with a more traditional architecture has yielded state-of-the-art\nperformance in the 2015 ILSVRC challenge; its performance was similar to the\nlatest generation Inception-v3 network. This raises the question of whether\nthere are any benefit in combining the Inception architecture with residual\nconnections. Here we give clear empirical evidence that training with residual\nconnections accelerates the training of Inception networks significantly. There\nis also some evidence of residual Inception networks outperforming similarly\nexpensive Inception networks without residual connections by a thin margin. We\nalso present several new streamlined architectures for both residual and\nnon-residual Inception networks. These variations improve the single-frame\nrecognition performance on the ILSVRC 2012 classification task significantly.\nWe further demonstrate how proper activation scaling stabilizes the training of\nvery wide residual Inception networks. With an ensemble of three residual and\none Inception-v4, we achieve 3.08 percent top-5 error on the test set of the\nImageNet classification (CLS) challenge", "authors": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alex Alemi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1602.07261v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.07261v2", "num_discussion": 0, "originally_published_time": "2/23/2016", "pid": "1602.07261v2", "published_time": "8/23/2016", "rawpid": "1602.07261", "tags": ["cs.CV"], "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on\n  Learning"}, {"abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC \u0026 COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "category": "cs.CV", "comment": "Tech report", "img": "/static/thumbs/1512.03385v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1512.03385v1", "num_discussion": 0, "originally_published_time": "12/10/2015", "pid": "1512.03385v1", "published_time": "12/10/2015", "rawpid": "1512.03385", "tags": ["cs.CV"], "title": "Deep Residual Learning for Image Recognition"}, {"abstract": "We give an overview of recent exciting achievements of deep reinforcement\nlearning (RL). We discuss six core elements, six important mechanisms, and\ntwelve applications. We start with background of machine learning, deep\nlearning and reinforcement learning. Next we discuss core RL elements,\nincluding value function, in particular, Deep Q-Network (DQN), policy, reward,\nmodel, planning, and exploration. After that, we discuss important mechanisms\nfor RL, including attention and memory, unsupervised learning, transfer\nlearning, multi-agent RL, hierarchical RL, and learning to learn. Then we\ndiscuss various applications of RL, including games, in particular, AlphaGo,\nrobotics, natural language processing, including dialogue systems, machine\ntranslation, and text generation, computer vision, neural architecture design,\nbusiness management, finance, healthcare, Industry 4.0, smart grid, intelligent\ntransportation systems, and computer systems. We mention topics not reviewed\nyet, and list a collection of RL resources. After presenting a brief summary,\nwe close with discussions.", "authors": ["Yuxi Li"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1701.07274v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.07274v5", "num_discussion": 0, "originally_published_time": "1/25/2017", "pid": "1701.07274v5", "published_time": "9/15/2017", "rawpid": "1701.07274", "tags": ["cs.LG"], "title": "Deep Reinforcement Learning: An Overview"}, {"abstract": "Deep Learning has revolutionized vision via convolutional neural networks\n(CNNs) and natural language processing via recurrent neural networks (RNNs).\nHowever, success stories of Deep Learning with standard feed-forward neural\nnetworks (FNNs) are rare. FNNs that perform well are typically shallow and,\ntherefore cannot exploit many levels of abstract representations. We introduce\nself-normalizing neural networks (SNNs) to enable high-level abstract\nrepresentations. While batch normalization requires explicit normalization,\nneuron activations of SNNs automatically converge towards zero mean and unit\nvariance. The activation function of SNNs are \"scaled exponential linear units\"\n(SELUs), which induce self-normalizing properties. Using the Banach fixed-point\ntheorem, we prove that activations close to zero mean and unit variance that\nare propagated through many network layers will converge towards zero mean and\nunit variance -- even under the presence of noise and perturbations. This\nconvergence property of SNNs allows to (1) train deep networks with many\nlayers, (2) employ strong regularization, and (3) to make learning highly\nrobust. Furthermore, for activations not close to unit variance, we prove an\nupper and lower bound on the variance, thus, vanishing and exploding gradients\nare impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning\nrepository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with\nstandard FNNs and other machine learning methods such as random forests and\nsupport vector machines. SNNs significantly outperformed all competing FNN\nmethods at 121 UCI tasks, outperformed all competing methods at the Tox21\ndataset, and set a new record at an astronomy data set. The winning SNN\narchitectures are often very deep. Implementations are available at:\ngithub.com/bioinf-jku/SNNs.", "authors": ["G\u00fcnter Klambauer", "Thomas Unterthiner", "Andreas Mayr", "Sepp Hochreiter"], "category": "cs.LG", "comment": "9 pages (+ 93 pages appendix)", "img": "/static/thumbs/1706.02515v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.02515v5", "num_discussion": 0, "originally_published_time": "6/8/2017", "pid": "1706.02515v5", "published_time": "9/7/2017", "rawpid": "1706.02515", "tags": ["cs.LG", "stat.ML"], "title": "Self-Normalizing Neural Networks"}, {"abstract": "Deep reinforcement learning is poised to revolutionise the field of AI and\nrepresents a step towards building autonomous systems with a higher level\nunderstanding of the visual world. Currently, deep learning is enabling\nreinforcement learning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep reinforcement\nlearning algorithms are also applied to robotics, allowing control policies for\nrobots to be learned directly from camera inputs in the real world. In this\nsurvey, we begin with an introduction to the general field of reinforcement\nlearning, then progress to the main streams of value-based and policy-based\nmethods. Our survey will cover central algorithms in deep reinforcement\nlearning, including the deep $Q$-network, trust region policy optimisation, and\nasynchronous advantage actor-critic. In parallel, we highlight the unique\nadvantages of deep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current areas of\nresearch within the field.", "authors": ["Kai Arulkumaran", "Marc Peter Deisenroth", "Miles Brundage", "Anil Anthony Bharath"], "category": "cs.LG", "comment": "IEEE Signal Processing Magazine, Special Issue on Deep Learning for\n  Image Understanding (arXiv ext...", "img": "/static/thumbs/1708.05866v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1708.05866v2", "num_discussion": 0, "originally_published_time": "8/19/2017", "pid": "1708.05866v2", "published_time": "9/28/2017", "rawpid": "1708.05866", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "A Brief Survey of Deep Reinforcement Learning"}, {"abstract": "Recent work has shown that convolutional networks can be substantially\ndeeper, more accurate, and efficient to train if they contain shorter\nconnections between layers close to the input and those close to the output. In\nthis paper, we embrace this observation and introduce the Dense Convolutional\nNetwork (DenseNet), which connects each layer to every other layer in a\nfeed-forward fashion. Whereas traditional convolutional networks with L layers\nhave L connections - one between each layer and its subsequent layer - our\nnetwork has L(L+1)/2 direct connections. For each layer, the feature-maps of\nall preceding layers are used as inputs, and its own feature-maps are used as\ninputs into all subsequent layers. DenseNets have several compelling\nadvantages: they alleviate the vanishing-gradient problem, strengthen feature\npropagation, encourage feature reuse, and substantially reduce the number of\nparameters. We evaluate our proposed architecture on four highly competitive\nobject recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet).\nDenseNets obtain significant improvements over the state-of-the-art on most of\nthem, whilst requiring less computation to achieve high performance. Code and\npre-trained models are available at https://github.com/liuzhuang13/DenseNet .", "authors": ["Gao Huang", "Zhuang Liu", "Laurens van der Maaten", "Kilian Q. Weinberger"], "category": "cs.CV", "comment": "CVPR 2017", "img": "/static/thumbs/1608.06993v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.06993v5", "num_discussion": 0, "originally_published_time": "8/25/2016", "pid": "1608.06993v5", "published_time": "1/28/2018", "rawpid": "1608.06993", "tags": ["cs.CV", "cs.LG"], "title": "Densely Connected Convolutional Networks"}, {"abstract": "The move from hand-designed features to learned features in machine learning\nhas been wildly successful. In spite of this, optimization algorithms are still\ndesigned by hand. In this paper we show how the design of an optimization\nalgorithm can be cast as a learning problem, allowing the algorithm to learn to\nexploit structure in the problems of interest in an automatic way. Our learned\nalgorithms, implemented by LSTMs, outperform generic, hand-designed competitors\non the tasks for which they are trained, and also generalize well to new tasks\nwith similar structure. We demonstrate this on a number of tasks, including\nsimple convex problems, training neural networks, and styling images with\nneural art.", "authors": ["Marcin Andrychowicz", "Misha Denil", "Sergio Gomez", "Matthew W. Hoffman", "David Pfau", "Tom Schaul", "Brendan Shillingford", "Nando de Freitas"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1606.04474v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.04474v2", "num_discussion": 0, "originally_published_time": "6/14/2016", "pid": "1606.04474v2", "published_time": "11/30/2016", "rawpid": "1606.04474", "tags": ["cs.NE", "cs.LG"], "title": "Learning to learn by gradient descent by gradient descent"}, {"abstract": "We introduce a new algorithm named WGAN, an alternative to traditional GAN\ntraining. In this new model, we show that we can improve the stability of\nlearning, get rid of problems like mode collapse, and provide meaningful\nlearning curves useful for debugging and hyperparameter searches. Furthermore,\nwe show that the corresponding optimization problem is sound, and provide\nextensive theoretical work highlighting the deep connections to other distances\nbetween distributions.", "authors": ["Martin Arjovsky", "Soumith Chintala", "L\u00e9on Bottou"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1701.07875v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.07875v3", "num_discussion": 0, "originally_published_time": "1/26/2017", "pid": "1701.07875v3", "published_time": "12/6/2017", "rawpid": "1701.07875", "tags": ["stat.ML", "cs.LG"], "title": "Wasserstein GAN"}, {"abstract": "We present YOLO, a new approach to object detection. Prior work on object\ndetection repurposes classifiers to perform detection. Instead, we frame object\ndetection as a regression problem to spatially separated bounding boxes and\nassociated class probabilities. A single neural network predicts bounding boxes\nand class probabilities directly from full images in one evaluation. Since the\nwhole detection pipeline is a single network, it can be optimized end-to-end\ndirectly on detection performance.\n  Our unified architecture is extremely fast. Our base YOLO model processes\nimages in real-time at 45 frames per second. A smaller version of the network,\nFast YOLO, processes an astounding 155 frames per second while still achieving\ndouble the mAP of other real-time detectors. Compared to state-of-the-art\ndetection systems, YOLO makes more localization errors but is far less likely\nto predict false detections where nothing exists. Finally, YOLO learns very\ngeneral representations of objects. It outperforms all other detection methods,\nincluding DPM and R-CNN, by a wide margin when generalizing from natural images\nto artwork on both the Picasso Dataset and the People-Art Dataset.", "authors": ["Joseph Redmon", "Santosh Divvala", "Ross Girshick", "Ali Farhadi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1506.02640v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.02640v5", "num_discussion": 1, "originally_published_time": "6/8/2015", "pid": "1506.02640v5", "published_time": "5/9/2016", "rawpid": "1506.02640", "tags": ["cs.CV"], "title": "You Only Look Once: Unified, Real-Time Object Detection"}, {"abstract": "In the last few years, deep learning has led to very good performance on a\nvariety of problems, such as visual recognition, speech recognition and natural\nlanguage processing. Among different types of deep neural networks,\nconvolutional neural networks have been most extensively studied. Leveraging on\nthe rapid growth in the amount of the annotated data and the great improvements\nin the strengths of graphics processor units, the research on convolutional\nneural networks has been emerged swiftly and achieved state-of-the-art results\non various tasks. In this paper, we provide a broad survey of the recent\nadvances in convolutional neural networks. We detailize the improvements of CNN\non different aspects, including layer design, activation function, loss\nfunction, regularization, optimization and fast computation. Besides, we also\nintroduce various applications of convolutional neural networks in computer\nvision, speech and natural language processing.", "authors": ["Jiuxiang Gu", "Zhenhua Wang", "Jason Kuen", "Lianyang Ma", "Amir Shahroudy", "Bing Shuai", "Ting Liu", "Xingxing Wang", "Li Wang", "Gang Wang", "Jianfei Cai", "Tsuhan Chen"], "category": "cs.CV", "comment": "Pattern Recognition, Elsevier", "img": "/static/thumbs/1512.07108v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1512.07108v6", "num_discussion": 0, "originally_published_time": "12/22/2015", "pid": "1512.07108v6", "published_time": "10/19/2017", "rawpid": "1512.07108", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "Recent Advances in Convolutional Neural Networks"}, {"abstract": "We propose a new framework for estimating generative models via an\nadversarial process, in which we simultaneously train two models: a generative\nmodel G that captures the data distribution, and a discriminative model D that\nestimates the probability that a sample came from the training data rather than\nG. The training procedure for G is to maximize the probability of D making a\nmistake. This framework corresponds to a minimax two-player game. In the space\nof arbitrary functions G and D, a unique solution exists, with G recovering the\ntraining data distribution and D equal to 1/2 everywhere. In the case where G\nand D are defined by multilayer perceptrons, the entire system can be trained\nwith backpropagation. There is no need for any Markov chains or unrolled\napproximate inference networks during either training or generation of samples.\nExperiments demonstrate the potential of the framework through qualitative and\nquantitative evaluation of the generated samples.", "authors": ["Ian J. Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1406.2661v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1406.2661v1", "num_discussion": 0, "originally_published_time": "6/10/2014", "pid": "1406.2661v1", "published_time": "6/10/2014", "rawpid": "1406.2661", "tags": ["stat.ML", "cs.LG"], "title": "Generative Adversarial Networks"}, {"abstract": "Deep learning yields great results across many fields, from speech\nrecognition, image classification, to translation. But for each problem,\ngetting a deep model to work well involves research into the architecture and a\nlong period of tuning. We present a single model that yields good results on a\nnumber of problems spanning multiple domains. In particular, this single model\nis trained concurrently on ImageNet, multiple translation tasks, image\ncaptioning (COCO dataset), a speech recognition corpus, and an English parsing\ntask. Our model architecture incorporates building blocks from multiple\ndomains. It contains convolutional layers, an attention mechanism, and\nsparsely-gated layers. Each of these computational blocks is crucial for a\nsubset of the tasks we train on. Interestingly, even if a block is not crucial\nfor a task, we observe that adding it never hurts performance and in most cases\nimproves it on all tasks. We also show that tasks with less data benefit\nlargely from joint training with other tasks, while performance on large tasks\ndegrades only slightly if at all.", "authors": ["Lukasz Kaiser", "Aidan N. Gomez", "Noam Shazeer", "Ashish Vaswani", "Niki Parmar", "Llion Jones", "Jakob Uszkoreit"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1706.05137v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.05137v1", "num_discussion": 0, "originally_published_time": "6/16/2017", "pid": "1706.05137v1", "published_time": "6/16/2017", "rawpid": "1706.05137", "tags": ["cs.LG", "stat.ML"], "title": "One Model To Learn Them All"}, {"abstract": "State-of-the-art object detection networks depend on region proposal\nalgorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN\nhave reduced the running time of these detection networks, exposing region\nproposal computation as a bottleneck. In this work, we introduce a Region\nProposal Network (RPN) that shares full-image convolutional features with the\ndetection network, thus enabling nearly cost-free region proposals. An RPN is a\nfully convolutional network that simultaneously predicts object bounds and\nobjectness scores at each position. The RPN is trained end-to-end to generate\nhigh-quality region proposals, which are used by Fast R-CNN for detection. We\nfurther merge RPN and Fast R-CNN into a single network by sharing their\nconvolutional features---using the recently popular terminology of neural\nnetworks with \u0027attention\u0027 mechanisms, the RPN component tells the unified\nnetwork where to look. For the very deep VGG-16 model, our detection system has\na frame rate of 5fps (including all steps) on a GPU, while achieving\nstate-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS\nCOCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015\ncompetitions, Faster R-CNN and RPN are the foundations of the 1st-place winning\nentries in several tracks. Code has been made publicly available.", "authors": ["Shaoqing Ren", "Kaiming He", "Ross Girshick", "Jian Sun"], "category": "cs.CV", "comment": "Extended tech report", "img": "/static/thumbs/1506.01497v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.01497v3", "num_discussion": 0, "originally_published_time": "6/4/2015", "pid": "1506.01497v3", "published_time": "1/6/2016", "rawpid": "1506.01497", "tags": ["cs.CV"], "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal\n  Networks"}, {"abstract": "We introduce YOLO9000, a state-of-the-art, real-time object detection system\nthat can detect over 9000 object categories. First we propose various\nimprovements to the YOLO detection method, both novel and drawn from prior\nwork. The improved model, YOLOv2, is state-of-the-art on standard detection\ntasks like PASCAL VOC and COCO. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At\n40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like\nFaster RCNN with ResNet and SSD while still running significantly faster.\nFinally we propose a method to jointly train on object detection and\nclassification. Using this method we train YOLO9000 simultaneously on the COCO\ndetection dataset and the ImageNet classification dataset. Our joint training\nallows YOLO9000 to predict detections for object classes that don\u0027t have\nlabelled detection data. We validate our approach on the ImageNet detection\ntask. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite\nonly having detection data for 44 of the 200 classes. On the 156 classes not in\nCOCO, YOLO9000 gets 16.0 mAP. But YOLO can detect more than just 200 classes;\nit predicts detections for more than 9000 different object categories. And it\nstill runs in real-time.", "authors": ["Joseph Redmon", "Ali Farhadi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.08242v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.08242v1", "num_discussion": 1, "originally_published_time": "12/25/2016", "pid": "1612.08242v1", "published_time": "12/25/2016", "rawpid": "1612.08242", "tags": ["cs.CV"], "title": "YOLO9000: Better, Faster, Stronger"}, {"abstract": "Generative adversarial networks (GANs) provide a way to learn deep\nrepresentations without extensively annotated training data. They achieve this\nthrough deriving backpropagation signals through a competitive process\ninvolving a pair of networks. The representations that can be learned by GANs\nmay be used in a variety of applications, including image synthesis, semantic\nimage editing, style transfer, image super-resolution and classification. The\naim of this review paper is to provide an overview of GANs for the signal\nprocessing community, drawing on familiar analogies and concepts where\npossible. In addition to identifying different methods for training and\nconstructing GANs, we also point to remaining challenges in their theory and\napplication.", "authors": ["Antonia Creswell", "Tom White", "Vincent Dumoulin", "Kai Arulkumaran", "Biswa Sengupta", "Anil A Bharath"], "category": "cs.CV", "comment": "Accepted in the IEEE Signal Processing Magazine Special Issue on Deep\n  Learning for Visual Understa...", "img": "/static/thumbs/1710.07035v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.07035v1", "num_discussion": 0, "originally_published_time": "10/19/2017", "pid": "1710.07035v1", "published_time": "10/19/2017", "rawpid": "1710.07035", "tags": ["cs.CV"], "title": "Generative Adversarial Networks: An Overview"}, {"abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition.", "authors": ["Aaron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "category": "cs.SD", "comment": "", "img": "/static/thumbs/1609.03499v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.03499v2", "num_discussion": 0, "originally_published_time": "9/12/2016", "pid": "1609.03499v2", "published_time": "9/19/2016", "rawpid": "1609.03499", "tags": ["cs.SD", "cs.LG"], "title": "WaveNet: A Generative Model for Raw Audio"}, {"abstract": "We introduce a method to train Binarized Neural Networks (BNNs) - neural\nnetworks with binary weights and activations at run-time. At training-time the\nbinary weights and activations are used for computing the parameters gradients.\nDuring the forward pass, BNNs drastically reduce memory size and accesses, and\nreplace most arithmetic operations with bit-wise operations, which is expected\nto substantially improve power-efficiency. To validate the effectiveness of\nBNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On\nboth, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10\nand SVHN datasets. Last but not least, we wrote a binary matrix multiplication\nGPU kernel with which it is possible to run our MNIST BNN 7 times faster than\nwith an unoptimized GPU kernel, without suffering any loss in classification\naccuracy. The code for training and running our BNNs is available on-line.", "authors": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "category": "cs.LG", "comment": "11 pages and 3 figures", "img": "/static/thumbs/1602.02830v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.02830v3", "num_discussion": 0, "originally_published_time": "2/9/2016", "pid": "1602.02830v3", "published_time": "3/17/2016", "rawpid": "1602.02830", "tags": ["cs.LG"], "title": "Binarized Neural Networks: Training Deep Neural Networks with Weights\n  and Activations Constrained to +1 or -1"}, {"abstract": "Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet", "authors": ["Forrest N. Iandola", "Song Han", "Matthew W. Moskewicz", "Khalid Ashraf", "William J. Dally", "Kurt Keutzer"], "category": "cs.CV", "comment": "In ICLR Format", "img": "/static/thumbs/1602.07360v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.07360v4", "num_discussion": 0, "originally_published_time": "2/24/2016", "pid": "1602.07360v4", "published_time": "11/4/2016", "rawpid": "1602.07360", "tags": ["cs.CV", "cs.AI"], "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and \u003c0.5MB\n  model size"}, {"abstract": "Deep residual networks have emerged as a family of extremely deep\narchitectures showing compelling accuracy and nice convergence behaviors. In\nthis paper, we analyze the propagation formulations behind the residual\nbuilding blocks, which suggest that the forward and backward signals can be\ndirectly propagated from one block to any other block, when using identity\nmappings as the skip connections and after-addition activation. A series of\nablation experiments support the importance of these identity mappings. This\nmotivates us to propose a new residual unit, which makes training easier and\nimproves generalization. We report improved results using a 1001-layer ResNet\non CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.\nCode is available at: https://github.com/KaimingHe/resnet-1k-layers", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "category": "cs.CV", "comment": "ECCV 2016 camera-ready", "img": "/static/thumbs/1603.05027v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.05027v3", "num_discussion": 0, "originally_published_time": "3/16/2016", "pid": "1603.05027v3", "published_time": "7/25/2016", "rawpid": "1603.05027", "tags": ["cs.CV", "cs.LG"], "title": "Identity Mappings in Deep Residual Networks"}, {"abstract": "We present a method for detecting objects in images using a single deep\nneural network. Our approach, named SSD, discretizes the output space of\nbounding boxes into a set of default boxes over different aspect ratios and\nscales per feature map location. At prediction time, the network generates\nscores for the presence of each object category in each default box and\nproduces adjustments to the box to better match the object shape. Additionally,\nthe network combines predictions from multiple feature maps with different\nresolutions to naturally handle objects of various sizes. Our SSD model is\nsimple relative to methods that require object proposals because it completely\neliminates proposal generation and subsequent pixel or feature resampling stage\nand encapsulates all computation in a single network. This makes SSD easy to\ntrain and straightforward to integrate into systems that require a detection\ncomponent. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets\nconfirm that SSD has comparable accuracy to methods that utilize an additional\nobject proposal step and is much faster, while providing a unified framework\nfor both training and inference. Compared to other single stage methods, SSD\nhas much better accuracy, even with a smaller input image size. For $300\\times\n300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan\nX and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a\ncomparable state of the art Faster R-CNN model. Code is available at\nhttps://github.com/weiliu89/caffe/tree/ssd .", "authors": ["Wei Liu", "Dragomir Anguelov", "Dumitru Erhan", "Christian Szegedy", "Scott Reed", "Cheng-Yang Fu", "Alexander C. Berg"], "category": "cs.CV", "comment": "ECCV 2016", "img": "/static/thumbs/1512.02325v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1512.02325v5", "num_discussion": 0, "originally_published_time": "12/8/2015", "pid": "1512.02325v5", "published_time": "12/29/2016", "rawpid": "1512.02325", "tags": ["cs.CV"], "title": "SSD: Single Shot MultiBox Detector"}, {"abstract": "Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer\u0027s inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters.", "authors": ["Sergey Ioffe", "Christian Szegedy"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1502.03167v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1502.03167v3", "num_discussion": 0, "originally_published_time": "2/11/2015", "pid": "1502.03167v3", "published_time": "3/2/2015", "rawpid": "1502.03167", "tags": ["cs.LG"], "title": "Batch Normalization: Accelerating Deep Network Training by Reducing\n  Internal Covariate Shift"}, {"abstract": "Despite their massive size, successful deep artificial neural networks can\nexhibit a remarkably small difference between training and test performance.\nConventional wisdom attributes small generalization error either to properties\nof the model family, or to the regularization techniques used during training.\n  Through extensive systematic experiments, we show how these traditional\napproaches fail to explain why large neural networks generalize well in\npractice. Specifically, our experiments establish that state-of-the-art\nconvolutional networks for image classification trained with stochastic\ngradient methods easily fit a random labeling of the training data. This\nphenomenon is qualitatively unaffected by explicit regularization, and occurs\neven if we replace the true images by completely unstructured random noise. We\ncorroborate these experimental findings with a theoretical construction showing\nthat simple depth two neural networks already have perfect finite sample\nexpressivity as soon as the number of parameters exceeds the number of data\npoints as it usually does in practice.\n  We interpret our experimental findings by comparison with traditional models.", "authors": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals"], "category": "cs.LG", "comment": "Published in ICLR 2017", "img": "/static/thumbs/1611.03530v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.03530v2", "num_discussion": 0, "originally_published_time": "11/10/2016", "pid": "1611.03530v2", "published_time": "2/26/2017", "rawpid": "1611.03530", "tags": ["cs.LG"], "title": "Understanding deep learning requires rethinking generalization"}, {"abstract": "We propose a conceptually simple and lightweight framework for deep\nreinforcement learning that uses asynchronous gradient descent for optimization\nof deep neural network controllers. We present asynchronous variants of four\nstandard reinforcement learning algorithms and show that parallel\nactor-learners have a stabilizing effect on training allowing all four methods\nto successfully train neural network controllers. The best performing method,\nan asynchronous variant of actor-critic, surpasses the current state-of-the-art\non the Atari domain while training for half the time on a single multi-core CPU\ninstead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds\non a wide variety of continuous motor control problems as well as on a new task\nof navigating random 3D mazes using a visual input.", "authors": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P. Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1602.01783v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.01783v2", "num_discussion": 0, "originally_published_time": "2/4/2016", "pid": "1602.01783v2", "published_time": "6/16/2016", "rawpid": "1602.01783", "tags": ["cs.LG"], "title": "Asynchronous Methods for Deep Reinforcement Learning"}, {"abstract": "Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.", "authors": ["Jimmy Lei Ba", "Jamie Ryan Kiros", "Geoffrey E. Hinton"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1607.06450v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1607.06450v1", "num_discussion": 0, "originally_published_time": "7/21/2016", "pid": "1607.06450v1", "published_time": "7/21/2016", "rawpid": "1607.06450", "tags": ["stat.ML", "cs.LG"], "title": "Layer Normalization"}, {"abstract": "Recent progress in artificial intelligence (AI) has renewed interest in\nbuilding systems that learn and think like people. Many advances have come from\nusing deep neural networks trained end-to-end in tasks such as object\nrecognition, video games, and board games, achieving performance that equals or\neven beats humans in some respects. Despite their biological inspiration and\nperformance achievements, these systems differ from human intelligence in\ncrucial ways. We review progress in cognitive science suggesting that truly\nhuman-like learning and thinking machines will have to reach beyond current\nengineering trends in both what they learn, and how they learn it.\nSpecifically, we argue that these machines should (a) build causal models of\nthe world that support explanation and understanding, rather than merely\nsolving pattern recognition problems; (b) ground learning in intuitive theories\nof physics and psychology, to support and enrich the knowledge that is learned;\nand (c) harness compositionality and learning-to-learn to rapidly acquire and\ngeneralize knowledge to new tasks and situations. We suggest concrete\nchallenges and promising routes towards these goals that can combine the\nstrengths of recent neural network advances with more structured cognitive\nmodels.", "authors": ["Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman"], "category": "cs.AI", "comment": "In press at Behavioral and Brain Sciences. Open call for commentary\n  proposals (until Nov. 22, 2016...", "img": "/static/thumbs/1604.00289v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.00289v3", "num_discussion": 0, "originally_published_time": "4/1/2016", "pid": "1604.00289v3", "published_time": "11/2/2016", "rawpid": "1604.00289", "tags": ["cs.AI", "cs.CV", "cs.LG", "cs.NE", "stat.ML"], "title": "Building Machines That Learn and Think Like People"}, {"abstract": "We explore the use of Evolution Strategies (ES), a class of black box\noptimization algorithms, as an alternative to popular MDP-based RL techniques\nsuch as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show\nthat ES is a viable solution strategy that scales extremely well with the\nnumber of CPUs available: By using a novel communication strategy based on\ncommon random numbers, our ES implementation only needs to communicate scalars,\nmaking it possible to scale to over a thousand parallel workers. This allows us\nto solve 3D humanoid walking in 10 minutes and obtain competitive results on\nmost Atari games after one hour of training. In addition, we highlight several\nadvantages of ES as a black box optimization technique: it is invariant to\naction frequency and delayed rewards, tolerant of extremely long horizons, and\ndoes not need temporal discounting or value function approximation.", "authors": ["Tim Salimans", "Jonathan Ho", "Xi Chen", "Szymon Sidor", "Ilya Sutskever"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1703.03864v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.03864v2", "num_discussion": 0, "originally_published_time": "3/10/2017", "pid": "1703.03864v2", "published_time": "9/7/2017", "rawpid": "1703.03864", "tags": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning"}, {"abstract": "We propose two efficient approximations to standard convolutional neural\nnetworks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks,\nthe filters are approximated with binary values resulting in 32x memory saving.\nIn XNOR-Networks, both the filters and the input to convolutional layers are\nbinary. XNOR-Networks approximate convolutions using primarily binary\noperations. This results in 58x faster convolutional operations and 32x memory\nsavings. XNOR-Nets offer the possibility of running state-of-the-art networks\non CPUs (rather than GPUs) in real-time. Our binary networks are simple,\naccurate, efficient, and work on challenging visual tasks. We evaluate our\napproach on the ImageNet classification task. The classification accuracy with\na Binary-Weight-Network version of AlexNet is only 2.9% less than the\nfull-precision AlexNet (in top-1 measure). We compare our method with recent\nnetwork binarization methods, BinaryConnect and BinaryNets, and outperform\nthese methods by large margins on ImageNet, more than 16% in top-1 accuracy.", "authors": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1603.05279v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.05279v4", "num_discussion": 0, "originally_published_time": "3/16/2016", "pid": "1603.05279v4", "published_time": "8/2/2016", "rawpid": "1603.05279", "tags": ["cs.CV"], "title": "XNOR-Net: ImageNet Classification Using Binary Convolutional Neural\n  Networks"}, {"abstract": "Deep residual networks were shown to be able to scale up to thousands of\nlayers and still have improving performance. However, each fraction of a\npercent of improved accuracy costs nearly doubling the number of layers, and so\ntraining very deep residual networks has a problem of diminishing feature\nreuse, which makes these networks very slow to train. To tackle these problems,\nin this paper we conduct a detailed experimental study on the architecture of\nResNet blocks, based on which we propose a novel architecture where we decrease\ndepth and increase width of residual networks. We call the resulting network\nstructures wide residual networks (WRNs) and show that these are far superior\nover their commonly used thin and very deep counterparts. For example, we\ndemonstrate that even a simple 16-layer-deep wide residual network outperforms\nin accuracy and efficiency all previous deep residual networks, including\nthousand-layer-deep networks, achieving new state-of-the-art results on CIFAR,\nSVHN, COCO, and significant improvements on ImageNet. Our code and models are\navailable at https://github.com/szagoruyko/wide-residual-networks", "authors": ["Sergey Zagoruyko", "Nikos Komodakis"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1605.07146v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.07146v4", "num_discussion": 0, "originally_published_time": "5/23/2016", "pid": "1605.07146v4", "published_time": "6/14/2017", "rawpid": "1605.07146", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "Wide Residual Networks"}, {"abstract": "Generative Adversarial Networks (GANs) are powerful generative models, but\nsuffer from training instability. The recently proposed Wasserstein GAN (WGAN)\nmakes progress toward stable training of GANs, but sometimes can still generate\nonly low-quality samples or fail to converge. We find that these problems are\noften due to the use of weight clipping in WGAN to enforce a Lipschitz\nconstraint on the critic, which can lead to undesired behavior. We propose an\nalternative to clipping weights: penalize the norm of gradient of the critic\nwith respect to its input. Our proposed method performs better than standard\nWGAN and enables stable training of a wide variety of GAN architectures with\nalmost no hyperparameter tuning, including 101-layer ResNets and language\nmodels over discrete data. We also achieve high quality generations on CIFAR-10\nand LSUN bedrooms.", "authors": ["Ishaan Gulrajani", "Faruk Ahmed", "Martin Arjovsky", "Vincent Dumoulin", "Aaron Courville"], "category": "cs.LG", "comment": "NIPS camera-ready", "img": "/static/thumbs/1704.00028v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.00028v3", "num_discussion": 0, "originally_published_time": "3/31/2017", "pid": "1704.00028v3", "published_time": "12/25/2017", "rawpid": "1704.00028", "tags": ["cs.LG", "stat.ML"], "title": "Improved Training of Wasserstein GANs"}, {"abstract": "In recent years, supervised learning with convolutional networks (CNNs) has\nseen huge adoption in computer vision applications. Comparatively, unsupervised\nlearning with CNNs has received less attention. In this work we hope to help\nbridge the gap between the success of CNNs for supervised learning and\nunsupervised learning. We introduce a class of CNNs called deep convolutional\ngenerative adversarial networks (DCGANs), that have certain architectural\nconstraints, and demonstrate that they are a strong candidate for unsupervised\nlearning. Training on various image datasets, we show convincing evidence that\nour deep convolutional adversarial pair learns a hierarchy of representations\nfrom object parts to scenes in both the generator and discriminator.\nAdditionally, we use the learned features for novel tasks - demonstrating their\napplicability as general image representations.", "authors": ["Alec Radford", "Luke Metz", "Soumith Chintala"], "category": "cs.LG", "comment": "Under review as a conference paper at ICLR 2016", "img": "/static/thumbs/1511.06434v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.06434v2", "num_discussion": 0, "originally_published_time": "11/19/2015", "pid": "1511.06434v2", "published_time": "1/7/2016", "rawpid": "1511.06434", "tags": ["cs.LG", "cs.CV"], "title": "Unsupervised Representation Learning with Deep Convolutional Generative\n  Adversarial Networks"}, {"abstract": "Learning both hierarchical and temporal representation has been among the\nlong-standing challenges of recurrent neural networks. Multiscale recurrent\nneural networks have been considered as a promising approach to resolve this\nissue, yet there has been a lack of empirical evidence showing that this type\nof models can actually capture the temporal dependencies by discovering the\nlatent hierarchical structure of the sequence. In this paper, we propose a\nnovel multiscale approach, called the hierarchical multiscale recurrent neural\nnetworks, which can capture the latent hierarchical structure in the sequence\nby encoding the temporal dependencies with different timescales using a novel\nupdate mechanism. We show some evidence that our proposed multiscale\narchitecture can discover underlying hierarchical structure in the sequences\nwithout using explicit boundary information. We evaluate our proposed model on\ncharacter-level language modelling and handwriting sequence modelling.", "authors": ["Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1609.01704v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.01704v7", "num_discussion": 0, "originally_published_time": "9/6/2016", "pid": "1609.01704v7", "published_time": "3/9/2017", "rawpid": "1609.01704", "tags": ["cs.LG"], "title": "Hierarchical Multiscale Recurrent Neural Networks"}, {"abstract": "In just three years, Variational Autoencoders (VAEs) have emerged as one of\nthe most popular approaches to unsupervised learning of complicated\ndistributions. VAEs are appealing because they are built on top of standard\nfunction approximators (neural networks), and can be trained with stochastic\ngradient descent. VAEs have already shown promise in generating many kinds of\ncomplicated data, including handwritten digits, faces, house numbers, CIFAR\nimages, physical models of scenes, segmentation, and predicting the future from\nstatic images. This tutorial introduces the intuitions behind VAEs, explains\nthe mathematics behind them, and describes some empirical behavior. No prior\nknowledge of variational Bayesian methods is assumed.", "authors": ["Carl Doersch"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1606.05908v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.05908v2", "num_discussion": 0, "originally_published_time": "6/19/2016", "pid": "1606.05908v2", "published_time": "8/13/2016", "rawpid": "1606.05908", "tags": ["stat.ML", "cs.LG"], "title": "Tutorial on Variational Autoencoders"}, {"abstract": "It is common practice to decay the learning rate. Here we show one can\nusually obtain the same learning curve on both training and test sets by\ninstead increasing the batch size during training. This procedure is successful\nfor stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum,\nand Adam. It reaches equivalent test accuracies after the same number of\ntraining epochs, but with fewer parameter updates, leading to greater\nparallelism and shorter training times. We can further reduce the number of\nparameter updates by increasing the learning rate $\\epsilon$ and scaling the\nbatch size $B \\propto \\epsilon$. Finally, one can increase the momentum\ncoefficient $m$ and scale $B \\propto 1/(1-m)$, although this tends to slightly\nreduce the test accuracy. Crucially, our techniques allow us to repurpose\nexisting training schedules for large batch training with no hyper-parameter\ntuning. We train ResNet-50 on ImageNet to $76.1\\%$ validation accuracy in under\n30 minutes.", "authors": ["Samuel L. Smith", "Pieter-Jan Kindermans", "Chris Ying", "Quoc V. Le"], "category": "cs.LG", "comment": "11 pages, 8 figures. Published as a conference paper at ICLR 2018", "img": "/static/thumbs/1711.00489v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.00489v2", "num_discussion": 0, "originally_published_time": "11/1/2017", "pid": "1711.00489v2", "published_time": "2/24/2018", "rawpid": "1711.00489", "tags": ["cs.LG", "cs.CV", "cs.DC", "stat.ML"], "title": "Don\u0027t Decay the Learning Rate, Increase the Batch Size"}, {"abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn\ngenerative models mapping from simple latent distributions to arbitrarily\ncomplex data distributions has been demonstrated empirically, with compelling\nresults showing that the latent space of such generators captures semantic\nvariation in the data distribution. Intuitively, models trained to predict\nthese semantic latent representations given data may serve as useful feature\nrepresentations for auxiliary problems where semantics are relevant. However,\nin their existing form, GANs have no means of learning the inverse mapping --\nprojecting data back into the latent space. We propose Bidirectional Generative\nAdversarial Networks (BiGANs) as a means of learning this inverse mapping, and\ndemonstrate that the resulting learned feature representation is useful for\nauxiliary supervised discrimination tasks, competitive with contemporary\napproaches to unsupervised and self-supervised feature learning.", "authors": ["Jeff Donahue", "Philipp Kr\u00e4henb\u00fchl", "Trevor Darrell"], "category": "cs.LG", "comment": "Published as a conference paper at ICLR 2017. Changelog: (v7) Table 2\n  results improved 1-2% due to...", "img": "/static/thumbs/1605.09782v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.09782v7", "num_discussion": 0, "originally_published_time": "5/31/2016", "pid": "1605.09782v7", "published_time": "4/3/2017", "rawpid": "1605.09782", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "title": "Adversarial Feature Learning"}, {"abstract": "Relational reasoning is a central component of generally intelligent\nbehavior, but has proven difficult for neural networks to learn. In this paper\nwe describe how to use Relation Networks (RNs) as a simple plug-and-play module\nto solve problems that fundamentally hinge on relational reasoning. We tested\nRN-augmented networks on three tasks: visual question answering using a\nchallenging dataset called CLEVR, on which we achieve state-of-the-art,\nsuper-human performance; text-based question answering using the bAbI suite of\ntasks; and complex reasoning about dynamic physical systems. Then, using a\ncurated dataset called Sort-of-CLEVR we show that powerful convolutional\nnetworks do not have a general capacity to solve relational questions, but can\ngain this capacity when augmented with RNs. Our work shows how a deep learning\narchitecture equipped with an RN module can implicitly discover and learn to\nreason about entities and their relations.", "authors": ["Adam Santoro", "David Raposo", "David G. T. Barrett", "Mateusz Malinowski", "Razvan Pascanu", "Peter Battaglia", "Timothy Lillicrap"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1706.01427v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.01427v1", "num_discussion": 0, "originally_published_time": "6/5/2017", "pid": "1706.01427v1", "published_time": "6/5/2017", "rawpid": "1706.01427", "tags": ["cs.CL", "cs.LG"], "title": "A simple neural network module for relational reasoning"}, {"abstract": "This paper is an attempt to explain all the matrix calculus you need in order\nto understand the training of deep neural networks. We assume no math knowledge\nbeyond what you learned in calculus 1, and provide links to help you refresh\nthe necessary math where needed. Note that you do not need to understand this\nmaterial before you start learning to train and use deep learning in practice;\nrather, this material is for those who are already familiar with the basics of\nneural networks, and wish to deepen their understanding of the underlying math.\nDon\u0027t worry if you get stuck at some point along the way---just go back and\nreread the previous section, and try writing down and working through some\nexamples. And if you\u0027re still stuck, we\u0027re happy to answer your questions in\nthe Theory category at forums.fast.ai. Note: There is a reference section at\nthe end of the paper summarizing all the key matrix calculus rules and\nterminology discussed here. See related articles at http://explained.ai", "authors": ["Terence Parr", "Jeremy Howard"], "category": "cs.LG", "comment": "PDF version of mobile/web friendly version\n  http://explained.ai/matrix-calculus/index.html", "img": "/static/thumbs/1802.01528v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.01528v3", "num_discussion": 0, "originally_published_time": "2/5/2018", "pid": "1802.01528v3", "published_time": "7/2/2018", "rawpid": "1802.01528", "tags": ["cs.LG", "stat.ML"], "title": "The Matrix Calculus You Need For Deep Learning"}, {"abstract": "We present weight normalization: a reparameterization of the weight vectors\nin a neural network that decouples the length of those weight vectors from\ntheir direction. By reparameterizing the weights in this way we improve the\nconditioning of the optimization problem and we speed up convergence of\nstochastic gradient descent. Our reparameterization is inspired by batch\nnormalization but does not introduce any dependencies between the examples in a\nminibatch. This means that our method can also be applied successfully to\nrecurrent models such as LSTMs and to noise-sensitive applications such as deep\nreinforcement learning or generative models, for which batch normalization is\nless well suited. Although our method is much simpler, it still provides much\nof the speed-up of full batch normalization. In addition, the computational\noverhead of our method is lower, permitting more optimization steps to be taken\nin the same amount of time. We demonstrate the usefulness of our method on\napplications in supervised image recognition, generative modelling, and deep\nreinforcement learning.", "authors": ["Tim Salimans", "Diederik P. Kingma"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1602.07868v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.07868v3", "num_discussion": 1, "originally_published_time": "2/25/2016", "pid": "1602.07868v3", "published_time": "6/4/2016", "rawpid": "1602.07868", "tags": ["cs.LG", "cs.AI", "cs.NE"], "title": "Weight Normalization: A Simple Reparameterization to Accelerate Training\n  of Deep Neural Networks"}, {"abstract": "We present a framework for efficient inference in structured image models\nthat explicitly reason about objects. We achieve this by performing\nprobabilistic inference using a recurrent neural network that attends to scene\nelements and processes them one at a time. Crucially, the model itself learns\nto choose the appropriate number of inference steps. We use this scheme to\nlearn to perform inference in partially specified 2D models (variable-sized\nvariational auto-encoders) and fully specified 3D models (probabilistic\nrenderers). We show that such models learn to identify multiple objects -\ncounting, locating and classifying the elements of a scene - without any\nsupervision, e.g., decomposing 3D images with various numbers of objects in a\nsingle forward pass of a neural network. We further show that the networks\nproduce accurate inferences when compared to supervised counterparts, and that\ntheir structure leads to improved generalization.", "authors": ["S. M. Ali Eslami", "Nicolas Heess", "Theophane Weber", "Yuval Tassa", "David Szepesvari", "Koray Kavukcuoglu", "Geoffrey E. Hinton"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1603.08575v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.08575v3", "num_discussion": 0, "originally_published_time": "3/28/2016", "pid": "1603.08575v3", "published_time": "8/12/2016", "rawpid": "1603.08575", "tags": ["cs.CV", "cs.LG"], "title": "Attend, Infer, Repeat: Fast Scene Understanding with Generative Models"}, {"abstract": "With a direct analysis of neural networks, this paper presents a\nmathematically tight generalization theory to partially address an open problem\nregarding the generalization of deep learning. Unlike previous bound-based\ntheory, our main theory is quantitatively as tight as possible for every\ndataset individually, while producing qualitative insights competitively. Our\nresults give insight into why and how deep learning can generalize well,\ndespite its large capacity, complexity, possible algorithmic instability,\nnonrobustness, and sharp minima, answering to an open question in the\nliterature. We also discuss limitations of our results and propose additional\nopen problems.", "authors": ["Kenji Kawaguchi", "Leslie Pack Kaelbling", "Yoshua Bengio"], "category": "stat.ML", "comment": "Extended version: all previous results remain unchanged and new\n  theoretical results were added wit...", "img": "/static/thumbs/1710.05468v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.05468v3", "num_discussion": 0, "originally_published_time": "10/16/2017", "pid": "1710.05468v3", "published_time": "2/22/2018", "rawpid": "1710.05468", "tags": ["stat.ML", "cs.AI", "cs.LG", "cs.NE"], "title": "Generalization in Deep Learning"}, {"abstract": "The prevalent approach to sequence to sequence learning maps an input\nsequence to a variable length output sequence via recurrent neural networks. We\nintroduce an architecture based entirely on convolutional neural networks.\nCompared to recurrent models, computations over all elements can be fully\nparallelized during training and optimization is easier since the number of\nnon-linearities is fixed and independent of the input length. Our use of gated\nlinear units eases gradient propagation and we equip each decoder layer with a\nseparate attention module. We outperform the accuracy of the deep LSTM setup of\nWu et al. (2016) on both WMT\u002714 English-German and WMT\u002714 English-French\ntranslation at an order of magnitude faster speed, both on GPU and CPU.", "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Denis Yarats", "Yann N. Dauphin"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1705.03122v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.03122v3", "num_discussion": 0, "originally_published_time": "5/8/2017", "pid": "1705.03122v3", "published_time": "7/25/2017", "rawpid": "1705.03122", "tags": ["cs.CL"], "title": "Convolutional Sequence to Sequence Learning"}, {"abstract": "Very deep convolutional networks with hundreds of layers have led to\nsignificant reductions in error on competitive benchmarks. Although the\nunmatched expressiveness of the many layers can be highly desirable at test\ntime, training very deep networks comes with its own set of challenges. The\ngradients can vanish, the forward flow often diminishes, and the training time\ncan be painfully slow. To address these problems, we propose stochastic depth,\na training procedure that enables the seemingly contradictory setup to train\nshort networks and use deep networks at test time. We start with very deep\nnetworks but during training, for each mini-batch, randomly drop a subset of\nlayers and bypass them with the identity function. This simple approach\ncomplements the recent success of residual networks. It reduces training time\nsubstantially and improves the test error significantly on almost all data sets\nthat we used for evaluation. With stochastic depth we can increase the depth of\nresidual networks even beyond 1200 layers and still yield meaningful\nimprovements in test error (4.91% on CIFAR-10).", "authors": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "category": "cs.LG", "comment": "first two authors contributed equally", "img": "/static/thumbs/1603.09382v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.09382v3", "num_discussion": 0, "originally_published_time": "3/30/2016", "pid": "1603.09382v3", "published_time": "7/28/2016", "rawpid": "1603.09382", "tags": ["cs.LG", "cs.CV", "cs.NE"], "title": "Deep Networks with Stochastic Depth"}, {"abstract": "Recently there has been a dramatic increase in the performance of recognition\nsystems due to the introduction of deep architectures for representation\nlearning and classification. However, the mathematical reasons for this success\nremain elusive. This tutorial will review recent work that aims to provide a\nmathematical justification for several properties of deep networks, such as\nglobal optimality, geometric stability, and invariance of the learned\nrepresentations.", "authors": ["Rene Vidal", "Joan Bruna", "Raja Giryes", "Stefano Soatto"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1712.04741v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1712.04741v1", "num_discussion": 0, "originally_published_time": "12/13/2017", "pid": "1712.04741v1", "published_time": "12/13/2017", "rawpid": "1712.04741", "tags": ["cs.LG", "cs.CV"], "title": "Mathematics of Deep Learning"}, {"abstract": "Image-to-image translation is a class of vision and graphics problems where\nthe goal is to learn the mapping between an input image and an output image\nusing a training set of aligned image pairs. However, for many tasks, paired\ntraining data will not be available. We present an approach for learning to\ntranslate an image from a source domain $X$ to a target domain $Y$ in the\nabsence of paired examples. Our goal is to learn a mapping $G: X \\rightarrow Y$\nsuch that the distribution of images from $G(X)$ is indistinguishable from the\ndistribution $Y$ using an adversarial loss. Because this mapping is highly\nunder-constrained, we couple it with an inverse mapping $F: Y \\rightarrow X$\nand introduce a cycle consistency loss to push $F(G(X)) \\approx X$ (and vice\nversa). Qualitative results are presented on several tasks where paired\ntraining data does not exist, including collection style transfer, object\ntransfiguration, season transfer, photo enhancement, etc. Quantitative\ncomparisons against several prior methods demonstrate the superiority of our\napproach.", "authors": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A. Efros"], "category": "cs.CV", "comment": "An extended version of our ICCV 2017 paper, v4 updates the\n  implementation details in the appendix", "img": "/static/thumbs/1703.10593v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.10593v4", "num_discussion": 0, "originally_published_time": "3/30/2017", "pid": "1703.10593v4", "published_time": "2/19/2018", "rawpid": "1703.10593", "tags": ["cs.CV"], "title": "Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial\n  Networks"}, {"abstract": "We present the first deep learning model to successfully learn control\npolicies directly from high-dimensional sensory input using reinforcement\nlearning. The model is a convolutional neural network, trained with a variant\nof Q-learning, whose input is raw pixels and whose output is a value function\nestimating future rewards. We apply our method to seven Atari 2600 games from\nthe Arcade Learning Environment, with no adjustment of the architecture or\nlearning algorithm. We find that it outperforms all previous approaches on six\nof the games and surpasses a human expert on three of them.", "authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "category": "cs.LG", "comment": "NIPS Deep Learning Workshop 2013", "img": "/static/thumbs/1312.5602v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1312.5602v1", "num_discussion": 0, "originally_published_time": "12/19/2013", "pid": "1312.5602v1", "published_time": "12/19/2013", "rawpid": "1312.5602", "tags": ["cs.LG"], "title": "Playing Atari with Deep Reinforcement Learning"}, {"abstract": "Batch Normalization (BN) is a milestone technique in the development of deep\nlearning, enabling various networks to train. However, normalizing along the\nbatch dimension introduces problems --- BN\u0027s error increases rapidly when the\nbatch size becomes smaller, caused by inaccurate batch statistics estimation.\nThis limits BN\u0027s usage for training larger models and transferring features to\ncomputer vision tasks including detection, segmentation, and video, which\nrequire small batches constrained by memory consumption. In this paper, we\npresent Group Normalization (GN) as a simple alternative to BN. GN divides the\nchannels into groups and computes within each group the mean and variance for\nnormalization. GN\u0027s computation is independent of batch sizes, and its accuracy\nis stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN\nhas 10.6% lower error than its BN counterpart when using a batch size of 2;\nwhen using typical batch sizes, GN is comparably good with BN and outperforms\nother normalization variants. Moreover, GN can be naturally transferred from\npre-training to fine-tuning. GN can outperform its BN-based counterparts for\nobject detection and segmentation in COCO, and for video classification in\nKinetics, showing that GN can effectively replace the powerful BN in a variety\nof tasks. GN can be easily implemented by a few lines of code in modern\nlibraries.", "authors": ["Yuxin Wu", "Kaiming He"], "category": "cs.CV", "comment": "v3: Update trained-from-scratch results in COCO to 41.0AP. Code and\n  models at\n  https://github.com...", "img": "/static/thumbs/1803.08494v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.08494v3", "num_discussion": 1, "originally_published_time": "3/22/2018", "pid": "1803.08494v3", "published_time": "6/11/2018", "rawpid": "1803.08494", "tags": ["cs.CV", "cs.LG"], "title": "Group Normalization"}, {"abstract": "We extend the capabilities of neural networks by coupling them to external\nmemory resources, which they can interact with by attentional processes. The\ncombined system is analogous to a Turing Machine or Von Neumann architecture\nbut is differentiable end-to-end, allowing it to be efficiently trained with\ngradient descent. Preliminary results demonstrate that Neural Turing Machines\ncan infer simple algorithms such as copying, sorting, and associative recall\nfrom input and output examples.", "authors": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1410.5401v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1410.5401v2", "num_discussion": 0, "originally_published_time": "10/20/2014", "pid": "1410.5401v2", "published_time": "12/10/2014", "rawpid": "1410.5401", "tags": ["cs.NE"], "title": "Neural Turing Machines"}, {"abstract": "Many sequential processing tasks require complex nonlinear transition\nfunctions from one step to the next. However, recurrent neural networks with\n\u0027deep\u0027 transition functions remain difficult to train, even when using Long\nShort-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of\nrecurrent networks based on Gersgorin\u0027s circle theorem that illuminates several\nmodeling and optimization issues and improves our understanding of the LSTM\ncell. Based on this analysis we propose Recurrent Highway Networks, which\nextend the LSTM architecture to allow step-to-step transition depths larger\nthan one. Several language modeling experiments demonstrate that the proposed\narchitecture results in powerful and efficient models. On the Penn Treebank\ncorpus, solely increasing the transition depth from 1 to 10 improves word-level\nperplexity from 90.6 to 65.4 using the same number of parameters. On the larger\nWikipedia datasets for character prediction (text8 and enwik8), RHNs outperform\nall previous results and achieve an entropy of 1.27 bits per character.", "authors": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u00edk", "J\u00fcrgen Schmidhuber"], "category": "cs.LG", "comment": "12 pages, 6 figures, 3 tables", "img": "/static/thumbs/1607.03474v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1607.03474v5", "num_discussion": 0, "originally_published_time": "7/12/2016", "pid": "1607.03474v5", "published_time": "7/4/2017", "rawpid": "1607.03474", "tags": ["cs.LG", "cs.CL", "cs.NE"], "title": "Recurrent Highway Networks"}, {"abstract": "Both convolutional and recurrent operations are building blocks that process\none local neighborhood at a time. In this paper, we present non-local\noperations as a generic family of building blocks for capturing long-range\ndependencies. Inspired by the classical non-local means method in computer\nvision, our non-local operation computes the response at a position as a\nweighted sum of the features at all positions. This building block can be\nplugged into many computer vision architectures. On the task of video\nclassification, even without any bells and whistles, our non-local models can\ncompete or outperform current competition winners on both Kinetics and Charades\ndatasets. In static image recognition, our non-local models improve object\ndetection/segmentation and pose estimation on the COCO suite of tasks. Code is\navailable at https://github.com/facebookresearch/video-nonlocal-net .", "authors": ["Xiaolong Wang", "Ross Girshick", "Abhinav Gupta", "Kaiming He"], "category": "cs.CV", "comment": "CVPR 2018, code is available at:\n  https://github.com/facebookresearch/video-nonlocal-net", "img": "/static/thumbs/1711.07971v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.07971v3", "num_discussion": 0, "originally_published_time": "11/21/2017", "pid": "1711.07971v3", "published_time": "4/13/2018", "rawpid": "1711.07971", "tags": ["cs.CV"], "title": "Non-local Neural Networks"}, {"abstract": "Inspired by recent work in machine translation and object detection, we\nintroduce an attention based model that automatically learns to describe the\ncontent of images. We describe how we can train this model in a deterministic\nmanner using standard backpropagation techniques and stochastically by\nmaximizing a variational lower bound. We also show through visualization how\nthe model is able to automatically learn to fix its gaze on salient objects\nwhile generating the corresponding words in the output sequence. We validate\nthe use of attention with state-of-the-art performance on three benchmark\ndatasets: Flickr8k, Flickr30k and MS COCO.", "authors": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1502.03044v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1502.03044v3", "num_discussion": 2, "originally_published_time": "2/10/2015", "pid": "1502.03044v3", "published_time": "4/19/2016", "rawpid": "1502.03044", "tags": ["cs.LG", "cs.CV"], "title": "Show, Attend and Tell: Neural Image Caption Generation with Visual\n  Attention"}, {"abstract": "Current deep learning models are mostly build upon neural networks, i.e.,\nmultiple layers of parameterized differentiable nonlinear modules that can be\ntrained by backpropagation. In this paper, we explore the possibility of\nbuilding deep models based on non-differentiable modules. We conjecture that\nthe mystery behind the success of deep neural networks owes much to three\ncharacteristics, i.e., layer-by-layer processing, in-model feature\ntransformation and sufficient model complexity. We propose the gcForest\napproach, which generates \\textit{deep forest} holding these characteristics.\nThis is a decision tree ensemble approach, with much less hyper-parameters than\ndeep neural networks, and its model complexity can be automatically determined\nin a data-dependent way. Experiments show that its performance is quite robust\nto hyper-parameter settings, such that in most cases, even across different\ndata from different domains, it is able to get excellent performance by using\nthe same default setting. This study opens the door of deep learning based on\nnon-differentiable modules, and exhibits the possibility of constructing deep\nmodels without using backpropagation.", "authors": ["Zhi-Hua Zhou", "Ji Feng"], "category": "cs.LG", "comment": "IJCAI 2017", "img": "/static/thumbs/1702.08835v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.08835v3", "num_discussion": 4, "originally_published_time": "2/28/2017", "pid": "1702.08835v3", "published_time": "5/14/2018", "rawpid": "1702.08835", "tags": ["cs.LG", "stat.ML"], "title": "Deep Forest"}, {"abstract": "We introduce an extremely computation-efficient CNN architecture named\nShuffleNet, which is designed specially for mobile devices with very limited\ncomputing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new\noperations, pointwise group convolution and channel shuffle, to greatly reduce\ncomputation cost while maintaining accuracy. Experiments on ImageNet\nclassification and MS COCO object detection demonstrate the superior\nperformance of ShuffleNet over other structures, e.g. lower top-1 error\n(absolute 7.8%) than recent MobileNet on ImageNet classification task, under\nthe computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet\nachieves ~13x actual speedup over AlexNet while maintaining comparable\naccuracy.", "authors": ["Xiangyu Zhang", "Xinyu Zhou", "Mengxiao Lin", "Jian Sun"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.01083v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.01083v2", "num_discussion": 0, "originally_published_time": "7/4/2017", "pid": "1707.01083v2", "published_time": "12/7/2017", "rawpid": "1707.01083", "tags": ["cs.CV"], "title": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for\n  Mobile Devices"}, {"abstract": "Despite the breakthroughs in accuracy and speed of single image\nsuper-resolution using faster and deeper convolutional neural networks, one\ncentral problem remains largely unsolved: how do we recover the finer texture\ndetails when we super-resolve at large upscaling factors? The behavior of\noptimization-based super-resolution methods is principally driven by the choice\nof the objective function. Recent work has largely focused on minimizing the\nmean squared reconstruction error. The resulting estimates have high peak\nsignal-to-noise ratios, but they are often lacking high-frequency details and\nare perceptually unsatisfying in the sense that they fail to match the fidelity\nexpected at the higher resolution. In this paper, we present SRGAN, a\ngenerative adversarial network (GAN) for image super-resolution (SR). To our\nknowledge, it is the first framework capable of inferring photo-realistic\nnatural images for 4x upscaling factors. To achieve this, we propose a\nperceptual loss function which consists of an adversarial loss and a content\nloss. The adversarial loss pushes our solution to the natural image manifold\nusing a discriminator network that is trained to differentiate between the\nsuper-resolved images and original photo-realistic images. In addition, we use\na content loss motivated by perceptual similarity instead of similarity in\npixel space. Our deep residual network is able to recover photo-realistic\ntextures from heavily downsampled images on public benchmarks. An extensive\nmean-opinion-score (MOS) test shows hugely significant gains in perceptual\nquality using SRGAN. The MOS scores obtained with SRGAN are closer to those of\nthe original high-resolution images than to those obtained with any\nstate-of-the-art method.", "authors": ["Christian Ledig", "Lucas Theis", "Ferenc Huszar", "Jose Caballero", "Andrew Cunningham", "Alejandro Acosta", "Andrew Aitken", "Alykhan Tejani", "Johannes Totz", "Zehan Wang", "Wenzhe Shi"], "category": "cs.CV", "comment": "19 pages, 15 figures, 2 tables, accepted for oral presentation at\n  CVPR, main paper + some suppleme...", "img": "/static/thumbs/1609.04802v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.04802v5", "num_discussion": 0, "originally_published_time": "9/15/2016", "pid": "1609.04802v5", "published_time": "5/25/2017", "rawpid": "1609.04802", "tags": ["cs.CV", "stat.ML"], "title": "Photo-Realistic Single Image Super-Resolution Using a Generative\n  Adversarial Network"}, {"abstract": "We propose a reparameterization of LSTM that brings the benefits of batch\nnormalization to recurrent neural networks. Whereas previous works only apply\nbatch normalization to the input-to-hidden transformation of RNNs, we\ndemonstrate that it is both possible and beneficial to batch-normalize the\nhidden-to-hidden transition, thereby reducing internal covariate shift between\ntime steps. We evaluate our proposal on various sequential problems such as\nsequence classification, language modeling and question answering. Our\nempirical results show that our batch-normalized LSTM consistently leads to\nfaster convergence and improved generalization.", "authors": ["Tim Cooijmans", "Nicolas Ballas", "C\u00e9sar Laurent", "\u00c7a\u011flar G\u00fcl\u00e7ehre", "Aaron Courville"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1603.09025v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.09025v5", "num_discussion": 0, "originally_published_time": "3/30/2016", "pid": "1603.09025v5", "published_time": "2/28/2017", "rawpid": "1603.09025", "tags": ["cs.LG"], "title": "Recurrent Batch Normalization"}, {"abstract": "Deep learning methods employ multiple processing layers to learn hierarchical\nrepresentations of data, and have produced state-of-the-art results in many\ndomains. Recently, a variety of model designs and methods have blossomed in the\ncontext of natural language processing (NLP). In this paper, we review\nsignificant deep learning related models and methods that have been employed\nfor numerous NLP tasks and provide a walk-through of their evolution. We also\nsummarize, compare and contrast the various models and put forward a detailed\nunderstanding of the past, present and future of deep learning in NLP.", "authors": ["Tom Young", "Devamanyu Hazarika", "Soujanya Poria", "Erik Cambria"], "category": "cs.CL", "comment": "22 pages, 28 figures, Computational Intelligence Magazine 2018", "img": "/static/thumbs/1708.02709v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1708.02709v5", "num_discussion": 0, "originally_published_time": "8/9/2017", "pid": "1708.02709v5", "published_time": "2/20/2018", "rawpid": "1708.02709", "tags": ["cs.CL"], "title": "Recent Trends in Deep Learning Based Natural Language Processing"}, {"abstract": "We present a class of efficient models called MobileNets for mobile and\nembedded vision applications. MobileNets are based on a streamlined\narchitecture that uses depth-wise separable convolutions to build light weight\ndeep neural networks. We introduce two simple global hyper-parameters that\nefficiently trade off between latency and accuracy. These hyper-parameters\nallow the model builder to choose the right sized model for their application\nbased on the constraints of the problem. We present extensive experiments on\nresource and accuracy tradeoffs and show strong performance compared to other\npopular models on ImageNet classification. We then demonstrate the\neffectiveness of MobileNets across a wide range of applications and use cases\nincluding object detection, finegrain classification, face attributes and large\nscale geo-localization.", "authors": ["Andrew G. Howard", "Menglong Zhu", "Bo Chen", "Dmitry Kalenichenko", "Weijun Wang", "Tobias Weyand", "Marco Andreetto", "Hartwig Adam"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.04861v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.04861v1", "num_discussion": 1, "originally_published_time": "4/17/2017", "pid": "1704.04861v1", "published_time": "4/17/2017", "rawpid": "1704.04861", "tags": ["cs.CV"], "title": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision\n  Applications"}, {"abstract": "Neural networks are powerful and flexible models that work well for many\ndifficult learning tasks in image, speech and natural language understanding.\nDespite their success, neural networks are still hard to design. In this paper,\nwe use a recurrent network to generate the model descriptions of neural\nnetworks and train this RNN with reinforcement learning to maximize the\nexpected accuracy of the generated architectures on a validation set. On the\nCIFAR-10 dataset, our method, starting from scratch, can design a novel network\narchitecture that rivals the best human-invented architecture in terms of test\nset accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is\n0.09 percent better and 1.05x faster than the previous state-of-the-art model\nthat used a similar architectural scheme. On the Penn Treebank dataset, our\nmodel can compose a novel recurrent cell that outperforms the widely-used LSTM\ncell, and other state-of-the-art baselines. Our cell achieves a test set\nperplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than\nthe previous state-of-the-art model. The cell can also be transferred to the\ncharacter language modeling task on PTB and achieves a state-of-the-art\nperplexity of 1.214.", "authors": ["Barret Zoph", "Quoc V. Le"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1611.01578v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.01578v2", "num_discussion": 1, "originally_published_time": "11/5/2016", "pid": "1611.01578v2", "published_time": "2/15/2017", "rawpid": "1611.01578", "tags": ["cs.LG", "cs.AI", "cs.NE"], "title": "Neural Architecture Search with Reinforcement Learning"}, {"abstract": "We present a variety of new architectural features and training procedures\nthat we apply to the generative adversarial networks (GANs) framework. We focus\non two applications of GANs: semi-supervised learning, and the generation of\nimages that humans find visually realistic. Unlike most work on generative\nmodels, our primary goal is not to train a model that assigns high likelihood\nto test data, nor do we require the model to be able to learn well without\nusing any labels. Using our new techniques, we achieve state-of-the-art results\nin semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated\nimages are of high quality as confirmed by a visual Turing test: our model\ngenerates MNIST samples that humans cannot distinguish from real data, and\nCIFAR-10 samples that yield a human error rate of 21.3%. We also present\nImageNet samples with unprecedented resolution and show that our methods enable\nthe model to learn recognizable features of ImageNet classes.", "authors": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1606.03498v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.03498v1", "num_discussion": 0, "originally_published_time": "6/10/2016", "pid": "1606.03498v1", "published_time": "6/10/2016", "rawpid": "1606.03498", "tags": ["cs.LG", "cs.CV", "cs.NE"], "title": "Improved Techniques for Training GANs"}, {"abstract": "Deep neural networks (DNNs) are currently widely used for many artificial\nintelligence (AI) applications including computer vision, speech recognition,\nand robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it\ncomes at the cost of high computational complexity. Accordingly, techniques\nthat enable efficient processing of DNNs to improve energy efficiency and\nthroughput without sacrificing application accuracy or increasing hardware cost\nare critical to the wide deployment of DNNs in AI systems.\n  This article aims to provide a comprehensive tutorial and survey about the\nrecent advances towards the goal of enabling efficient processing of DNNs.\nSpecifically, it will provide an overview of DNNs, discuss various hardware\nplatforms and architectures that support DNNs, and highlight key trends in\nreducing the computation cost of DNNs either solely via hardware design changes\nor via joint hardware design and DNN algorithm changes. It will also summarize\nvarious development resources that enable researchers and practitioners to\nquickly get started in this field, and highlight important benchmarking metrics\nand design considerations that should be used for evaluating the rapidly\ngrowing number of DNN hardware designs, optionally including algorithmic\nco-designs, being proposed in academia and industry.\n  The reader will take away the following concepts from this article:\nunderstand the key design considerations for DNNs; be able to evaluate\ndifferent DNN hardware implementations with benchmarks and comparison metrics;\nunderstand the trade-offs between various hardware architectures and platforms;\nbe able to evaluate the utility of various DNN design techniques for efficient\nprocessing; and understand recent implementation trends and opportunities.", "authors": ["Vivienne Sze", "Yu-Hsin Chen", "Tien-Ju Yang", "Joel Emer"], "category": "cs.CV", "comment": "Based on tutorial on DNN Hardware at eyeriss.mit.edu/tutorial.html", "img": "/static/thumbs/1703.09039v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.09039v2", "num_discussion": 0, "originally_published_time": "3/27/2017", "pid": "1703.09039v2", "published_time": "8/13/2017", "rawpid": "1703.09039", "tags": ["cs.CV"], "title": "Efficient Processing of Deep Neural Networks: A Tutorial and Survey"}, {"abstract": "Despite their great success, there is still no comprehensive theoretical\nunderstanding of learning with Deep Neural Networks (DNNs) or their inner\norganization. Previous work proposed to analyze DNNs in the \\textit{Information\nPlane}; i.e., the plane of the Mutual Information values that each layer\npreserves on the input and output variables. They suggested that the goal of\nthe network is to optimize the Information Bottleneck (IB) tradeoff between\ncompression and prediction, successively, for each layer.\n  In this work we follow up on this idea and demonstrate the effectiveness of\nthe Information-Plane visualization of DNNs. Our main results are: (i) most of\nthe training epochs in standard DL are spent on {\\emph compression} of the\ninput to efficient representation and not on fitting the training labels. (ii)\nThe representation compression phase begins when the training errors becomes\nsmall and the Stochastic Gradient Decent (SGD) epochs change from a fast drift\nto smaller training error into a stochastic relaxation, or random diffusion,\nconstrained by the training error value. (iii) The converged layers lie on or\nvery close to the Information Bottleneck (IB) theoretical bound, and the maps\nfrom the input to any hidden layer and from this hidden layer to the output\nsatisfy the IB self-consistent equations. This generalization through noise\nmechanism is unique to Deep Neural Networks and absent in one layer networks.\n(iv) The training time is dramatically reduced when adding more hidden layers.\nThus the main advantage of the hidden layers is computational. This can be\nexplained by the reduced relaxation time, as this it scales super-linearly\n(exponentially for simple diffusion) with the information compression from the\nprevious layer.", "authors": ["Ravid Shwartz-Ziv", "Naftali Tishby"], "category": "cs.LG", "comment": "19 pages, 8 figures", "img": "/static/thumbs/1703.00810v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.00810v3", "num_discussion": 2, "originally_published_time": "3/2/2017", "pid": "1703.00810v3", "published_time": "4/29/2017", "rawpid": "1703.00810", "tags": ["cs.LG"], "title": "Opening the Black Box of Deep Neural Networks via Information"}, {"abstract": "We present region-based, fully convolutional networks for accurate and\nefficient object detection. In contrast to previous region-based detectors such\nas Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of\ntimes, our region-based detector is fully convolutional with almost all\ncomputation shared on the entire image. To achieve this goal, we propose\nposition-sensitive score maps to address a dilemma between\ntranslation-invariance in image classification and translation-variance in\nobject detection. Our method can thus naturally adopt fully convolutional image\nclassifier backbones, such as the latest Residual Networks (ResNets), for\nobject detection. We show competitive results on the PASCAL VOC datasets (e.g.,\n83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is\nachieved at a test-time speed of 170ms per image, 2.5-20x faster than the\nFaster R-CNN counterpart. Code is made publicly available at:\nhttps://github.com/daijifeng001/r-fcn", "authors": ["Jifeng Dai", "Yi Li", "Kaiming He", "Jian Sun"], "category": "cs.CV", "comment": "Tech report", "img": "/static/thumbs/1605.06409v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06409v2", "num_discussion": 0, "originally_published_time": "5/20/2016", "pid": "1605.06409v2", "published_time": "6/21/2016", "rawpid": "1605.06409", "tags": ["cs.CV"], "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks"}, {"abstract": "The highest accuracy object detectors to date are based on a two-stage\napproach popularized by R-CNN, where a classifier is applied to a sparse set of\ncandidate object locations. In contrast, one-stage detectors that are applied\nover a regular, dense sampling of possible object locations have the potential\nto be faster and simpler, but have trailed the accuracy of two-stage detectors\nthus far. In this paper, we investigate why this is the case. We discover that\nthe extreme foreground-background class imbalance encountered during training\nof dense detectors is the central cause. We propose to address this class\nimbalance by reshaping the standard cross entropy loss such that it\ndown-weights the loss assigned to well-classified examples. Our novel Focal\nLoss focuses training on a sparse set of hard examples and prevents the vast\nnumber of easy negatives from overwhelming the detector during training. To\nevaluate the effectiveness of our loss, we design and train a simple dense\ndetector we call RetinaNet. Our results show that when trained with the focal\nloss, RetinaNet is able to match the speed of previous one-stage detectors\nwhile surpassing the accuracy of all existing state-of-the-art two-stage\ndetectors. Code is at: https://github.com/facebookresearch/Detectron.", "authors": ["Tsung-Yi Lin", "Priya Goyal", "Ross Girshick", "Kaiming He", "Piotr Doll\u00e1r"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1708.02002v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1708.02002v2", "num_discussion": 0, "originally_published_time": "8/7/2017", "pid": "1708.02002v2", "published_time": "2/7/2018", "rawpid": "1708.02002", "tags": ["cs.CV"], "title": "Focal Loss for Dense Object Detection"}, {"abstract": "The capacity of a neural network to absorb information is limited by its\nnumber of parameters. Conditional computation, where parts of the network are\nactive on a per-example basis, has been proposed in theory as a way of\ndramatically increasing model capacity without a proportional increase in\ncomputation. In practice, however, there are significant algorithmic and\nperformance challenges. In this work, we address these challenges and finally\nrealize the promise of conditional computation, achieving greater than 1000x\nimprovements in model capacity with only minor losses in computational\nefficiency on modern GPU clusters. We introduce a Sparsely-Gated\nMixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward\nsub-networks. A trainable gating network determines a sparse combination of\nthese experts to use for each example. We apply the MoE to the tasks of\nlanguage modeling and machine translation, where model capacity is critical for\nabsorbing the vast quantities of knowledge available in the training corpora.\nWe present model architectures in which a MoE with up to 137 billion parameters\nis applied convolutionally between stacked LSTM layers. On large language\nmodeling and machine translation benchmarks, these models achieve significantly\nbetter results than state-of-the-art at lower computational cost.", "authors": ["Noam Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc Le", "Geoffrey Hinton", "Jeff Dean"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1701.06538v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.06538v1", "num_discussion": 0, "originally_published_time": "1/23/2017", "pid": "1701.06538v1", "published_time": "1/23/2017", "rawpid": "1701.06538", "tags": ["cs.LG", "cs.CL", "cs.NE", "stat.ML"], "title": "Outrageously Large Neural Networks: The Sparsely-Gated\n  Mixture-of-Experts Layer"}, {"abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT\u0027s use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google\u0027s\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT\u002714\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google\u0027s phrase-based production system.", "authors": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey", "Jeff Klingner", "Apurva Shah", "Melvin Johnson", "Xiaobing Liu", "\u0141ukasz Kaiser", "Stephan Gouws", "Yoshikiyo Kato", "Taku Kudo", "Hideto Kazawa", "Keith Stevens", "George Kurian", "Nishant Patil", "Wei Wang", "Cliff Young", "Jason Smith", "Jason Riesa", "Alex Rudnick", "Oriol Vinyals", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1609.08144v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.08144v2", "num_discussion": 0, "originally_published_time": "9/26/2016", "pid": "1609.08144v2", "published_time": "10/8/2016", "rawpid": "1609.08144", "tags": ["cs.CL", "cs.AI", "cs.LG"], "title": "Google\u0027s Neural Machine Translation System: Bridging the Gap between\n  Human and Machine Translation"}, {"abstract": "We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either.", "authors": ["Phillip Isola", "Jun-Yan Zhu", "Tinghui Zhou", "Alexei A. Efros"], "category": "cs.CV", "comment": "Website: https://phillipi.github.io/pix2pix/", "img": "/static/thumbs/1611.07004v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.07004v2", "num_discussion": 0, "originally_published_time": "11/21/2016", "pid": "1611.07004v2", "published_time": "11/22/2017", "rawpid": "1611.07004", "tags": ["cs.CV"], "title": "Image-to-Image Translation with Conditional Adversarial Networks"}, {"abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by\ndirectly maximising cumulative reward. However, environments contain a much\nwider variety of possible training signals. In this paper, we introduce an\nagent that also maximises many other pseudo-reward functions simultaneously by\nreinforcement learning. All of these tasks share a common representation that,\nlike unsupervised learning, continues to develop in the absence of extrinsic\nrewards. We also introduce a novel mechanism for focusing this representation\nupon extrinsic rewards, so that learning can rapidly adapt to the most relevant\naspects of the actual task. Our agent significantly outperforms the previous\nstate-of-the-art on Atari, averaging 880\\% expert human performance, and a\nchallenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks\nleading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert\nhuman performance on Labyrinth.", "authors": ["Max Jaderberg", "Volodymyr Mnih", "Wojciech Marian Czarnecki", "Tom Schaul", "Joel Z Leibo", "David Silver", "Koray Kavukcuoglu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1611.05397v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.05397v1", "num_discussion": 0, "originally_published_time": "11/16/2016", "pid": "1611.05397v1", "published_time": "11/16/2016", "rawpid": "1611.05397", "tags": ["cs.LG", "cs.NE"], "title": "Reinforcement Learning with Unsupervised Auxiliary Tasks"}, {"abstract": "Learning from a few examples remains a key challenge in machine learning.\nDespite recent advances in important domains such as vision and language, the\nstandard supervised deep learning paradigm does not offer a satisfactory\nsolution for learning new concepts rapidly from little data. In this work, we\nemploy ideas from metric learning based on deep neural features and from recent\nadvances that augment neural networks with external memories. Our framework\nlearns a network that maps a small labelled support set and an unlabelled\nexample to its label, obviating the need for fine-tuning to adapt to new class\ntypes. We then define one-shot learning problems on vision (using Omniglot,\nImageNet) and language tasks. Our algorithm improves one-shot accuracy on\nImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to\ncompeting approaches. We also demonstrate the usefulness of the same model on\nlanguage modeling by introducing a one-shot task on the Penn Treebank.", "authors": ["Oriol Vinyals", "Charles Blundell", "Timothy Lillicrap", "Koray Kavukcuoglu", "Daan Wierstra"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1606.04080v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.04080v2", "num_discussion": 2, "originally_published_time": "6/13/2016", "pid": "1606.04080v2", "published_time": "12/29/2017", "rawpid": "1606.04080", "tags": ["cs.LG", "stat.ML"], "title": "Matching Networks for One Shot Learning"}, {"abstract": "Until recently, research on artificial neural networks was largely restricted\nto systems with only two types of variable: Neural activities that represent\nthe current or recent input and weights that learn to capture regularities\namong inputs, outputs and payoffs. There is no good reason for this\nrestriction. Synapses have dynamics at many different time-scales and this\nsuggests that artificial neural networks might benefit from variables that\nchange slower than activities but much faster than the standard weights. These\n\"fast weights\" can be used to store temporary memories of the recent past and\nthey provide a neurally plausible way of implementing the type of attention to\nthe past that has recently proved very helpful in sequence-to-sequence models.\nBy using fast weights we can avoid the need to store copies of neural activity\npatterns.", "authors": ["Jimmy Ba", "Geoffrey Hinton", "Volodymyr Mnih", "Joel Z. Leibo", "Catalin Ionescu"], "category": "stat.ML", "comment": "Added [Schmidhuber 1993] citation to the last paragraph of the\n  introduction. Fixed typo appendix A...", "img": "/static/thumbs/1610.06258v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.06258v3", "num_discussion": 0, "originally_published_time": "10/20/2016", "pid": "1610.06258v3", "published_time": "12/5/2016", "rawpid": "1610.06258", "tags": ["stat.ML", "cs.LG", "cs.NE"], "title": "Using Fast Weights to Attend to the Recent Past"}, {"abstract": "How can we perform efficient inference and learning in directed probabilistic\nmodels, in the presence of continuous latent variables with intractable\nposterior distributions, and large datasets? We introduce a stochastic\nvariational inference and learning algorithm that scales to large datasets and,\nunder some mild differentiability conditions, even works in the intractable\ncase. Our contributions is two-fold. First, we show that a reparameterization\nof the variational lower bound yields a lower bound estimator that can be\nstraightforwardly optimized using standard stochastic gradient methods. Second,\nwe show that for i.i.d. datasets with continuous latent variables per\ndatapoint, posterior inference can be made especially efficient by fitting an\napproximate inference model (also called a recognition model) to the\nintractable posterior using the proposed lower bound estimator. Theoretical\nadvantages are reflected in experimental results.", "authors": ["Diederik P Kingma", "Max Welling"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1312.6114v10.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1312.6114v10", "num_discussion": 0, "originally_published_time": "12/20/2013", "pid": "1312.6114v10", "published_time": "5/1/2014", "rawpid": "1312.6114", "tags": ["stat.ML", "cs.LG"], "title": "Auto-Encoding Variational Bayes"}, {"abstract": "Common recurrent neural network architectures scale poorly due to the\nintrinsic difficulty in parallelizing their state computations. In this work,\nwe propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that\nsimplifies the computation and exposes more parallelism. In SRU, the majority\nof computation for each step is independent of the recurrence and can be easily\nparallelized. SRU is as fast as a convolutional layer and 5-10x faster than an\noptimized LSTM implementation. We study SRUs on a wide range of applications,\nincluding classification, question answering, language modeling, translation\nand speech recognition. Our experiments demonstrate the effectiveness of SRU\nand the trade-off it enables between speed and performance. We open source our\nimplementation in PyTorch and CNTK.", "authors": ["Tao Lei", "Yu Zhang", "Yoav Artzi"], "category": "cs.CL", "comment": "submission version", "img": "/static/thumbs/1709.02755v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.02755v4", "num_discussion": 0, "originally_published_time": "9/8/2017", "pid": "1709.02755v4", "published_time": "12/26/2017", "rawpid": "1709.02755", "tags": ["cs.CL", "cs.NE"], "title": "Training RNNs as Fast as CNNs"}, {"abstract": "Deep learning thrives with large neural networks and large datasets. However,\nlarger networks and larger datasets result in longer training times that impede\nresearch and development progress. Distributed synchronous SGD offers a\npotential solution to this problem by dividing SGD minibatches over a pool of\nparallel workers. Yet to make this scheme efficient, the per-worker workload\nmust be large, which implies nontrivial growth in the SGD minibatch size. In\nthis paper, we empirically show that on the ImageNet dataset large minibatches\ncause optimization difficulties, but when these are addressed the trained\nnetworks exhibit good generalization. Specifically, we show no loss of accuracy\nwhen training with large minibatch sizes up to 8192 images. To achieve this\nresult, we adopt a hyper-parameter-free linear scaling rule for adjusting\nlearning rates as a function of minibatch size and develop a new warmup scheme\nthat overcomes optimization challenges early in training. With these simple\ntechniques, our Caffe2-based system trains ResNet-50 with a minibatch size of\n8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using\ncommodity hardware, our implementation achieves ~90% scaling efficiency when\nmoving from 8 to 256 GPUs. Our findings enable training visual recognition\nmodels on internet-scale data with high efficiency.", "authors": ["Priya Goyal", "Piotr Doll\u00e1r", "Ross Girshick", "Pieter Noordhuis", "Lukasz Wesolowski", "Aapo Kyrola", "Andrew Tulloch", "Yangqing Jia", "Kaiming He"], "category": "cs.CV", "comment": "Tech report (v2: correct typos)", "img": "/static/thumbs/1706.02677v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.02677v2", "num_discussion": 0, "originally_published_time": "6/8/2017", "pid": "1706.02677v2", "published_time": "4/30/2018", "rawpid": "1706.02677", "tags": ["cs.CV", "cs.DC", "cs.LG"], "title": "Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"}, {"abstract": "A very simple way to improve the performance of almost any machine learning\nalgorithm is to train many different models on the same data and then to\naverage their predictions. Unfortunately, making predictions using a whole\nensemble of models is cumbersome and may be too computationally expensive to\nallow deployment to a large number of users, especially if the individual\nmodels are large neural nets. Caruana and his collaborators have shown that it\nis possible to compress the knowledge in an ensemble into a single model which\nis much easier to deploy and we develop this approach further using a different\ncompression technique. We achieve some surprising results on MNIST and we show\nthat we can significantly improve the acoustic model of a heavily used\ncommercial system by distilling the knowledge in an ensemble of models into a\nsingle model. We also introduce a new type of ensemble composed of one or more\nfull models and many specialist models which learn to distinguish fine-grained\nclasses that the full models confuse. Unlike a mixture of experts, these\nspecialist models can be trained rapidly and in parallel.", "authors": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "category": "stat.ML", "comment": "NIPS 2014 Deep Learning Workshop", "img": "/static/thumbs/1503.02531v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1503.02531v1", "num_discussion": 0, "originally_published_time": "3/9/2015", "pid": "1503.02531v1", "published_time": "3/9/2015", "rawpid": "1503.02531", "tags": ["stat.ML", "cs.LG", "cs.NE"], "title": "Distilling the Knowledge in a Neural Network"}, {"abstract": "In recent years, Deep Learning has become the go-to solution for a broad\nrange of applications, often outperforming state-of-the-art. However, it is\nimportant, for both theoreticians and practitioners, to gain a deeper\nunderstanding of the difficulties and limitations associated with common\napproaches and algorithms. We describe four types of simple problems, for which\nthe gradient-based algorithms commonly used in deep learning either fail or\nsuffer from significant difficulties. We illustrate the failures through\npractical experiments, and provide theoretical insights explaining their\nsource, and how they might be remedied.", "authors": ["Shai Shalev-Shwartz", "Ohad Shamir", "Shaked Shammah"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1703.07950v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.07950v2", "num_discussion": 0, "originally_published_time": "3/23/2017", "pid": "1703.07950v2", "published_time": "4/26/2017", "rawpid": "1703.07950", "tags": ["cs.LG"], "title": "Failures of Gradient-Based Deep Learning"}, {"abstract": "This paper is a review of the evolutionary history of deep learning models.\nIt covers from the genesis of neural networks when associationism modeling of\nthe brain is studied, to the models that dominate the last decade of research\nin deep learning like convolutional neural networks, deep belief networks, and\nrecurrent neural networks. In addition to a review of these models, this paper\nprimarily focuses on the precedents of the models above, examining how the\ninitial ideas are assembled to construct the early models and how these\npreliminary models are developed into their current forms. Many of these\nevolutionary paths last more than half a century and have a diversity of\ndirections. For example, CNN is built on prior knowledge of biological vision\nsystem; DBN is evolved from a trade-off of modeling power and computation\ncomplexity of graphical models and many nowadays models are neural counterparts\nof ancient linear models. This paper reviews these evolutionary paths and\noffers a concise thought flow of how these models are developed, and aims to\nprovide a thorough background for deep learning. More importantly, along with\nthe path, this paper summarizes the gist behind these milestones and proposes\nmany directions to guide the future research of deep learning.", "authors": ["Haohan Wang", "Bhiksha Raj"], "category": "cs.LG", "comment": "70 pages, 200 references", "img": "/static/thumbs/1702.07800v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.07800v4", "num_discussion": 0, "originally_published_time": "2/24/2017", "pid": "1702.07800v4", "published_time": "3/3/2017", "rawpid": "1702.07800", "tags": ["cs.LG", "cs.NE", "stat.ML"], "title": "On the Origin of Deep Learning"}, {"abstract": "Although deep learning has historical roots going back decades, neither the\nterm \"deep learning\" nor the approach was popular just over five years ago,\nwhen the field was reignited by papers such as Krizhevsky, Sutskever and\nHinton\u0027s now classic (2012) deep network model of Imagenet. What has the field\ndiscovered in the five subsequent years? Against a background of considerable\nprogress in areas such as speech recognition, image recognition, and game\nplaying, and considerable enthusiasm in the popular press, I present ten\nconcerns for deep learning, and suggest that deep learning must be supplemented\nby other techniques if we are to reach artificial general intelligence.", "authors": ["Gary Marcus"], "category": "cs.AI", "comment": "1 figure", "img": "/static/thumbs/1801.00631v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1801.00631v1", "num_discussion": 0, "originally_published_time": "1/2/2018", "pid": "1801.00631v1", "published_time": "1/2/2018", "rawpid": "1801.00631", "tags": ["cs.AI", "97R40", "I.2.0; I.2.6"], "title": "Deep Learning: A Critical Appraisal"}, {"abstract": "Neural network training relies on our ability to find \"good\" minimizers of\nhighly non-convex loss functions. It is well known that certain network\narchitecture designs (e.g., skip connections) produce loss functions that train\neasier, and well-chosen training parameters (batch size, learning rate,\noptimizer) produce minimizers that generalize better. However, the reasons for\nthese differences, and their effect on the underlying loss landscape, is not\nwell understood. In this paper, we explore the structure of neural loss\nfunctions, and the effect of loss landscapes on generalization, using a range\nof visualization methods. First, we introduce a simple \"filter normalization\"\nmethod that helps us visualize loss function curvature, and make meaningful\nside-by-side comp arisons between loss functions. Then, using a variety of\nvisualizations, we explore how network architecture affects the loss landscape,\nand how training parameters affect the shape of minimizers.", "authors": ["Hao Li", "Zheng Xu", "Gavin Taylor", "Christoph Studer", "Tom Goldstein"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1712.09913v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1712.09913v2", "num_discussion": 0, "originally_published_time": "12/28/2017", "pid": "1712.09913v2", "published_time": "3/5/2018", "rawpid": "1712.09913", "tags": ["cs.LG", "cs.CV", "stat.ML"], "title": "Visualizing the Loss Landscape of Neural Nets"}, {"abstract": "The game of chess is the most widely-studied domain in the history of\nartificial intelligence. The strongest programs are based on a combination of\nsophisticated search techniques, domain-specific adaptations, and handcrafted\nevaluation functions that have been refined by human experts over several\ndecades. In contrast, the AlphaGo Zero program recently achieved superhuman\nperformance in the game of Go, by tabula rasa reinforcement learning from games\nof self-play. In this paper, we generalise this approach into a single\nAlphaZero algorithm that can achieve, tabula rasa, superhuman performance in\nmany challenging domains. Starting from random play, and given no domain\nknowledge except the game rules, AlphaZero achieved within 24 hours a\nsuperhuman level of play in the games of chess and shogi (Japanese chess) as\nwell as Go, and convincingly defeated a world-champion program in each case.", "authors": ["David Silver", "Thomas Hubert", "Julian Schrittwieser", "Ioannis Antonoglou", "Matthew Lai", "Arthur Guez", "Marc Lanctot", "Laurent Sifre", "Dharshan Kumaran", "Thore Graepel", "Timothy Lillicrap", "Karen Simonyan", "Demis Hassabis"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1712.01815v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1712.01815v1", "num_discussion": 1, "originally_published_time": "12/5/2017", "pid": "1712.01815v1", "published_time": "12/5/2017", "rawpid": "1712.01815", "tags": ["cs.AI"], "title": "Mastering Chess and Shogi by Self-Play with a General Reinforcement\n  Learning Algorithm"}, {"abstract": "We explore building generative neural network models of popular reinforcement\nlearning environments. Our world model can be trained quickly in an\nunsupervised manner to learn a compressed spatial and temporal representation\nof the environment. By using features extracted from the world model as inputs\nto an agent, we can train a very compact and simple policy that can solve the\nrequired task. We can even train our agent entirely inside of its own\nhallucinated dream generated by its world model, and transfer this policy back\ninto the actual environment.\n  An interactive version of this paper is available at\nhttps://worldmodels.github.io/", "authors": ["David Ha", "J\u00fcrgen Schmidhuber"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1803.10122v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.10122v4", "num_discussion": 0, "originally_published_time": "3/27/2018", "pid": "1803.10122v4", "published_time": "5/9/2018", "rawpid": "1803.10122", "tags": ["cs.LG", "stat.ML"], "title": "World Models"}, {"abstract": "Feature pyramids are a basic component in recognition systems for detecting\nobjects at different scales. But recent deep learning object detectors have\navoided pyramid representations, in part because they are compute and memory\nintensive. In this paper, we exploit the inherent multi-scale, pyramidal\nhierarchy of deep convolutional networks to construct feature pyramids with\nmarginal extra cost. A top-down architecture with lateral connections is\ndeveloped for building high-level semantic feature maps at all scales. This\narchitecture, called a Feature Pyramid Network (FPN), shows significant\nimprovement as a generic feature extractor in several applications. Using FPN\nin a basic Faster R-CNN system, our method achieves state-of-the-art\nsingle-model results on the COCO detection benchmark without bells and\nwhistles, surpassing all existing single-model entries including those from the\nCOCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU\nand thus is a practical and accurate solution to multi-scale object detection.\nCode will be made publicly available.", "authors": ["Tsung-Yi Lin", "Piotr Doll\u00e1r", "Ross Girshick", "Kaiming He", "Bharath Hariharan", "Serge Belongie"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1612.03144v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.03144v2", "num_discussion": 2, "originally_published_time": "12/9/2016", "pid": "1612.03144v2", "published_time": "4/19/2017", "rawpid": "1612.03144", "tags": ["cs.CV"], "title": "Feature Pyramid Networks for Object Detection"}, {"abstract": "Generative adversarial networks (GANs) can implicitly learn rich\ndistributions over images, audio, and data which are hard to model with an\nexplicit likelihood. We present a practical Bayesian formulation for\nunsupervised and semi-supervised learning with GANs. Within this framework, we\nuse stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of\nthe generator and discriminator networks. The resulting approach is\nstraightforward and obtains good performance without any standard interventions\nsuch as feature matching, or mini-batch discrimination. By exploring an\nexpressive posterior over the parameters of the generator, the Bayesian GAN\navoids mode-collapse, produces interpretable and diverse candidate samples, and\nprovides state-of-the-art quantitative results for semi-supervised learning on\nbenchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,\nWasserstein GANs, and DCGAN ensembles.", "authors": ["Yunus Saatchi", "Andrew Gordon Wilson"], "category": "stat.ML", "comment": "Updated to the version that appears at Advances in Neural Information\n  Processing Systems 30 (NIPS)...", "img": "/static/thumbs/1705.09558v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.09558v3", "num_discussion": 0, "originally_published_time": "5/26/2017", "pid": "1705.09558v3", "published_time": "11/8/2017", "rawpid": "1705.09558", "tags": ["stat.ML", "cs.AI", "cs.CV", "cs.LG"], "title": "Bayesian GAN"}, {"abstract": "In recent years deep reinforcement learning (RL) systems have attained\nsuperhuman performance in a number of challenging task domains. However, a\nmajor limitation of such applications is their demand for massive amounts of\ntraining data. A critical present objective is thus to develop deep RL methods\nthat can adapt rapidly to new tasks. In the present work we introduce a novel\napproach to this challenge, which we refer to as deep meta-reinforcement\nlearning. Previous work has shown that recurrent networks can support\nmeta-learning in a fully supervised context. We extend this approach to the RL\nsetting. What emerges is a system that is trained using one RL algorithm, but\nwhose recurrent dynamics implement a second, quite separate RL procedure. This\nsecond, learned RL algorithm can differ from the original one in arbitrary\nways. Importantly, because it is learned, it is configured to exploit structure\nin the training domain. We unpack these points in a series of seven\nproof-of-concept experiments, each of which examines a key aspect of deep\nmeta-RL. We consider prospects for extending and scaling up the approach, and\nalso point out some potentially important implications for neuroscience.", "authors": ["Jane X Wang", "Zeb Kurth-Nelson", "Dhruva Tirumala", "Hubert Soyer", "Joel Z Leibo", "Remi Munos", "Charles Blundell", "Dharshan Kumaran", "Matt Botvinick"], "category": "cs.LG", "comment": "17 pages, 7 figures, 1 table", "img": "/static/thumbs/1611.05763v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.05763v3", "num_discussion": 0, "originally_published_time": "11/17/2016", "pid": "1611.05763v3", "published_time": "1/23/2017", "rawpid": "1611.05763", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "Learning to reinforcement learn"}, {"abstract": "The choice of activation functions in deep networks has a significant effect\non the training dynamics and task performance. Currently, the most successful\nand widely-used activation function is the Rectified Linear Unit (ReLU).\nAlthough various hand-designed alternatives to ReLU have been proposed, none\nhave managed to replace it due to inconsistent gains. In this work, we propose\nto leverage automatic search techniques to discover new activation functions.\nUsing a combination of exhaustive and reinforcement learning-based search, we\ndiscover multiple novel activation functions. We verify the effectiveness of\nthe searches by conducting an empirical evaluation with the best discovered\nactivation function. Our experiments show that the best discovered activation\nfunction, $f(x) = x \\cdot \\text{sigmoid}(\\beta x)$, which we name Swish, tends\nto work better than ReLU on deeper models across a number of challenging\ndatasets. For example, simply replacing ReLUs with Swish units improves top-1\nclassification accuracy on ImageNet by 0.9\\% for Mobile NASNet-A and 0.6\\% for\nInception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it\neasy for practitioners to replace ReLUs with Swish units in any neural network.", "authors": ["Prajit Ramachandran", "Barret Zoph", "Quoc V. Le"], "category": "cs.NE", "comment": "Updated version of \"Swish: a Self-Gated Activation Function\"", "img": "/static/thumbs/1710.05941v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.05941v2", "num_discussion": 0, "originally_published_time": "10/16/2017", "pid": "1710.05941v2", "published_time": "10/27/2017", "rawpid": "1710.05941", "tags": ["cs.NE", "cs.CV", "cs.LG"], "title": "Searching for Activation Functions"}, {"abstract": "We introduce a guide to help deep learning practitioners understand and\nmanipulate convolutional neural network architectures. The guide clarifies the\nrelationship between various properties (input shape, kernel shape, zero\npadding, strides and output shape) of convolutional, pooling and transposed\nconvolutional layers, as well as the relationship between convolutional and\ntransposed convolutional layers. Relationships are derived for various cases,\nand are illustrated in order to make them intuitive.", "authors": ["Vincent Dumoulin", "Francesco Visin"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1603.07285v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.07285v2", "num_discussion": 0, "originally_published_time": "3/23/2016", "pid": "1603.07285v2", "published_time": "1/11/2018", "rawpid": "1603.07285", "tags": ["stat.ML", "cs.LG", "cs.NE"], "title": "A guide to convolution arithmetic for deep learning"}, {"abstract": "This paper introduces a deep-learning approach to photographic style transfer\nthat handles a large variety of image content while faithfully transferring the\nreference style. Our approach builds upon the recent work on painterly transfer\nthat separates style from the content of an image by considering different\nlayers of a neural network. However, as is, this approach is not suitable for\nphotorealistic style transfer. Even when both the input and reference images\nare photographs, the output still exhibits distortions reminiscent of a\npainting. Our contribution is to constrain the transformation from the input to\nthe output to be locally affine in colorspace, and to express this constraint\nas a custom fully differentiable energy term. We show that this approach\nsuccessfully suppresses distortion and yields satisfying photorealistic style\ntransfers in a broad variety of scenarios, including transfer of the time of\nday, weather, season, and artistic edits.", "authors": ["Fujun Luan", "Sylvain Paris", "Eli Shechtman", "Kavita Bala"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1703.07511v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.07511v3", "num_discussion": 0, "originally_published_time": "3/22/2017", "pid": "1703.07511v3", "published_time": "4/11/2017", "rawpid": "1703.07511", "tags": ["cs.CV"], "title": "Deep Photo Style Transfer"}, {"abstract": "Training directed neural networks typically requires forward-propagating data\nthrough a computation graph, followed by backpropagating error signal, to\nproduce weight updates. All layers, or more generally, modules, of the network\nare therefore locked, in the sense that they must wait for the remainder of the\nnetwork to execute forwards and propagate error backwards before they can be\nupdated. In this work we break this constraint by decoupling modules by\nintroducing a model of the future computation of the network graph. These\nmodels predict what the result of the modelled subgraph will produce using only\nlocal information. In particular we focus on modelling error gradients: by\nusing the modelled synthetic gradient in place of true backpropagated error\ngradients we decouple subgraphs, and can update them independently and\nasynchronously i.e. we realise decoupled neural interfaces. We show results for\nfeed-forward models, where every layer is trained asynchronously, recurrent\nneural networks (RNNs) where predicting one\u0027s future gradient extends the time\nover which the RNN can effectively model, and also a hierarchical RNN system\nwith ticking at different timescales. Finally, we demonstrate that in addition\nto predicting gradients, the same framework can be used to predict inputs,\nresulting in models which are decoupled in both the forward and backwards pass\n-- amounting to independent networks which co-learn such that they can be\ncomposed into a single functioning corporation.", "authors": ["Max Jaderberg", "Wojciech Marian Czarnecki", "Simon Osindero", "Oriol Vinyals", "Alex Graves", "David Silver", "Koray Kavukcuoglu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1608.05343v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.05343v2", "num_discussion": 0, "originally_published_time": "8/18/2016", "pid": "1608.05343v2", "published_time": "7/3/2017", "rawpid": "1608.05343", "tags": ["cs.LG"], "title": "Decoupled Neural Interfaces using Synthetic Gradients"}, {"abstract": "In this paper, we propose the \"adversarial autoencoder\" (AAE), which is a\nprobabilistic autoencoder that uses the recently proposed generative\nadversarial networks (GAN) to perform variational inference by matching the\naggregated posterior of the hidden code vector of the autoencoder with an\narbitrary prior distribution. Matching the aggregated posterior to the prior\nensures that generating from any part of prior space results in meaningful\nsamples. As a result, the decoder of the adversarial autoencoder learns a deep\ngenerative model that maps the imposed prior to the data distribution. We show\nhow the adversarial autoencoder can be used in applications such as\nsemi-supervised classification, disentangling style and content of images,\nunsupervised clustering, dimensionality reduction and data visualization. We\nperformed experiments on MNIST, Street View House Numbers and Toronto Face\ndatasets and show that adversarial autoencoders achieve competitive results in\ngenerative modeling and semi-supervised classification tasks.", "authors": ["Alireza Makhzani", "Jonathon Shlens", "Navdeep Jaitly", "Ian Goodfellow", "Brendan Frey"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1511.05644v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.05644v2", "num_discussion": 0, "originally_published_time": "11/18/2015", "pid": "1511.05644v2", "published_time": "5/25/2016", "rawpid": "1511.05644", "tags": ["cs.LG"], "title": "Adversarial Autoencoders"}, {"abstract": "One of the core problems of modern statistics is to approximate\ndifficult-to-compute probability densities. This problem is especially\nimportant in Bayesian statistics, which frames all inference about unknown\nquantities as a calculation involving the posterior density. In this paper, we\nreview variational inference (VI), a method from machine learning that\napproximates probability densities through optimization. VI has been used in\nmany applications and tends to be faster than classical methods, such as Markov\nchain Monte Carlo sampling. The idea behind VI is to first posit a family of\ndensities and then to find the member of that family which is close to the\ntarget. Closeness is measured by Kullback-Leibler divergence. We review the\nideas behind mean-field variational inference, discuss the special case of VI\napplied to exponential family models, present a full example with a Bayesian\nmixture of Gaussians, and derive a variant that uses stochastic optimization to\nscale up to massive data. We discuss modern research in VI and highlight\nimportant open problems. VI is powerful, but it is not yet well understood. Our\nhope in writing this paper is to catalyze statistical research on this class of\nalgorithms.", "authors": ["David M. Blei", "Alp Kucukelbir", "Jon D. McAuliffe"], "category": "stat.CO", "comment": "", "img": "/static/thumbs/1601.00670v9.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1601.00670v9", "num_discussion": 0, "originally_published_time": "1/4/2016", "pid": "1601.00670v9", "published_time": "5/9/2018", "rawpid": "1601.00670", "tags": ["stat.CO", "cs.LG", "stat.ML"], "title": "Variational Inference: A Review for Statisticians"}, {"abstract": "In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery.", "authors": ["Leon A. Gatys", "Alexander S. Ecker", "Matthias Bethge"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1508.06576v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1508.06576v2", "num_discussion": 0, "originally_published_time": "8/26/2015", "pid": "1508.06576v2", "published_time": "9/2/2015", "rawpid": "1508.06576", "tags": ["cs.CV", "cs.NE", "q-bio.NC"], "title": "A Neural Algorithm of Artistic Style"}, {"abstract": "This report is targeted to groups who are subject matter experts in their\napplication but deep learning novices. It contains practical advice for those\ninterested in testing the use of deep neural networks on applications that are\nnovel for deep learning. We suggest making your project more manageable by\ndividing it into phases. For each phase this report contains numerous\nrecommendations and insights to assist novice practitioners.", "authors": ["Leslie N. Smith"], "category": "cs.SE", "comment": "", "img": "/static/thumbs/1704.01568v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.01568v1", "num_discussion": 0, "originally_published_time": "4/5/2017", "pid": "1704.01568v1", "published_time": "4/5/2017", "rawpid": "1704.01568", "tags": ["cs.SE", "cs.AI", "cs.NE"], "title": "Best Practices for Applying Deep Learning to Novel Applications"}, {"abstract": "This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data.", "authors": ["Alex Graves"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1603.08983v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.08983v6", "num_discussion": 0, "originally_published_time": "3/29/2016", "pid": "1603.08983v6", "published_time": "2/21/2017", "rawpid": "1603.08983", "tags": ["cs.NE"], "title": "Adaptive Computation Time for Recurrent Neural Networks"}, {"abstract": "Recurrent Neural Networks (RNNs), and specifically a variant with Long\nShort-Term Memory (LSTM), are enjoying renewed interest as a result of\nsuccessful applications in a wide range of machine learning problems that\ninvolve sequential data. However, while LSTMs provide exceptional results in\npractice, the source of their performance and their limitations remain rather\npoorly understood. Using character-level language models as an interpretable\ntestbed, we aim to bridge this gap by providing an analysis of their\nrepresentations, predictions and error types. In particular, our experiments\nreveal the existence of interpretable cells that keep track of long-range\ndependencies such as line lengths, quotes and brackets. Moreover, our\ncomparative analysis with finite horizon n-gram models traces the source of the\nLSTM improvements to long-range structural dependencies. Finally, we provide\nanalysis of the remaining errors and suggests areas for further study.", "authors": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei"], "category": "cs.LG", "comment": "changing style, adding references, minor changes to text", "img": "/static/thumbs/1506.02078v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.02078v2", "num_discussion": 0, "originally_published_time": "6/5/2015", "pid": "1506.02078v2", "published_time": "11/17/2015", "rawpid": "1506.02078", "tags": ["cs.LG", "cs.CL", "cs.NE"], "title": "Visualizing and Understanding Recurrent Networks"}, {"abstract": "This paper describes InfoGAN, an information-theoretic extension to the\nGenerative Adversarial Network that is able to learn disentangled\nrepresentations in a completely unsupervised manner. InfoGAN is a generative\nadversarial network that also maximizes the mutual information between a small\nsubset of the latent variables and the observation. We derive a lower bound to\nthe mutual information objective that can be optimized efficiently, and show\nthat our training procedure can be interpreted as a variation of the Wake-Sleep\nalgorithm. Specifically, InfoGAN successfully disentangles writing styles from\ndigit shapes on the MNIST dataset, pose from lighting of 3D rendered images,\nand background digits from the central digit on the SVHN dataset. It also\ndiscovers visual concepts that include hair styles, presence/absence of\neyeglasses, and emotions on the CelebA face dataset. Experiments show that\nInfoGAN learns interpretable representations that are competitive with\nrepresentations learned by existing fully supervised methods.", "authors": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1606.03657v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.03657v1", "num_discussion": 0, "originally_published_time": "6/12/2016", "pid": "1606.03657v1", "published_time": "6/12/2016", "rawpid": "1606.03657", "tags": ["cs.LG", "stat.ML"], "title": "InfoGAN: Interpretable Representation Learning by Information Maximizing\n  Generative Adversarial Nets"}, {"abstract": "We explore the method of style transfer presented in the article \"A Neural\nAlgorithm of Artistic Style\" by Leon A. Gatys, Alexander S. Ecker and Matthias\nBethge (arXiv:1508.06576).\n  We first demonstrate the power of the suggested style space on a few\nexamples. We then vary different hyper-parameters and program properties that\nwere not discussed in the original paper, among which are the recognition\nnetwork used, starting point of the gradient descent and different ways to\npartition style and content layers. We also give a brief comparison of some of\nthe existing algorithm implementations and deep learning frameworks used.\n  To study the style space further we attempt to generate synthetic images by\nmaximizing a single entry in one of the Gram matrices $\\mathcal{G}_l$ and some\ninteresting results are observed. Next, we try to mimic the sparsity and\nintensity distribution of Gram matrices obtained from a real painting and\ngenerate more complex textures.\n  Finally, we propose two new style representations built on top of network\u0027s\nfeatures and discuss how one could be used to achieve local and potentially\ncontent-aware style transfer.", "authors": ["Yaroslav Nikulin", "Roman Novak"], "category": "cs.CV", "comment": "A short class project report (14 pages, 14 figures)", "img": "/static/thumbs/1602.07188v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.07188v2", "num_discussion": 0, "originally_published_time": "2/23/2016", "pid": "1602.07188v2", "published_time": "3/13/2016", "rawpid": "1602.07188", "tags": ["cs.CV"], "title": "Exploring the Neural Algorithm of Artistic Style"}, {"abstract": "Convolutional neural networks are built upon the convolution operation, which\nextracts informative features by fusing spatial and channel-wise information\ntogether within local receptive fields. In order to boost the representational\npower of a network, several recent approaches have shown the benefit of\nenhancing spatial encoding. In this work, we focus on the channel relationship\nand propose a novel architectural unit, which we term the\n\"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise\nfeature responses by explicitly modelling interdependencies between channels.\nWe demonstrate that by stacking these blocks together, we can construct SENet\narchitectures that generalise extremely well across challenging datasets.\nCrucially, we find that SE blocks produce significant performance improvements\nfor existing state-of-the-art deep architectures at minimal additional\ncomputational cost. SENets formed the foundation of our ILSVRC 2017\nclassification submission which won first place and significantly reduced the\ntop-5 error to 2.251%, achieving a ~25% relative improvement over the winning\nentry of 2016. Code and models are available at\nhttps://github.com/hujie-frank/SENet.", "authors": ["Jie Hu", "Li Shen", "Gang Sun"], "category": "cs.CV", "comment": "ILSVRC 2017 image classification winner; CVPR 2018 Oral", "img": "/static/thumbs/1709.01507v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.01507v2", "num_discussion": 0, "originally_published_time": "9/5/2017", "pid": "1709.01507v2", "published_time": "4/5/2018", "rawpid": "1709.01507", "tags": ["cs.CV"], "title": "Squeeze-and-Excitation Networks"}, {"abstract": "Intelligent creatures can explore their environments and learn useful skills\nwithout supervision. In this paper, we propose DIAYN (\u0027Diversity is All You\nNeed\u0027), a method for learning useful skills without a reward function. Our\nproposed method learns skills by maximizing an information theoretic objective\nusing a maximum entropy policy. On a variety of simulated robotic tasks, we\nshow that this simple objective results in the unsupervised emergence of\ndiverse skills, such as walking and jumping. In a number of reinforcement\nlearning benchmark environments, our method is able to learn a skill that\nsolves the benchmark task despite never receiving the true task reward. We show\nhow pretrained skills can provide a good parameter initialization for\ndownstream tasks, and can be composed hierarchically to solve complex, sparse\nreward tasks. Our results suggest that unsupervised discovery of skills can\nserve as an effective pretraining mechanism for overcoming challenges of\nexploration and data efficiency in reinforcement learning.", "authors": ["Benjamin Eysenbach", "Abhishek Gupta", "Julian Ibarz", "Sergey Levine"], "category": "cs.AI", "comment": "Videos and code for our experiments are available at:\n  https://sites.google.com/view/diayn", "img": "/static/thumbs/1802.06070v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.06070v5", "num_discussion": 3, "originally_published_time": "2/16/2018", "pid": "1802.06070v5", "published_time": "6/6/2018", "rawpid": "1802.06070", "tags": ["cs.AI", "cs.RO"], "title": "Diversity is All You Need: Learning Skills without a Reward Function"}, {"abstract": "Despite recent breakthroughs in the applications of deep neural networks, one\nsetting that presents a persistent challenge is that of \"one-shot learning.\"\nTraditional gradient-based networks require a lot of data to learn, often\nthrough extensive iterative training. When new data is encountered, the models\nmust inefficiently relearn their parameters to adequately incorporate the new\ninformation without catastrophic interference. Architectures with augmented\nmemory capacities, such as Neural Turing Machines (NTMs), offer the ability to\nquickly encode and retrieve new information, and hence can potentially obviate\nthe downsides of conventional models. Here, we demonstrate the ability of a\nmemory-augmented neural network to rapidly assimilate new data, and leverage\nthis data to make accurate predictions after only a few samples. We also\nintroduce a new method for accessing an external memory that focuses on memory\ncontent, unlike previous methods that additionally use memory location-based\nfocusing mechanisms.", "authors": ["Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy Lillicrap"], "category": "cs.LG", "comment": "13 pages, 8 figures", "img": "/static/thumbs/1605.06065v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06065v1", "num_discussion": 0, "originally_published_time": "5/19/2016", "pid": "1605.06065v1", "published_time": "5/19/2016", "rawpid": "1605.06065", "tags": ["cs.LG"], "title": "One-shot Learning with Memory-Augmented Neural Networks"}, {"abstract": "Humans have an impressive ability to reason about new concepts and\nexperiences from just a single example. In particular, humans have an ability\nfor one-shot generalization: an ability to encounter a new concept, understand\nits structure, and then be able to generate compelling alternative variations\nof the concept. We develop machine learning systems with this important\ncapacity by developing new deep generative models, models that combine the\nrepresentational power of deep learning with the inferential power of Bayesian\nreasoning. We develop a class of sequential generative models that are built on\nthe principles of feedback and attention. These two characteristics lead to\ngenerative models that are among the state-of-the art in density estimation and\nimage generation. We demonstrate the one-shot generalization ability of our\nmodels using three tasks: unconditional sampling, generating new exemplars of a\ngiven concept, and generating new exemplars of a family of concepts. In all\ncases our models are able to generate compelling and diverse samples---having\nseen new examples just once---providing an important class of general-purpose\nmodels for one-shot machine learning.", "authors": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Ivo Danihelka", "Karol Gregor", "Daan Wierstra"], "category": "stat.ML", "comment": "8pgs, 1pg references, 1pg appendix, In Proceedings of the 33rd\n  International Conference on Machine...", "img": "/static/thumbs/1603.05106v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.05106v2", "num_discussion": 0, "originally_published_time": "3/16/2016", "pid": "1603.05106v2", "published_time": "5/25/2016", "rawpid": "1603.05106", "tags": ["stat.ML", "cs.AI", "cs.LG"], "title": "One-Shot Generalization in Deep Generative Models"}, {"abstract": "We describe a new training methodology for generative adversarial networks.\nThe key idea is to grow both the generator and discriminator progressively:\nstarting from a low resolution, we add new layers that model increasingly fine\ndetails as training progresses. This both speeds the training up and greatly\nstabilizes it, allowing us to produce images of unprecedented quality, e.g.,\nCelebA images at 1024^2. We also propose a simple way to increase the variation\nin generated images, and achieve a record inception score of 8.80 in\nunsupervised CIFAR10. Additionally, we describe several implementation details\nthat are important for discouraging unhealthy competition between the generator\nand discriminator. Finally, we suggest a new metric for evaluating GAN results,\nboth in terms of image quality and variation. As an additional contribution, we\nconstruct a higher-quality version of the CelebA dataset.", "authors": ["Tero Karras", "Timo Aila", "Samuli Laine", "Jaakko Lehtinen"], "category": "cs.NE", "comment": "Final ICLR 2018 version", "img": "/static/thumbs/1710.10196v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.10196v3", "num_discussion": 0, "originally_published_time": "10/27/2017", "pid": "1710.10196v3", "published_time": "2/26/2018", "rawpid": "1710.10196", "tags": ["cs.NE", "cs.LG", "stat.ML"], "title": "Progressive Growing of GANs for Improved Quality, Stability, and\n  Variation"}, {"abstract": "Batch Normalization is quite effective at accelerating and improving the\ntraining of deep models. However, its effectiveness diminishes when the\ntraining minibatches are small, or do not consist of independent samples. We\nhypothesize that this is due to the dependence of model layer inputs on all the\nexamples in the minibatch, and different activations being produced between\ntraining and inference. We propose Batch Renormalization, a simple and\neffective extension to ensure that the training and inference models generate\nthe same outputs that depend on individual examples rather than the entire\nminibatch. Models trained with Batch Renormalization perform substantially\nbetter than batchnorm when training with small or non-i.i.d. minibatches. At\nthe same time, Batch Renormalization retains the benefits of batchnorm such as\ninsensitivity to initialization and training efficiency.", "authors": ["Sergey Ioffe"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1702.03275v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.03275v2", "num_discussion": 0, "originally_published_time": "2/10/2017", "pid": "1702.03275v2", "published_time": "3/30/2017", "rawpid": "1702.03275", "tags": ["cs.LG"], "title": "Batch Renormalization: Towards Reducing Minibatch Dependence in\n  Batch-Normalized Models"}, {"abstract": "Gradient descent optimization algorithms, while increasingly popular, are\noften used as black-box optimizers, as practical explanations of their\nstrengths and weaknesses are hard to come by. This article aims to provide the\nreader with intuitions with regard to the behaviour of different algorithms\nthat will allow her to put them to use. In the course of this overview, we look\nat different variants of gradient descent, summarize challenges, introduce the\nmost common optimization algorithms, review architectures in a parallel and\ndistributed setting, and investigate additional strategies for optimizing\ngradient descent.", "authors": ["Sebastian Ruder"], "category": "cs.LG", "comment": "Added derivations of AdaMax and Nadam", "img": "/static/thumbs/1609.04747v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.04747v2", "num_discussion": 0, "originally_published_time": "9/15/2016", "pid": "1609.04747v2", "published_time": "6/15/2017", "rawpid": "1609.04747", "tags": ["cs.LG"], "title": "An overview of gradient descent optimization algorithms"}, {"abstract": "Convolutional neural networks (CNNs) are inherently limited to model\ngeometric transformations due to the fixed geometric structures in its building\nmodules. In this work, we introduce two new modules to enhance the\ntransformation modeling capacity of CNNs, namely, deformable convolution and\ndeformable RoI pooling. Both are based on the idea of augmenting the spatial\nsampling locations in the modules with additional offsets and learning the\noffsets from target tasks, without additional supervision. The new modules can\nreadily replace their plain counterparts in existing CNNs and can be easily\ntrained end-to-end by standard back-propagation, giving rise to deformable\nconvolutional networks. Extensive experiments validate the effectiveness of our\napproach on sophisticated vision tasks of object detection and semantic\nsegmentation. The code would be released.", "authors": ["Jifeng Dai", "Haozhi Qi", "Yuwen Xiong", "Yi Li", "Guodong Zhang", "Han Hu", "Yichen Wei"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1703.06211v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.06211v3", "num_discussion": 0, "originally_published_time": "3/17/2017", "pid": "1703.06211v3", "published_time": "6/5/2017", "rawpid": "1703.06211", "tags": ["cs.CV"], "title": "Deformable Convolutional Networks"}, {"abstract": "Convolutional Neural Networks define an exceptionally powerful class of\nmodels, but are still limited by the lack of ability to be spatially invariant\nto the input data in a computationally and parameter efficient manner. In this\nwork we introduce a new learnable module, the Spatial Transformer, which\nexplicitly allows the spatial manipulation of data within the network. This\ndifferentiable module can be inserted into existing convolutional\narchitectures, giving neural networks the ability to actively spatially\ntransform feature maps, conditional on the feature map itself, without any\nextra training supervision or modification to the optimisation process. We show\nthat the use of spatial transformers results in models which learn invariance\nto translation, scale, rotation and more generic warping, resulting in\nstate-of-the-art performance on several benchmarks, and for a number of classes\nof transformations.", "authors": ["Max Jaderberg", "Karen Simonyan", "Andrew Zisserman", "Koray Kavukcuoglu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1506.02025v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.02025v3", "num_discussion": 0, "originally_published_time": "6/5/2015", "pid": "1506.02025v3", "published_time": "2/4/2016", "rawpid": "1506.02025", "tags": ["cs.CV"], "title": "Spatial Transformer Networks"}, {"abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast\nR-CNN) for object detection. Fast R-CNN builds on previous work to efficiently\nclassify object proposals using deep convolutional networks. Compared to\nprevious work, Fast R-CNN employs several innovations to improve training and\ntesting speed while also increasing detection accuracy. Fast R-CNN trains the\nvery deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and\nachieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains\nVGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is\nimplemented in Python and C++ (using Caffe) and is available under the\nopen-source MIT License at https://github.com/rbgirshick/fast-rcnn.", "authors": ["Ross Girshick"], "category": "cs.CV", "comment": "To appear in ICCV 2015", "img": "/static/thumbs/1504.08083v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1504.08083v2", "num_discussion": 0, "originally_published_time": "4/30/2015", "pid": "1504.08083v2", "published_time": "9/27/2015", "rawpid": "1504.08083", "tags": ["cs.CV"], "title": "Fast R-CNN"}, {"abstract": "Very deep convolutional neural networks introduced new problems like\nvanishing gradient and degradation. The recent successful contributions towards\nsolving these problems are Residual and Highway Networks. These networks\nintroduce skip connections that allow the information (from the input or those\nlearned in earlier layers) to flow more into the deeper layers. These very deep\nmodels have lead to a considerable decrease in test errors, on benchmarks like\nImageNet and COCO. In this paper, we propose the use of exponential linear unit\ninstead of the combination of ReLU and Batch Normalization in Residual\nNetworks. We show that this not only speeds up learning in Residual Networks\nbut also improves the accuracy as the depth increases. It improves the test\nerror on almost all data sets, like CIFAR-10 and CIFAR-100", "authors": ["Anish Shah", "Eashan Kadam", "Hena Shah", "Sameer Shinde", "Sandip Shingade"], "category": "cs.CV", "comment": "submitted in Vision Net 2016, Jaipur, India", "img": "/static/thumbs/1604.04112v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.04112v4", "num_discussion": 0, "originally_published_time": "4/14/2016", "pid": "1604.04112v4", "published_time": "10/5/2016", "rawpid": "1604.04112", "tags": ["cs.CV"], "title": "Deep Residual Networks with Exponential Linear Unit"}, {"abstract": "Imitation learning has been commonly applied to solve different tasks in\nisolation. This usually requires either careful feature engineering, or a\nsignificant number of samples. This is far from what we desire: ideally, robots\nshould be able to learn from very few demonstrations of any given task, and\ninstantly generalize to new situations of the same task, without requiring\ntask-specific engineering. In this paper, we propose a meta-learning framework\nfor achieving such capability, which we call one-shot imitation learning.\n  Specifically, we consider the setting where there is a very large set of\ntasks, and each task has many instantiations. For example, a task could be to\nstack all blocks on a table into a single tower, another task could be to place\nall blocks on a table into two-block towers, etc. In each case, different\ninstances of the task would consist of different sets of blocks with different\ninitial states. At training time, our algorithm is presented with pairs of\ndemonstrations for a subset of all tasks. A neural net is trained that takes as\ninput one demonstration and the current state (which initially is the initial\nstate of the other demonstration of the pair), and outputs an action with the\ngoal that the resulting sequence of states and actions matches as closely as\npossible with the second demonstration. At test time, a demonstration of a\nsingle instance of a new task is presented, and the neural net is expected to\nperform well on new instances of this new task. The use of soft attention\nallows the model to generalize to conditions and tasks unseen in the training\ndata. We anticipate that by training this model on a much greater variety of\ntasks and settings, we will obtain a general system that can turn any\ndemonstrations into robust policies that can accomplish an overwhelming variety\nof tasks.\n  Videos available at https://bit.ly/nips2017-oneshot .", "authors": ["Yan Duan", "Marcin Andrychowicz", "Bradly C. Stadie", "Jonathan Ho", "Jonas Schneider", "Ilya Sutskever", "Pieter Abbeel", "Wojciech Zaremba"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1703.07326v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.07326v3", "num_discussion": 0, "originally_published_time": "3/21/2017", "pid": "1703.07326v3", "published_time": "12/4/2017", "rawpid": "1703.07326", "tags": ["cs.AI", "cs.LG", "cs.NE", "cs.RO"], "title": "One-Shot Imitation Learning"}, {"abstract": "In this paper we address the question of how to render sequence-level\nnetworks better at handling structured input. We propose a machine reading\nsimulator which processes text incrementally from left to right and performs\nshallow reasoning with memory and attention. The reader extends the Long\nShort-Term Memory architecture with a memory network in place of a single\nmemory cell. This enables adaptive memory usage during recurrence with neural\nattention, offering a way to weakly induce relations among tokens. The system\nis initially designed to process a single sequence but we also demonstrate how\nto integrate it with an encoder-decoder architecture. Experiments on language\nmodeling, sentiment analysis, and natural language inference show that our\nmodel matches or outperforms the state of the art.", "authors": ["Jianpeng Cheng", "Li Dong", "Mirella Lapata"], "category": "cs.CL", "comment": "Published as a conference paper at EMNLP 2016", "img": "/static/thumbs/1601.06733v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1601.06733v7", "num_discussion": 0, "originally_published_time": "1/25/2016", "pid": "1601.06733v7", "published_time": "9/20/2016", "rawpid": "1601.06733", "tags": ["cs.CL", "cs.NE"], "title": "Long Short-Term Memory-Networks for Machine Reading"}, {"abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.", "authors": ["Diederik P. Kingma", "Jimmy Ba"], "category": "cs.LG", "comment": "Published as a conference paper at the 3rd International Conference\n  for Learning Representations, ...", "img": "/static/thumbs/1412.6980v9.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1412.6980v9", "num_discussion": 0, "originally_published_time": "12/22/2014", "pid": "1412.6980v9", "published_time": "1/30/2017", "rawpid": "1412.6980", "tags": ["cs.LG"], "title": "Adam: A Method for Stochastic Optimization"}, {"abstract": "We show how the success of deep learning could depend not only on mathematics\nbut also on physics: although well-known mathematical theorems guarantee that\nneural networks can approximate arbitrary functions well, the class of\nfunctions of practical interest can frequently be approximated through \"cheap\nlearning\" with exponentially fewer parameters than generic ones. We explore how\nproperties frequently encountered in physics such as symmetry, locality,\ncompositionality, and polynomial log-probability translate into exceptionally\nsimple neural networks. We further argue that when the statistical process\ngenerating the data is of a certain hierarchical form prevalent in physics and\nmachine-learning, a deep neural network can be more efficient than a shallow\none. We formalize these claims using information theory and discuss the\nrelation to the renormalization group. We prove various \"no-flattening\ntheorems\" showing when efficient linear deep networks cannot be accurately\napproximated by shallow ones without efficiency loss, for example, we show that\n$n$ variables cannot be multiplied using fewer than 2^n neurons in a single\nhidden layer.", "authors": ["Henry W. Lin", "Max Tegmark", "David Rolnick"], "category": "cond-mat.dis-nn", "comment": "Replaced to match version published in Journal of Statistical\n  Physics: https://link.springer.com/a...", "img": "/static/thumbs/1608.08225v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1608.08225v4", "num_discussion": 0, "originally_published_time": "8/29/2016", "pid": "1608.08225v4", "published_time": "8/3/2017", "rawpid": "1608.08225", "tags": ["cond-mat.dis-nn", "cs.LG", "cs.NE", "stat.ML"], "title": "Why does deep and cheap learning work so well?"}, {"abstract": "Modeling the distribution of natural images is a landmark problem in\nunsupervised learning. This task requires an image model that is at once\nexpressive, tractable and scalable. We present a deep neural network that\nsequentially predicts the pixels in an image along the two spatial dimensions.\nOur method models the discrete probability of the raw pixel values and encodes\nthe complete set of dependencies in the image. Architectural novelties include\nfast two-dimensional recurrent layers and an effective use of residual\nconnections in deep recurrent networks. We achieve log-likelihood scores on\nnatural images that are considerably better than the previous state of the art.\nOur main results also provide benchmarks on the diverse ImageNet dataset.\nSamples generated from the model appear crisp, varied and globally coherent.", "authors": ["Aaron van den Oord", "Nal Kalchbrenner", "Koray Kavukcuoglu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1601.06759v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1601.06759v3", "num_discussion": 0, "originally_published_time": "1/25/2016", "pid": "1601.06759v3", "published_time": "8/19/2016", "rawpid": "1601.06759", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "Pixel Recurrent Neural Networks"}, {"abstract": "Learning useful representations without supervision remains a key challenge\nin machine learning. In this paper, we propose a simple yet powerful generative\nmodel that learns such discrete representations. Our model, the Vector\nQuantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways:\nthe encoder network outputs discrete, rather than continuous, codes; and the\nprior is learnt rather than static. In order to learn a discrete latent\nrepresentation, we incorporate ideas from vector quantisation (VQ). Using the\nVQ method allows the model to circumvent issues of \"posterior collapse\" --\nwhere the latents are ignored when they are paired with a powerful\nautoregressive decoder -- typically observed in the VAE framework. Pairing\nthese representations with an autoregressive prior, the model can generate high\nquality images, videos, and speech as well as doing high quality speaker\nconversion and unsupervised learning of phonemes, providing further evidence of\nthe utility of the learnt representations.", "authors": ["Aaron van den Oord", "Oriol Vinyals", "Koray Kavukcuoglu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1711.00937v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.00937v2", "num_discussion": 0, "originally_published_time": "11/2/2017", "pid": "1711.00937v2", "published_time": "5/30/2018", "rawpid": "1711.00937", "tags": ["cs.LG"], "title": "Neural Discrete Representation Learning"}, {"abstract": "As a new way of training generative models, Generative Adversarial Nets (GAN)\nthat uses a discriminative model to guide the training of the generative model\nhas enjoyed considerable success in generating real-valued data. However, it\nhas limitations when the goal is for generating sequences of discrete tokens. A\nmajor reason lies in that the discrete outputs from the generative model make\nit difficult to pass the gradient update from the discriminative model to the\ngenerative model. Also, the discriminative model can only assess a complete\nsequence, while for a partially generated sequence, it is non-trivial to\nbalance its current score and the future one once the entire sequence has been\ngenerated. In this paper, we propose a sequence generation framework, called\nSeqGAN, to solve the problems. Modeling the data generator as a stochastic\npolicy in reinforcement learning (RL), SeqGAN bypasses the generator\ndifferentiation problem by directly performing gradient policy update. The RL\nreward signal comes from the GAN discriminator judged on a complete sequence,\nand is passed back to the intermediate state-action steps using Monte Carlo\nsearch. Extensive experiments on synthetic data and real-world tasks\ndemonstrate significant improvements over strong baselines.", "authors": ["Lantao Yu", "Weinan Zhang", "Jun Wang", "Yong Yu"], "category": "cs.LG", "comment": "The Thirty-First AAAI Conference on Artificial Intelligence (AAAI\n  2017)", "img": "/static/thumbs/1609.05473v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.05473v6", "num_discussion": 0, "originally_published_time": "9/18/2016", "pid": "1609.05473v6", "published_time": "8/25/2017", "rawpid": "1609.05473", "tags": ["cs.LG", "cs.AI"], "title": "SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient"}, {"abstract": "Although deep learning has produced dazzling successes for applications of\nimage, speech, and video processing in the past few years, most trainings are\nwith suboptimal hyper-parameters, requiring unnecessarily long training times.\nSetting the hyper-parameters remains a black art that requires years of\nexperience to acquire. This report proposes several efficient ways to set the\nhyper-parameters that significantly reduce training time and improves\nperformance. Specifically, this report shows how to examine the training\nvalidation/test loss function for subtle clues of underfitting and overfitting\nand suggests guidelines for moving toward the optimal balance point. Then it\ndiscusses how to increase/decrease the learning rate/momentum to speed up\ntraining. Our experiments show that it is crucial to balance every manner of\nregularization for each dataset and architecture. Weight decay is used as a\nsample regularizer to show how its optimal value is tightly coupled with the\nlearning rates and momentums. Files to help replicate the results reported here\nare available.", "authors": ["Leslie N. Smith"], "category": "cs.LG", "comment": "Files to help replicate the results reported here are available on\n  Github", "img": "/static/thumbs/1803.09820v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.09820v2", "num_discussion": 0, "originally_published_time": "3/26/2018", "pid": "1803.09820v2", "published_time": "4/24/2018", "rawpid": "1803.09820", "tags": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "title": "A disciplined approach to neural network hyper-parameters: Part 1 --\n  learning rate, batch size, momentum, and weight decay"}, {"abstract": "This tutorial introduces a new and powerful set of techniques variously\ncalled \"neural machine translation\" or \"neural sequence-to-sequence models\".\nThese techniques have been used in a number of tasks regarding the handling of\nhuman language, and can be a powerful tool in the toolbox of anyone who wants\nto model sequential data of some sort. The tutorial assumes that the reader\nknows the basics of math and programming, but does not assume any particular\nexperience with neural networks or natural language processing. It attempts to\nexplain the intuition behind the various methods covered, then delves into them\nwith enough mathematical detail to understand them concretely, and culiminates\nwith a suggestion for an implementation exercise, where readers can test that\nthey understood the content in practice.", "authors": ["Graham Neubig"], "category": "cs.CL", "comment": "65 Pages", "img": "/static/thumbs/1703.01619v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.01619v1", "num_discussion": 0, "originally_published_time": "3/5/2017", "pid": "1703.01619v1", "published_time": "3/5/2017", "rawpid": "1703.01619", "tags": ["cs.CL", "cs.LG", "stat.ML"], "title": "Neural Machine Translation and Sequence-to-sequence Models: A Tutorial"}, {"abstract": "This paper explores a simple and efficient baseline for text classification.\nOur experiments show that our fast text classifier fastText is often on par\nwith deep learning classifiers in terms of accuracy, and many orders of\nmagnitude faster for training and evaluation. We can train fastText on more\nthan one billion words in less than ten minutes using a standard multicore~CPU,\nand classify half a million sentences among~312K classes in less than a minute.", "authors": ["Armand Joulin", "Edouard Grave", "Piotr Bojanowski", "Tomas Mikolov"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1607.01759v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1607.01759v3", "num_discussion": 0, "originally_published_time": "7/6/2016", "pid": "1607.01759v3", "published_time": "8/9/2016", "rawpid": "1607.01759", "tags": ["cs.CL"], "title": "Bag of Tricks for Efficient Text Classification"}, {"abstract": "We adapt the ideas underlying the success of Deep Q-Learning to the\ncontinuous action domain. We present an actor-critic, model-free algorithm\nbased on the deterministic policy gradient that can operate over continuous\naction spaces. Using the same learning algorithm, network architecture and\nhyper-parameters, our algorithm robustly solves more than 20 simulated physics\ntasks, including classic problems such as cartpole swing-up, dexterous\nmanipulation, legged locomotion and car driving. Our algorithm is able to find\npolicies whose performance is competitive with those found by a planning\nalgorithm with full access to the dynamics of the domain and its derivatives.\nWe further demonstrate that for many of the tasks the algorithm can learn\npolicies end-to-end: directly from raw pixel inputs.", "authors": ["Timothy P. Lillicrap", "Jonathan J. Hunt", "Alexander Pritzel", "Nicolas Heess", "Tom Erez", "Yuval Tassa", "David Silver", "Daan Wierstra"], "category": "cs.LG", "comment": "10 pages + supplementary", "img": "/static/thumbs/1509.02971v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1509.02971v5", "num_discussion": 0, "originally_published_time": "9/9/2015", "pid": "1509.02971v5", "published_time": "2/29/2016", "rawpid": "1509.02971", "tags": ["cs.LG", "stat.ML"], "title": "Continuous control with deep reinforcement learning"}, {"abstract": "We present an interpretation of Inception modules in convolutional neural\nnetworks as being an intermediate step in-between regular convolution and the\ndepthwise separable convolution operation (a depthwise convolution followed by\na pointwise convolution). In this light, a depthwise separable convolution can\nbe understood as an Inception module with a maximally large number of towers.\nThis observation leads us to propose a novel deep convolutional neural network\narchitecture inspired by Inception, where Inception modules have been replaced\nwith depthwise separable convolutions. We show that this architecture, dubbed\nXception, slightly outperforms Inception V3 on the ImageNet dataset (which\nInception V3 was designed for), and significantly outperforms Inception V3 on a\nlarger image classification dataset comprising 350 million images and 17,000\nclasses. Since the Xception architecture has the same number of parameters as\nInception V3, the performance gains are not due to increased capacity but\nrather to a more efficient use of model parameters.", "authors": ["Fran\u00e7ois Chollet"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1610.02357v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.02357v3", "num_discussion": 0, "originally_published_time": "10/7/2016", "pid": "1610.02357v3", "published_time": "4/4/2017", "rawpid": "1610.02357", "tags": ["cs.CV"], "title": "Xception: Deep Learning with Depthwise Separable Convolutions"}, {"abstract": "Deep neural networks are typically trained by optimizing a loss function with\nan SGD variant, in conjunction with a decaying learning rate, until\nconvergence. We show that simple averaging of multiple points along the\ntrajectory of SGD, with a cyclical or constant learning rate, leads to better\ngeneralization than conventional training. We also show that this Stochastic\nWeight Averaging (SWA) procedure finds much broader optima than SGD, and\napproximates the recent Fast Geometric Ensembling (FGE) approach with a single\nmodel. Using SWA we achieve notable improvement in test accuracy over\nconventional SGD training on a range of state-of-the-art residual networks,\nPyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and\nImageNet. In short, SWA is extremely easy to implement, improves\ngeneralization, and has almost no computational overhead.", "authors": ["Pavel Izmailov", "Dmitrii Podoprikhin", "Timur Garipov", "Dmitry Vetrov", "Andrew Gordon Wilson"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1803.05407v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1803.05407v1", "num_discussion": 0, "originally_published_time": "3/14/2018", "pid": "1803.05407v1", "published_time": "3/14/2018", "rawpid": "1803.05407", "tags": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "title": "Averaging Weights Leads to Wider Optima and Better Generalization"}, {"abstract": "Inductive transfer learning has greatly impacted computer vision, but\nexisting approaches in NLP still require task-specific modifications and\ntraining from scratch. We propose Universal Language Model Fine-tuning\n(ULMFiT), an effective transfer learning method that can be applied to any task\nin NLP, and introduce techniques that are key for fine-tuning a language model.\nOur method significantly outperforms the state-of-the-art on six text\nclassification tasks, reducing the error by 18-24% on the majority of datasets.\nFurthermore, with only 100 labeled examples, it matches the performance of\ntraining from scratch on 100x more data. We open-source our pretrained models\nand code.", "authors": ["Jeremy Howard", "Sebastian Ruder"], "category": "cs.CL", "comment": "ACL 2018, fixed denominator in Equation 3, line 3", "img": "/static/thumbs/1801.06146v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1801.06146v5", "num_discussion": 0, "originally_published_time": "1/18/2018", "pid": "1801.06146v5", "published_time": "5/23/2018", "rawpid": "1801.06146", "tags": ["cs.CL", "cs.LG", "stat.ML"], "title": "Universal Language Model Fine-tuning for Text Classification"}, {"abstract": "A new prior is proposed for representation learning, which can be combined\nwith other priors in order to help disentangling abstract factors from each\nother. It is inspired by the phenomenon of consciousness seen as the formation\nof a low-dimensional combination of a few concepts constituting a conscious\nthought, i.e., consciousness as awareness at a particular time instant. This\nprovides a powerful constraint on the representation in that such\nlow-dimensional thought vectors can correspond to statements about reality\nwhich are true, highly probable, or very useful for taking decisions. The fact\nthat a few elements of the current state can be combined into such a predictive\nor useful statement is a strong constraint and deviates considerably from the\nmaximum likelihood approaches to modelling data and how states unfold in the\nfuture based on an agent\u0027s actions. Instead of making predictions in the\nsensory (e.g. pixel) space, the consciousness prior allows the agent to make\npredictions in the abstract space, with only a few dimensions of that space\nbeing involved in each of these predictions. The consciousness prior also makes\nit natural to map conscious states to natural language utterances or to express\nclassical AI knowledge in the form of facts and rules, although the conscious\nstates may be richer than what can be expressed easily in the form of a\nsentence, a fact or a rule.", "authors": ["Yoshua Bengio"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1709.08568v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.08568v1", "num_discussion": 0, "originally_published_time": "9/25/2017", "pid": "1709.08568v1", "published_time": "9/25/2017", "rawpid": "1709.08568", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "The Consciousness Prior"}, {"abstract": "We present a method for visualising the response of a deep neural network to\na specific input. For image data for instance our method will highlight areas\nthat provide evidence in favor of, and against choosing a certain class. The\nmethod overcomes several shortcomings of previous methods and provides great\nadditional insight into the decision making process of convolutional networks,\nwhich is important both to improve models and to accelerate the adoption of\nsuch methods in e.g. medicine. In experiments on ImageNet data, we illustrate\nhow the method works and can be applied in different ways to understand deep\nneural nets.", "authors": ["Luisa M. Zintgraf", "Taco S. Cohen", "Max Welling"], "category": "cs.CV", "comment": "Please note that this version of the article is outdated. The new\n  version (published at ICLR2017) ...", "img": "/static/thumbs/1603.02518v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.02518v3", "num_discussion": 0, "originally_published_time": "3/8/2016", "pid": "1603.02518v3", "published_time": "6/12/2017", "rawpid": "1603.02518", "tags": ["cs.CV"], "title": "A New Method to Visualize Deep Neural Networks"}, {"abstract": "The deep reinforcement learning community has made several independent\nimprovements to the DQN algorithm. However, it is unclear which of these\nextensions are complementary and can be fruitfully combined. This paper\nexamines six extensions to the DQN algorithm and empirically studies their\ncombination. Our experiments show that the combination provides\nstate-of-the-art performance on the Atari 2600 benchmark, both in terms of data\nefficiency and final performance. We also provide results from a detailed\nablation study that shows the contribution of each component to overall\nperformance.", "authors": ["Matteo Hessel", "Joseph Modayil", "Hado van Hasselt", "Tom Schaul", "Georg Ostrovski", "Will Dabney", "Dan Horgan", "Bilal Piot", "Mohammad Azar", "David Silver"], "category": "cs.AI", "comment": "Under review as a conference paper at AAAI 2018", "img": "/static/thumbs/1710.02298v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.02298v1", "num_discussion": 0, "originally_published_time": "10/6/2017", "pid": "1710.02298v1", "published_time": "10/6/2017", "rawpid": "1710.02298", "tags": ["cs.AI", "cs.LG"], "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning"}, {"abstract": "We propose a new technique for visual attribute transfer across images that\nmay have very different appearance but have perceptually similar semantic\nstructure. By visual attribute transfer, we mean transfer of visual information\n(such as color, tone, texture, and style) from one image to another. For\nexample, one image could be that of a painting or a sketch while the other is a\nphoto of a real scene, and both depict the same type of scene.\n  Our technique finds semantically-meaningful dense correspondences between two\ninput images. To accomplish this, it adapts the notion of \"image analogy\" with\nfeatures extracted from a Deep Convolutional Neutral Network for matching; we\ncall our technique Deep Image Analogy. A coarse-to-fine strategy is used to\ncompute the nearest-neighbor field for generating the results. We validate the\neffectiveness of our proposed method in a variety of cases, including\nstyle/texture transfer, color/style swap, sketch/painting to photo, and time\nlapse.", "authors": ["Jing Liao", "Yuan Yao", "Lu Yuan", "Gang Hua", "Sing Bing Kang"], "category": "cs.CV", "comment": "Accepted by SIGGRAPH 2017", "img": "/static/thumbs/1705.01088v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.01088v2", "num_discussion": 2, "originally_published_time": "5/2/2017", "pid": "1705.01088v2", "published_time": "6/6/2017", "rawpid": "1705.01088", "tags": ["cs.CV"], "title": "Visual Attribute Transfer through Deep Image Analogy"}, {"abstract": "Convolutional networks are powerful visual models that yield hierarchies of\nfeatures. We show that convolutional networks by themselves, trained\nend-to-end, pixels-to-pixels, improve on the previous best result in semantic\nsegmentation. Our key insight is to build \"fully convolutional\" networks that\ntake input of arbitrary size and produce correspondingly-sized output with\nefficient inference and learning. We define and detail the space of fully\nconvolutional networks, explain their application to spatially dense prediction\ntasks, and draw connections to prior models. We adapt contemporary\nclassification networks (AlexNet, the VGG net, and GoogLeNet) into fully\nconvolutional networks and transfer their learned representations by\nfine-tuning to the segmentation task. We then define a skip architecture that\ncombines semantic information from a deep, coarse layer with appearance\ninformation from a shallow, fine layer to produce accurate and detailed\nsegmentations. Our fully convolutional network achieves improved segmentation\nof PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT\nFlow, and PASCAL-Context, while inference takes one tenth of a second for a\ntypical image.", "authors": ["Evan Shelhamer", "Jonathan Long", "Trevor Darrell"], "category": "cs.CV", "comment": "to appear in PAMI (accepted May, 2016); journal edition of\n  arXiv:1411.4038", "img": "/static/thumbs/1605.06211v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.06211v1", "num_discussion": 0, "originally_published_time": "5/20/2016", "pid": "1605.06211v1", "published_time": "5/20/2016", "rawpid": "1605.06211", "tags": ["cs.CV"], "title": "Fully Convolutional Networks for Semantic Segmentation"}, {"abstract": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent\nperformance on difficult learning tasks. Although DNNs work well whenever large\nlabeled training sets are available, they cannot be used to map sequences to\nsequences. In this paper, we present a general end-to-end approach to sequence\nlearning that makes minimal assumptions on the sequence structure. Our method\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to\na vector of a fixed dimensionality, and then another deep LSTM to decode the\ntarget sequence from the vector. Our main result is that on an English to\nFrench translation task from the WMT\u002714 dataset, the translations produced by\nthe LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM\u0027s\nBLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did\nnot have difficulty on long sentences. For comparison, a phrase-based SMT\nsystem achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM\nto rerank the 1000 hypotheses produced by the aforementioned SMT system, its\nBLEU score increases to 36.5, which is close to the previous best result on\nthis task. The LSTM also learned sensible phrase and sentence representations\nthat are sensitive to word order and are relatively invariant to the active and\nthe passive voice. Finally, we found that reversing the order of the words in\nall source sentences (but not target sentences) improved the LSTM\u0027s performance\nmarkedly, because doing so introduced many short term dependencies between the\nsource and the target sentence which made the optimization problem easier.", "authors": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "category": "cs.CL", "comment": "9 pages", "img": "/static/thumbs/1409.3215v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1409.3215v3", "num_discussion": 0, "originally_published_time": "9/10/2014", "pid": "1409.3215v3", "published_time": "12/14/2014", "rawpid": "1409.3215", "tags": ["cs.CL", "cs.LG"], "title": "Sequence to Sequence Learning with Neural Networks"}, {"abstract": "Batch Normalization (BatchNorm) is a widely adopted technique that enables\nfaster and more stable training of deep neural networks (DNNs). Despite its\npervasiveness, the exact reasons for BatchNorm\u0027s effectiveness are still poorly\nunderstood. The popular belief is that this effectiveness stems from\ncontrolling the change of the layers\u0027 input distributions during training to\nreduce the so-called \"internal covariate shift\". In this work, we demonstrate\nthat such distributional stability of layer inputs has little to do with the\nsuccess of BatchNorm. Instead, we uncover a more fundamental impact of\nBatchNorm on the training process: it makes the optimization landscape\nsignificantly smoother. This smoothness induces a more predictive and stable\nbehavior of the gradients, allowing for faster training. These findings bring\nus closer to a true understanding of our DNN training toolkit.", "authors": ["Shibani Santurkar", "Dimitris Tsipras", "Andrew Ilyas", "Aleksander Madry"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1805.11604v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1805.11604v2", "num_discussion": 0, "originally_published_time": "5/29/2018", "pid": "1805.11604v2", "published_time": "6/5/2018", "rawpid": "1805.11604", "tags": ["stat.ML", "cs.LG", "cs.NE"], "title": "How Does Batch Normalization Help Optimization? (No, It Is Not About\n  Internal Covariate Shift)"}, {"abstract": "We propose an algorithm for meta-learning that is model-agnostic, in the\nsense that it is compatible with any model trained with gradient descent and\napplicable to a variety of different learning problems, including\nclassification, regression, and reinforcement learning. The goal of\nmeta-learning is to train a model on a variety of learning tasks, such that it\ncan solve new learning tasks using only a small number of training samples. In\nour approach, the parameters of the model are explicitly trained such that a\nsmall number of gradient steps with a small amount of training data from a new\ntask will produce good generalization performance on that task. In effect, our\nmethod trains the model to be easy to fine-tune. We demonstrate that this\napproach leads to state-of-the-art performance on two few-shot image\nclassification benchmarks, produces good results on few-shot regression, and\naccelerates fine-tuning for policy gradient reinforcement learning with neural\nnetwork policies.", "authors": ["Chelsea Finn", "Pieter Abbeel", "Sergey Levine"], "category": "cs.LG", "comment": "ICML 2017. Code at https://github.com/cbfinn/maml, Videos of RL\n  results at https://sites.google.co...", "img": "/static/thumbs/1703.03400v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.03400v3", "num_discussion": 1, "originally_published_time": "3/9/2017", "pid": "1703.03400v3", "published_time": "7/18/2017", "rawpid": "1703.03400", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.NE"], "title": "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"}, {"abstract": "Deep neural networks have proved to be a very effective way to perform\nclassification tasks. They excel when the input data is high dimensional, the\nrelationship between the input and the output is complicated, and the number of\nlabeled training examples is large. But it is hard to explain why a learned\nnetwork makes a particular classification decision on a particular test case.\nThis is due to their reliance on distributed hierarchical representations. If\nwe could take the knowledge acquired by the neural net and express the same\nknowledge in a model that relies on hierarchical decisions instead, explaining\na particular decision would be much easier. We describe a way of using a\ntrained neural net to create a type of soft decision tree that generalizes\nbetter than one learned directly from the training data.", "authors": ["Nicholas Frosst", "Geoffrey Hinton"], "category": "cs.LG", "comment": "presented at the CEX workshop at AI*IA 2017 conference", "img": "/static/thumbs/1711.09784v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.09784v1", "num_discussion": 0, "originally_published_time": "11/27/2017", "pid": "1711.09784v1", "published_time": "11/27/2017", "rawpid": "1711.09784", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "Distilling a Neural Network Into a Soft Decision Tree"}, {"abstract": "Deep reinforcement learning methods attain super-human performance in a wide\nrange of environments. Such methods are grossly inefficient, often taking\norders of magnitudes more data than humans to achieve reasonable performance.\nWe propose Neural Episodic Control: a deep reinforcement learning agent that is\nable to rapidly assimilate new experiences and act upon them. Our agent uses a\nsemi-tabular representation of the value function: a buffer of past experience\ncontaining slowly changing state representations and rapidly updated estimates\nof the value function. We show across a wide range of environments that our\nagent learns significantly faster than other state-of-the-art, general purpose\ndeep reinforcement learning agents.", "authors": ["Alexander Pritzel", "Benigno Uria", "Sriram Srinivasan", "Adri\u00e0 Puigdom\u00e8nech", "Oriol Vinyals", "Demis Hassabis", "Daan Wierstra", "Charles Blundell"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1703.01988v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.01988v1", "num_discussion": 1, "originally_published_time": "3/6/2017", "pid": "1703.01988v1", "published_time": "3/6/2017", "rawpid": "1703.01988", "tags": ["cs.LG", "stat.ML"], "title": "Neural Episodic Control"}, {"abstract": "The ability to learn tasks in a sequential fashion is crucial to the\ndevelopment of artificial intelligence. Neural networks are not, in general,\ncapable of this and it has been widely thought that catastrophic forgetting is\nan inevitable feature of connectionist models. We show that it is possible to\novercome this limitation and train networks that can maintain expertise on\ntasks which they have not experienced for a long time. Our approach remembers\nold tasks by selectively slowing down learning on the weights important for\nthose tasks. We demonstrate our approach is scalable and effective by solving a\nset of classification tasks based on the MNIST hand written digit dataset and\nby learning several Atari 2600 games sequentially.", "authors": ["James Kirkpatrick", "Razvan Pascanu", "Neil Rabinowitz", "Joel Veness", "Guillaume Desjardins", "Andrei A. Rusu", "Kieran Milan", "John Quan", "Tiago Ramalho", "Agnieszka Grabska-Barwinska", "Demis Hassabis", "Claudia Clopath", "Dharshan Kumaran", "Raia Hadsell"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1612.00796v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.00796v2", "num_discussion": 0, "originally_published_time": "12/2/2016", "pid": "1612.00796v2", "published_time": "1/25/2017", "rawpid": "1612.00796", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "Overcoming catastrophic forgetting in neural networks"}, {"abstract": "This work explores hypernetworks: an approach of using a one network, also\nknown as a hypernetwork, to generate the weights for another network.\nHypernetworks provide an abstraction that is similar to what is found in\nnature: the relationship between a genotype - the hypernetwork - and a\nphenotype - the main network. Though they are also reminiscent of HyperNEAT in\nevolution, our hypernetworks are trained end-to-end with backpropagation and\nthus are usually faster. The focus of this work is to make hypernetworks useful\nfor deep convolutional networks and long recurrent networks, where\nhypernetworks can be viewed as relaxed form of weight-sharing across layers.\nOur main result is that hypernetworks can generate non-shared weights for LSTM\nand achieve near state-of-the-art results on a variety of sequence modelling\ntasks including character-level language modelling, handwriting generation and\nneural machine translation, challenging the weight-sharing paradigm for\nrecurrent networks. Our results also show that hypernetworks applied to\nconvolutional networks still achieve respectable results for image recognition\ntasks compared to state-of-the-art baseline models while requiring fewer\nlearnable parameters.", "authors": ["David Ha", "Andrew Dai", "Quoc V. Le"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1609.09106v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.09106v4", "num_discussion": 1, "originally_published_time": "9/27/2016", "pid": "1609.09106v4", "published_time": "12/1/2016", "rawpid": "1609.09106", "tags": ["cs.LG"], "title": "HyperNetworks"}, {"abstract": "In many real-world scenarios, rewards extrinsic to the agent are extremely\nsparse, or absent altogether. In such cases, curiosity can serve as an\nintrinsic reward signal to enable the agent to explore its environment and\nlearn skills that might be useful later in its life. We formulate curiosity as\nthe error in an agent\u0027s ability to predict the consequence of its own actions\nin a visual feature space learned by a self-supervised inverse dynamics model.\nOur formulation scales to high-dimensional continuous state spaces like images,\nbypasses the difficulties of directly predicting pixels, and, critically,\nignores the aspects of the environment that cannot affect the agent. The\nproposed approach is evaluated in two environments: VizDoom and Super Mario\nBros. Three broad settings are investigated: 1) sparse extrinsic reward, where\ncuriosity allows for far fewer interactions with the environment to reach the\ngoal; 2) exploration with no extrinsic reward, where curiosity pushes the agent\nto explore more efficiently; and 3) generalization to unseen scenarios (e.g.\nnew levels of the same game) where the knowledge gained from earlier experience\nhelps the agent explore new places much faster than starting from scratch. Demo\nvideo and code available at https://pathak22.github.io/noreward-rl/", "authors": ["Deepak Pathak", "Pulkit Agrawal", "Alexei A. Efros", "Trevor Darrell"], "category": "cs.LG", "comment": "In ICML 2017. Website at https://pathak22.github.io/noreward-rl/", "img": "/static/thumbs/1705.05363v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.05363v1", "num_discussion": 0, "originally_published_time": "5/15/2017", "pid": "1705.05363v1", "published_time": "5/15/2017", "rawpid": "1705.05363", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.RO", "stat.ML"], "title": "Curiosity-driven Exploration by Self-supervised Prediction"}, {"abstract": "We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical\nreinforcement learning. Our approach is inspired by the feudal reinforcement\nlearning proposal of Dayan and Hinton, and gains power and efficacy by\ndecoupling end-to-end learning across multiple levels -- allowing it to utilise\ndifferent resolutions of time. Our framework employs a Manager module and a\nWorker module. The Manager operates at a lower temporal resolution and sets\nabstract goals which are conveyed to and enacted by the Worker. The Worker\ngenerates primitive actions at every tick of the environment. The decoupled\nstructure of FuN conveys several benefits -- in addition to facilitating very\nlong timescale credit assignment it also encourages the emergence of\nsub-policies associated with different goals set by the Manager. These\nproperties allow FuN to dramatically outperform a strong baseline agent on\ntasks that involve long-term credit assignment or memorisation. We demonstrate\nthe performance of our proposed system on a range of tasks from the ATARI suite\nand also from a 3D DeepMind Lab environment.", "authors": ["Alexander Sasha Vezhnevets", "Simon Osindero", "Tom Schaul", "Nicolas Heess", "Max Jaderberg", "David Silver", "Koray Kavukcuoglu"], "category": "cs.AI", "comment": "", "img": "/static/thumbs/1703.01161v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.01161v2", "num_discussion": 0, "originally_published_time": "3/3/2017", "pid": "1703.01161v2", "published_time": "3/6/2017", "rawpid": "1703.01161", "tags": ["cs.AI"], "title": "FeUdal Networks for Hierarchical Reinforcement Learning"}, {"abstract": "Generative adversarial networks (GANs) are a recently proposed class of\ngenerative models in which a generator is trained to optimize a cost function\nthat is being simultaneously learned by a discriminator. While the idea of\nlearning cost functions is relatively new to the field of generative modeling,\nlearning costs has long been studied in control and reinforcement learning (RL)\ndomains, typically for imitation learning from demonstrations. In these fields,\nlearning cost function underlying observed behavior is known as inverse\nreinforcement learning (IRL) or inverse optimal control. While at first the\nconnection between cost learning in RL and cost learning in generative modeling\nmay appear to be a superficial one, we show in this paper that certain IRL\nmethods are in fact mathematically equivalent to GANs. In particular, we\ndemonstrate an equivalence between a sample-based algorithm for maximum entropy\nIRL and a GAN in which the generator\u0027s density can be evaluated and is provided\nas an additional input to the discriminator. Interestingly, maximum entropy IRL\nis a special case of an energy-based model. We discuss the interpretation of\nGANs as an algorithm for training energy-based models, and relate this\ninterpretation to other recent work that seeks to connect GANs and EBMs. By\nformally highlighting the connection between GANs, IRL, and EBMs, we hope that\nresearchers in all three communities can better identify and apply transferable\nideas from one domain to another, particularly for developing more stable and\nscalable algorithms: a major challenge in all three domains.", "authors": ["Chelsea Finn", "Paul Christiano", "Pieter Abbeel", "Sergey Levine"], "category": "cs.LG", "comment": "NIPS 2016 Workshop on Adversarial Training. First two authors\n  contributed equally", "img": "/static/thumbs/1611.03852v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.03852v3", "num_discussion": 0, "originally_published_time": "11/11/2016", "pid": "1611.03852v3", "published_time": "11/25/2016", "rawpid": "1611.03852", "tags": ["cs.LG", "cs.AI"], "title": "A Connection between Generative Adversarial Networks, Inverse\n  Reinforcement Learning, and Energy-Based Models"}, {"abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an\nimplementation for executing such algorithms. A computation expressed using\nTensorFlow can be executed with little or no change on a wide variety of\nheterogeneous systems, ranging from mobile devices such as phones and tablets\nup to large-scale distributed systems of hundreds of machines and thousands of\ncomputational devices such as GPU cards. The system is flexible and can be used\nto express a wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been used for conducting\nresearch and for deploying machine learning systems into production across more\nthan a dozen areas of computer science and other fields, including speech\nrecognition, computer vision, robotics, information retrieval, natural language\nprocessing, geographic information extraction, and computational drug\ndiscovery. This paper describes the TensorFlow interface and an implementation\nof that interface that we have built at Google. The TensorFlow API and a\nreference implementation were released as an open-source package under the\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org.", "authors": ["Mart\u00edn Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Mane", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Viegas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "category": "cs.DC", "comment": "Version 2 updates only the metadata, to correct the formatting of\n  Mart\\\u0027in Abadi\u0027s name", "img": "/static/thumbs/1603.04467v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.04467v2", "num_discussion": 0, "originally_published_time": "3/14/2016", "pid": "1603.04467v2", "published_time": "3/16/2016", "rawpid": "1603.04467", "tags": ["cs.DC", "cs.LG"], "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed\n  Systems"}, {"abstract": "Deep artificial neural networks (DNNs) are typically trained via\ngradient-based learning algorithms, namely backpropagation. Evolution\nstrategies (ES) can rival backprop-based algorithms such as Q-learning and\npolicy gradients on challenging deep reinforcement learning (RL) problems.\nHowever, ES can be considered a gradient-based algorithm because it performs\nstochastic gradient descent via an operation similar to a finite-difference\napproximation of the gradient. That raises the question of whether\nnon-gradient-based evolutionary algorithms can work at DNN scales. Here we\ndemonstrate they can: we evolve the weights of a DNN with a simple,\ngradient-free, population-based genetic algorithm (GA) and it performs well on\nhard deep RL problems, including Atari and humanoid locomotion. The Deep GA\nsuccessfully evolves networks with over four million free parameters, the\nlargest neural networks ever evolved with a traditional evolutionary algorithm.\nThese results (1) expand our sense of the scale at which GAs can operate, (2)\nsuggest intriguingly that in some cases following the gradient is not the best\nchoice for optimizing performance, and (3) make immediately available the\nmultitude of neuroevolution techniques that improve performance. We demonstrate\nthe latter by showing that combining DNNs with novelty search, which encourages\nexploration on tasks with deceptive or sparse reward functions, can solve a\nhigh-dimensional problem on which reward-maximizing algorithms (e.g.\\ DQN, A3C,\nES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN\n(it can train Atari in ${\\raise.17ex\\hbox{$\\scriptstyle\\sim$}}$4 hours on one\ndesktop or ${\\raise.17ex\\hbox{$\\scriptstyle\\sim$}}$1 hour distributed on 720\ncores), and enables a state-of-the-art, up to 10,000-fold compact encoding\ntechnique.", "authors": ["Felipe Petroski Such", "Vashisht Madhavan", "Edoardo Conti", "Joel Lehman", "Kenneth O. Stanley", "Jeff Clune"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1712.06567v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1712.06567v3", "num_discussion": 0, "originally_published_time": "12/18/2017", "pid": "1712.06567v3", "published_time": "4/20/2018", "rawpid": "1712.06567", "tags": ["cs.NE", "cs.LG"], "title": "Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative\n  for Training Deep Neural Networks for Reinforcement Learning"}, {"abstract": "Recent research has revealed that the output of Deep Neural Networks (DNN)\ncan be easily altered by adding relatively small perturbations to the input\nvector. In this paper, we analyze an attack in an extremely limited scenario\nwhere only one pixel can be modified. For that we propose a novel method for\ngenerating one-pixel adversarial perturbations based on differential evolution.\nIt requires less adversarial information and can fool more types of networks.\nThe results show that 70.97% of the natural images can be perturbed to at least\none target class by modifying just one pixel with 97.47% confidence on average.\nThus, the proposed attack explores a different take on adversarial machine\nlearning in an extreme limited scenario, showing that current DNNs are also\nvulnerable to such low dimension attacks.", "authors": ["Jiawei Su", "Danilo Vasconcellos Vargas", "Sakurai Kouichi"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1710.08864v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1710.08864v4", "num_discussion": 0, "originally_published_time": "10/24/2017", "pid": "1710.08864v4", "published_time": "2/22/2018", "rawpid": "1710.08864", "tags": ["cs.LG", "cs.CV", "stat.ML"], "title": "One pixel attack for fooling deep neural networks"}, {"abstract": "For artificial general intelligence (AGI) it would be efficient if multiple\nusers trained the same giant neural network, permitting parameter reuse,\nwithout catastrophic forgetting. PathNet is a first step in this direction. It\nis a neural network algorithm that uses agents embedded in the neural network\nwhose task is to discover which parts of the network to re-use for new tasks.\nAgents are pathways (views) through the network which determine the subset of\nparameters that are used and updated by the forwards and backwards passes of\nthe backpropogation algorithm. During learning, a tournament selection genetic\nalgorithm is used to select pathways through the neural network for replication\nand mutation. Pathway fitness is the performance of that pathway measured\naccording to a cost function. We demonstrate successful transfer learning;\nfixing the parameters along a path learned on task A and re-evolving a new\npopulation of paths for task B, allows task B to be learned faster than it\ncould be learned from scratch or after fine-tuning. Paths evolved on task B\nre-use parts of the optimal path evolved on task A. Positive transfer was\ndemonstrated for binary MNIST, CIFAR, and SVHN supervised learning\nclassification tasks, and a set of Atari and Labyrinth reinforcement learning\ntasks, suggesting PathNets have general applicability for neural network\ntraining. Finally, PathNet also significantly improves the robustness to\nhyperparameter choices of a parallel asynchronous reinforcement learning\nalgorithm (A3C).", "authors": ["Chrisantha Fernando", "Dylan Banarse", "Charles Blundell", "Yori Zwols", "David Ha", "Andrei A. Rusu", "Alexander Pritzel", "Daan Wierstra"], "category": "cs.NE", "comment": "", "img": "/static/thumbs/1701.08734v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.08734v1", "num_discussion": 0, "originally_published_time": "1/30/2017", "pid": "1701.08734v1", "published_time": "1/30/2017", "rawpid": "1701.08734", "tags": ["cs.NE", "cs.LG"], "title": "PathNet: Evolution Channels Gradient Descent in Super Neural Networks"}, {"abstract": "The goal of this paper is to serve as a guide for selecting a detection\narchitecture that achieves the right speed/memory/accuracy balance for a given\napplication and platform. To this end, we investigate various ways to trade\naccuracy for speed and memory usage in modern convolutional object detection\nsystems. A number of successful systems have been proposed in recent years, but\napples-to-apples comparisons are difficult due to different base feature\nextractors (e.g., VGG, Residual Networks), different default image resolutions,\nas well as different hardware and software platforms. We present a unified\nimplementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016]\nand SSD [Liu et al., 2015] systems, which we view as \"meta-architectures\" and\ntrace out the speed/accuracy trade-off curve created by using alternative\nfeature extractors and varying other critical parameters such as image size\nwithin each of these meta-architectures. On one extreme end of this spectrum\nwhere speed and memory are critical, we present a detector that achieves real\ntime speeds and can be deployed on a mobile device. On the opposite end in\nwhich accuracy is critical, we present a detector that achieves\nstate-of-the-art performance measured on the COCO detection task.", "authors": ["Jonathan Huang", "Vivek Rathod", "Chen Sun", "Menglong Zhu", "Anoop Korattikara", "Alireza Fathi", "Ian Fischer", "Zbigniew Wojna", "Yang Song", "Sergio Guadarrama", "Kevin Murphy"], "category": "cs.CV", "comment": "Accepted to CVPR 2017", "img": "/static/thumbs/1611.10012v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.10012v3", "num_discussion": 0, "originally_published_time": "11/30/2016", "pid": "1611.10012v3", "published_time": "4/25/2017", "rawpid": "1611.10012", "tags": ["cs.CV"], "title": "Speed/accuracy trade-offs for modern convolutional object detectors"}, {"abstract": "Recurrent neural networks are a powerful tool for modeling sequential data,\nbut the dependence of each timestep\u0027s computation on the previous timestep\u0027s\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\nsequence modeling that alternates convolutional layers, which apply in parallel\nacross timesteps, and a minimalist recurrent pooling function that applies in\nparallel across channels. Despite lacking trainable recurrent layers, stacked\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\nsize. Due to their increased parallelism, they are up to 16 times faster at\ntrain and test time. Experiments on language modeling, sentiment\nclassification, and character-level neural machine translation demonstrate\nthese advantages and underline the viability of QRNNs as a basic building block\nfor a variety of sequence tasks.", "authors": ["James Bradbury", "Stephen Merity", "Caiming Xiong", "Richard Socher"], "category": "cs.NE", "comment": "Submitted to conference track at ICLR 2017", "img": "/static/thumbs/1611.01576v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.01576v2", "num_discussion": 0, "originally_published_time": "11/5/2016", "pid": "1611.01576v2", "published_time": "11/21/2016", "rawpid": "1611.01576", "tags": ["cs.NE", "cs.AI", "cs.CL", "cs.LG"], "title": "Quasi-Recurrent Neural Networks"}, {"abstract": "Most tasks in natural language processing can be cast into question answering\n(QA) problems over language input. We introduce the dynamic memory network\n(DMN), a neural network architecture which processes input sequences and\nquestions, forms episodic memories, and generates relevant answers. Questions\ntrigger an iterative attention process which allows the model to condition its\nattention on the inputs and the result of previous iterations. These results\nare then reasoned over in a hierarchical recurrent sequence model to generate\nanswers. The DMN can be trained end-to-end and obtains state-of-the-art results\non several types of tasks and datasets: question answering (Facebook\u0027s bAbI\ndataset), text classification for sentiment analysis (Stanford Sentiment\nTreebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The\ntraining for these different tasks relies exclusively on trained word vector\nrepresentations and input-question-answer triplets.", "authors": ["Ankit Kumar", "Ozan Irsoy", "Peter Ondruska", "Mohit Iyyer", "James Bradbury", "Ishaan Gulrajani", "Victor Zhong", "Romain Paulus", "Richard Socher"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1506.07285v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.07285v5", "num_discussion": 0, "originally_published_time": "6/24/2015", "pid": "1506.07285v5", "published_time": "3/5/2016", "rawpid": "1506.07285", "tags": ["cs.CL", "cs.LG", "cs.NE"], "title": "Ask Me Anything: Dynamic Memory Networks for Natural Language Processing"}, {"abstract": "We propose a deep convolutional neural network architecture codenamed\n\"Inception\", which was responsible for setting the new state of the art for\nclassification and detection in the ImageNet Large-Scale Visual Recognition\nChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the\nimproved utilization of the computing resources inside the network. This was\nachieved by a carefully crafted design that allows for increasing the depth and\nwidth of the network while keeping the computational budget constant. To\noptimize quality, the architectural decisions were based on the Hebbian\nprinciple and the intuition of multi-scale processing. One particular\nincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22\nlayers deep network, the quality of which is assessed in the context of\nclassification and detection.", "authors": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1409.4842v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1409.4842v1", "num_discussion": 0, "originally_published_time": "9/17/2014", "pid": "1409.4842v1", "published_time": "9/17/2014", "rawpid": "1409.4842", "tags": ["cs.CV"], "title": "Going Deeper with Convolutions"}, {"abstract": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN)\nwhich views the discriminator as an energy function that attributes low\nenergies to the regions near the data manifold and higher energies to other\nregions. Similar to the probabilistic GANs, a generator is seen as being\ntrained to produce contrastive samples with minimal energies, while the\ndiscriminator is trained to assign high energies to these generated samples.\nViewing the discriminator as an energy function allows to use a wide variety of\narchitectures and loss functionals in addition to the usual binary classifier\nwith logistic output. Among them, we show one instantiation of EBGAN framework\nas using an auto-encoder architecture, with the energy being the reconstruction\nerror, in place of the discriminator. We show that this form of EBGAN exhibits\nmore stable behavior than regular GANs during training. We also show that a\nsingle-scale architecture can be trained to generate high-resolution images.", "authors": ["Junbo Zhao", "Michael Mathieu", "Yann LeCun"], "category": "cs.LG", "comment": "Submitted to ICLR 2017", "img": "/static/thumbs/1609.03126v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1609.03126v4", "num_discussion": 0, "originally_published_time": "9/11/2016", "pid": "1609.03126v4", "published_time": "3/6/2017", "rawpid": "1609.03126", "tags": ["cs.LG", "stat.ML"], "title": "Energy-based Generative Adversarial Network"}, {"abstract": "Object detection is considered one of the most challenging problems in this\nfield of computer vision, as it involves the combination of object\nclassification and object localization within a scene. Recently, deep neural\nnetworks (DNNs) have been demonstrated to achieve superior object detection\nperformance compared to other approaches, with YOLOv2 (an improved You Only\nLook Once model) being one of the state-of-the-art in DNN-based object\ndetection methods in terms of both speed and accuracy. Although YOLOv2 can\nachieve real-time performance on a powerful GPU, it still remains very\nchallenging for leveraging this approach for real-time object detection in\nvideo on embedded computing devices with limited computational power and\nlimited memory. In this paper, we propose a new framework called Fast YOLO, a\nfast You Only Look Once framework which accelerates YOLOv2 to be able to\nperform object detection in video on embedded devices in a real-time manner.\nFirst, we leverage the evolutionary deep intelligence framework to evolve the\nYOLOv2 network architecture and produce an optimized architecture (referred to\nas O-YOLOv2 here) that has 2.8X fewer parameters with just a ~2% IOU drop. To\nfurther reduce power consumption on embedded devices while maintaining\nperformance, a motion-adaptive inference method is introduced into the proposed\nFast YOLO framework to reduce the frequency of deep inference with O-YOLOv2\nbased on temporal motion characteristics. Experimental results show that the\nproposed Fast YOLO framework can reduce the number of deep inferences by an\naverage of 38.13%, and an average speedup of ~3.3X for objection detection in\nvideo compared to the original YOLOv2, leading Fast YOLO to run an average of\n~18FPS on a Nvidia Jetson TX1 embedded system.", "authors": ["Mohammad Javad Shafiee", "Brendan Chywl", "Francis Li", "Alexander Wong"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1709.05943v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.05943v1", "num_discussion": 0, "originally_published_time": "9/18/2017", "pid": "1709.05943v1", "published_time": "9/18/2017", "rawpid": "1709.05943", "tags": ["cs.CV", "cs.AI", "cs.NE"], "title": "Fast YOLO: A Fast You Only Look Once System for Real-time Embedded\n  Object Detection in Video"}, {"abstract": "Unsupervised image-to-image translation aims at learning a joint distribution\nof images in different domains by using images from the marginal distributions\nin individual domains. Since there exists an infinite set of joint\ndistributions that can arrive the given marginal distributions, one could infer\nnothing about the joint distribution from the marginal distributions without\nadditional assumptions. To address the problem, we make a shared-latent space\nassumption and propose an unsupervised image-to-image translation framework\nbased on Coupled GANs. We compare the proposed framework with competing\napproaches and present high quality image translation results on various\nchallenging unsupervised image translation tasks, including street scene image\ntranslation, animal image translation, and face image translation. We also\napply the proposed framework to domain adaptation and achieve state-of-the-art\nperformance on benchmark datasets. Code and additional results are available in\nhttps://github.com/mingyuliutw/unit .", "authors": ["Ming-Yu Liu", "Thomas Breuel", "Jan Kautz"], "category": "cs.CV", "comment": "NIPS 2017, 11 pages, 6 figures", "img": "/static/thumbs/1703.00848v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.00848v6", "num_discussion": 0, "originally_published_time": "3/2/2017", "pid": "1703.00848v6", "published_time": "7/23/2018", "rawpid": "1703.00848", "tags": ["cs.CV", "cs.AI"], "title": "Unsupervised Image-to-Image Translation Networks"}, {"abstract": "Learning goal-directed behavior in environments with sparse feedback is a\nmajor challenge for reinforcement learning algorithms. The primary difficulty\narises due to insufficient exploration, resulting in an agent being unable to\nlearn robust value functions. Intrinsically motivated agents can explore new\nbehavior for its own sake rather than to directly solve problems. Such\nintrinsic behaviors could eventually help the agent solve tasks posed by the\nenvironment. We present hierarchical-DQN (h-DQN), a framework to integrate\nhierarchical value functions, operating at different temporal scales, with\nintrinsically motivated deep reinforcement learning. A top-level value function\nlearns a policy over intrinsic goals, and a lower-level function learns a\npolicy over atomic actions to satisfy the given goals. h-DQN allows for\nflexible goal specifications, such as functions over entities and relations.\nThis provides an efficient space for exploration in complicated environments.\nWe demonstrate the strength of our approach on two problems with very sparse,\ndelayed feedback: (1) a complex discrete stochastic decision process, and (2)\nthe classic ATARI game `Montezuma\u0027s Revenge\u0027.", "authors": ["Tejas D. Kulkarni", "Karthik R. Narasimhan", "Ardavan Saeedi", "Joshua B. Tenenbaum"], "category": "cs.LG", "comment": "14 pages, 7 figures", "img": "/static/thumbs/1604.06057v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1604.06057v2", "num_discussion": 0, "originally_published_time": "4/20/2016", "pid": "1604.06057v2", "published_time": "5/31/2016", "rawpid": "1604.06057", "tags": ["cs.LG", "cs.AI", "cs.CV", "cs.NE", "stat.ML"], "title": "Hierarchical Deep Reinforcement Learning: Integrating Temporal\n  Abstraction and Intrinsic Motivation"}, {"abstract": "Synthesizing high-quality images from text descriptions is a challenging\nproblem in computer vision and has many practical applications. Samples\ngenerated by existing text-to-image approaches can roughly reflect the meaning\nof the given descriptions, but they fail to contain necessary details and vivid\nobject parts. In this paper, we propose Stacked Generative Adversarial Networks\n(StackGAN) to generate 256x256 photo-realistic images conditioned on text\ndescriptions. We decompose the hard problem into more manageable sub-problems\nthrough a sketch-refinement process. The Stage-I GAN sketches the primitive\nshape and colors of the object based on the given text description, yielding\nStage-I low-resolution images. The Stage-II GAN takes Stage-I results and text\ndescriptions as inputs, and generates high-resolution images with\nphoto-realistic details. It is able to rectify defects in Stage-I results and\nadd compelling details with the refinement process. To improve the diversity of\nthe synthesized images and stabilize the training of the conditional-GAN, we\nintroduce a novel Conditioning Augmentation technique that encourages\nsmoothness in the latent conditioning manifold. Extensive experiments and\ncomparisons with state-of-the-arts on benchmark datasets demonstrate that the\nproposed method achieves significant improvements on generating photo-realistic\nimages conditioned on text descriptions.", "authors": ["Han Zhang", "Tao Xu", "Hongsheng Li", "Shaoting Zhang", "Xiaogang Wang", "Xiaolei Huang", "Dimitris Metaxas"], "category": "cs.CV", "comment": "ICCV 2017 Oral Presentation", "img": "/static/thumbs/1612.03242v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.03242v2", "num_discussion": 0, "originally_published_time": "12/10/2016", "pid": "1612.03242v2", "published_time": "8/5/2017", "rawpid": "1612.03242", "tags": ["cs.CV", "cs.AI", "stat.ML"], "title": "StackGAN: Text to Photo-realistic Image Synthesis with Stacked\n  Generative Adversarial Networks"}, {"abstract": "In this paper, we describe a phenomenon, which we named \"super-convergence\",\nwhere neural networks can be trained an order of magnitude faster than with\nstandard training methods. The existence of super-convergence is relevant to\nunderstanding why deep networks generalize well. One of the key elements of\nsuper-convergence is training with one learning rate cycle and a large maximum\nlearning rate. A primary insight that allows super-convergence training is that\nlarge learning rates regularize the training, hence requiring a reduction of\nall other forms of regularization in order to preserve an optimal\nregularization balance. We also derive a simplification of the Hessian Free\noptimization method to compute an estimate of the optimal learning rate.\nExperiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet\ndatasets, and resnet, wide-resnet, densenet, and inception architectures. In\naddition, we show that super-convergence provides a greater boost in\nperformance relative to standard training when the amount of labeled training\ndata is limited. The architectures and code to replicate the figures in this\npaper are available at github.com/lnsmith54/super-convergence. See\nhttp://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of\nsuper-convergence to win the DAWNBench challenge (see\nhttps://dawn.cs.stanford.edu/benchmark/).", "authors": ["Leslie N. Smith", "Nicholay Topin"], "category": "cs.LG", "comment": "This paper was significantly revised to show super-convergence as a\n  general fast training methodol...", "img": "/static/thumbs/1708.07120v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1708.07120v3", "num_discussion": 0, "originally_published_time": "8/23/2017", "pid": "1708.07120v3", "published_time": "5/17/2018", "rawpid": "1708.07120", "tags": ["cs.LG", "cs.CV", "cs.NE", "stat.ML"], "title": "Super-Convergence: Very Fast Training of Neural Networks Using Large\n  Learning Rates"}, {"abstract": "We introduce Imagination-Augmented Agents (I2As), a novel architecture for\ndeep reinforcement learning combining model-free and model-based aspects. In\ncontrast to most existing model-based reinforcement learning and planning\nmethods, which prescribe how a model should be used to arrive at a policy, I2As\nlearn to interpret predictions from a learned environment model to construct\nimplicit plans in arbitrary ways, by using the predictions as additional\ncontext in deep policy networks. I2As show improved data efficiency,\nperformance, and robustness to model misspecification compared to several\nbaselines.", "authors": ["Th\u00e9ophane Weber", "S\u00e9bastien Racani\u00e8re", "David P. Reichert", "Lars Buesing", "Arthur Guez", "Danilo Jimenez Rezende", "Adria Puigdom\u00e8nech Badia", "Oriol Vinyals", "Nicolas Heess", "Yujia Li", "Razvan Pascanu", "Peter Battaglia", "Demis Hassabis", "David Silver", "Daan Wierstra"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1707.06203v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.06203v2", "num_discussion": 0, "originally_published_time": "7/19/2017", "pid": "1707.06203v2", "published_time": "2/14/2018", "rawpid": "1707.06203", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "Imagination-Augmented Agents for Deep Reinforcement Learning"}, {"abstract": "In this work we explore a straightforward variational Bayes scheme for\nRecurrent Neural Networks. Firstly, we show that a simple adaptation of\ntruncated backpropagation through time can yield good quality uncertainty\nestimates and superior regularisation at only a small extra computational cost\nduring training, also reducing the amount of parameters by 80\\%. Secondly, we\ndemonstrate how a novel kind of posterior approximation yields further\nimprovements to the performance of Bayesian RNNs. We incorporate local gradient\ninformation into the approximate posterior to sharpen it around the current\nbatch statistics. We show how this technique is not exclusive to recurrent\nneural networks and can be applied more widely to train Bayesian neural\nnetworks. We also empirically demonstrate how Bayesian RNNs are superior to\ntraditional RNNs on a language modelling benchmark and an image captioning\ntask, as well as showing how each of these methods improve our model over a\nvariety of other schemes for training them. We also introduce a new benchmark\nfor studying uncertainty for language models so future methods can be easily\ncompared.", "authors": ["Meire Fortunato", "Charles Blundell", "Oriol Vinyals"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1704.02798v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.02798v3", "num_discussion": 0, "originally_published_time": "4/10/2017", "pid": "1704.02798v3", "published_time": "3/21/2018", "rawpid": "1704.02798", "tags": ["cs.LG", "stat.ML"], "title": "Bayesian Recurrent Neural Networks"}, {"abstract": "Variational Autoencoders (VAEs) are expressive latent variable models that\ncan be used to learn complex probability distributions from training data.\nHowever, the quality of the resulting model crucially relies on the\nexpressiveness of the inference model. We introduce Adversarial Variational\nBayes (AVB), a technique for training Variational Autoencoders with arbitrarily\nexpressive inference models. We achieve this by introducing an auxiliary\ndiscriminative network that allows to rephrase the maximum-likelihood-problem\nas a two-player game, hence establishing a principled connection between VAEs\nand Generative Adversarial Networks (GANs). We show that in the nonparametric\nlimit our method yields an exact maximum-likelihood assignment for the\nparameters of the generative model, as well as the exact posterior distribution\nover the latent variables given an observation. Contrary to competing\napproaches which combine VAEs with GANs, our approach has a clear theoretical\njustification, retains most advantages of standard Variational Autoencoders and\nis easy to implement.", "authors": ["Lars Mescheder", "Sebastian Nowozin", "Andreas Geiger"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1701.04722v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1701.04722v4", "num_discussion": 0, "originally_published_time": "1/17/2017", "pid": "1701.04722v4", "published_time": "6/11/2018", "rawpid": "1701.04722", "tags": ["cs.LG"], "title": "Adversarial Variational Bayes: Unifying Variational Autoencoders and\n  Generative Adversarial Networks"}, {"abstract": "Both generative adversarial networks (GAN) in unsupervised learning and\nactor-critic methods in reinforcement learning (RL) have gained a reputation\nfor being difficult to optimize. Practitioners in both fields have amassed a\nlarge number of strategies to mitigate these instabilities and improve\ntraining. Here we show that GANs can be viewed as actor-critic methods in an\nenvironment where the actor cannot affect the reward. We review the strategies\nfor stabilizing training for each class of models, both those that generalize\nbetween the two and those that are particular to that model. We also review a\nnumber of extensions to GANs and RL algorithms with even more complicated\ninformation flow. We hope that by highlighting this formal connection we will\nencourage both GAN and RL communities to develop general, scalable, and stable\nalgorithms for multilevel optimization with deep networks, and to draw\ninspiration across communities.", "authors": ["David Pfau", "Oriol Vinyals"], "category": "cs.LG", "comment": "Added comments on inverse reinforcement learning", "img": "/static/thumbs/1610.01945v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1610.01945v3", "num_discussion": 0, "originally_published_time": "10/6/2016", "pid": "1610.01945v3", "published_time": "1/18/2017", "rawpid": "1610.01945", "tags": ["cs.LG", "stat.ML"], "title": "Connecting Generative Adversarial Networks and Actor-Critic Methods"}, {"abstract": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural\nnetwork architecture for image generation. DRAW networks combine a novel\nspatial attention mechanism that mimics the foveation of the human eye, with a\nsequential variational auto-encoding framework that allows for the iterative\nconstruction of complex images. The system substantially improves on the state\nof the art for generative models on MNIST, and, when trained on the Street View\nHouse Numbers dataset, it generates images that cannot be distinguished from\nreal data with the naked eye.", "authors": ["Karol Gregor", "Ivo Danihelka", "Alex Graves", "Danilo Jimenez Rezende", "Daan Wierstra"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1502.04623v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1502.04623v2", "num_discussion": 0, "originally_published_time": "2/16/2015", "pid": "1502.04623v2", "published_time": "5/20/2015", "rawpid": "1502.04623", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "DRAW: A Recurrent Neural Network For Image Generation"}, {"abstract": "Effective training of neural networks requires much data. In the low-data\nregime, parameters are underdetermined, and learnt networks generalise poorly.\nData Augmentation alleviates this by using existing data more effectively.\nHowever standard data augmentation produces only limited plausible alternative\ndata. Given there is potential to generate a much broader set of augmentations,\nwe design and train a generative model to do data augmentation. The model,\nbased on image conditional Generative Adversarial Networks, takes data from a\nsource domain and learns to take any data item and generalise it to generate\nother within-class data items. As this generative process does not depend on\nthe classes themselves, it can be applied to novel unseen classes of data. We\nshow that a Data Augmentation Generative Adversarial Network (DAGAN) augments\nstandard vanilla classifiers well. We also show a DAGAN can enhance few-shot\nlearning systems such as Matching Networks. We demonstrate these approaches on\nOmniglot, on EMNIST having learnt the DAGAN on Omniglot, and VGG-Face data. In\nour experiments we can see over 13% increase in accuracy in the low-data regime\nexperiments in Omniglot (from 69% to 82%), EMNIST (73.9% to 76%) and VGG-Face\n(4.5% to 12%); in Matching Networks for Omniglot we observe an increase of 0.5%\n(from 96.9% to 97.4%) and an increase of 1.8% in EMNIST (from 59.5% to 61.3%).", "authors": ["Antreas Antoniou", "Amos Storkey", "Harrison Edwards"], "category": "stat.ML", "comment": "10 pages", "img": "/static/thumbs/1711.04340v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1711.04340v3", "num_discussion": 0, "originally_published_time": "11/12/2017", "pid": "1711.04340v3", "published_time": "3/21/2018", "rawpid": "1711.04340", "tags": ["stat.ML", "cs.CV", "cs.LG", "cs.NE"], "title": "Data Augmentation Generative Adversarial Networks"}, {"abstract": "Recent years have witnessed amazing progress in AI related fields such as\ncomputer vision, machine learning and autonomous vehicles. As with any rapidly\ngrowing field, however, it becomes increasingly difficult to stay up-to-date or\nenter the field as a beginner. While several topic specific survey papers have\nbeen written, to date no general survey on problems, datasets and methods in\ncomputer vision for autonomous vehicles exists. This paper attempts to narrow\nthis gap by providing a state-of-the-art survey on this topic. Our survey\nincludes both the historically most relevant literature as well as the current\nstate-of-the-art on several specific topics, including recognition,\nreconstruction, motion estimation, tracking, scene understanding and end-to-end\nlearning. Towards this goal, we first provide a taxonomy to classify each\napproach and then analyze the performance of the state-of-the-art on several\nchallenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes.\nBesides, we discuss open problems and current research challenges. To ease\naccessibility and accommodate missing references, we will also provide an\ninteractive platform which allows to navigate topics and methods, and provides\nadditional information and project links for each paper.", "authors": ["Joel Janai", "Fatma G\u00fcney", "Aseem Behl", "Andreas Geiger"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1704.05519v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1704.05519v1", "num_discussion": 0, "originally_published_time": "4/18/2017", "pid": "1704.05519v1", "published_time": "4/18/2017", "rawpid": "1704.05519", "tags": ["cs.CV", "cs.RO"], "title": "Computer Vision for Autonomous Vehicles: Problems, Datasets and\n  State-of-the-Art"}, {"abstract": "With the availability of large databases and recent improvements in deep\nlearning methodology, the performance of AI systems is reaching or even\nexceeding the human level on an increasing number of complex tasks. Impressive\nexamples of this development can be found in domains such as image\nclassification, sentiment analysis, speech understanding or strategic game\nplaying. However, because of their nested non-linear structure, these highly\nsuccessful machine learning and artificial intelligence models are usually\napplied in a black box manner, i.e., no information is provided about what\nexactly makes them arrive at their predictions. Since this lack of transparency\ncan be a major drawback, e.g., in medical applications, the development of\nmethods for visualizing, explaining and interpreting deep learning models has\nrecently attracted increasing attention. This paper summarizes recent\ndevelopments in this field and makes a plea for more interpretability in\nartificial intelligence. Furthermore, it presents two approaches to explaining\npredictions of deep learning models, one method which computes the sensitivity\nof the prediction with respect to changes in the input and one approach which\nmeaningfully decomposes the decision in terms of the input variables. These\nmethods are evaluated on three classification tasks.", "authors": ["Wojciech Samek", "Thomas Wiegand", "Klaus-Robert M\u00fcller"], "category": "cs.AI", "comment": "8 pages, 2 figures", "img": "/static/thumbs/1708.08296v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1708.08296v1", "num_discussion": 0, "originally_published_time": "8/28/2017", "pid": "1708.08296v1", "published_time": "8/28/2017", "rawpid": "1708.08296", "tags": ["cs.AI", "cs.CY", "cs.NE", "stat.ML"], "title": "Explainable Artificial Intelligence: Understanding, Visualizing and\n  Interpreting Deep Learning Models"}, {"abstract": "We introduce a design strategy for neural network macro-architecture based on\nself-similarity. Repeated application of a simple expansion rule generates deep\nnetworks whose structural layouts are precisely truncated fractals. These\nnetworks contain interacting subpaths of different lengths, but do not include\nany pass-through or residual connections; every internal signal is transformed\nby a filter and nonlinearity before being seen by subsequent layers. In\nexperiments, fractal networks match the excellent performance of standard\nresidual networks on both CIFAR and ImageNet classification tasks, thereby\ndemonstrating that residual representations may not be fundamental to the\nsuccess of extremely deep convolutional neural networks. Rather, the key may be\nthe ability to transition, during training, from effectively shallow to deep.\nWe note similarities with student-teacher behavior and develop drop-path, a\nnatural extension of dropout, to regularize co-adaptation of subpaths in\nfractal architectures. Such regularization allows extraction of\nhigh-performance fixed-depth subnetworks. Additionally, fractal networks\nexhibit an anytime property: shallow subnetworks provide a quick answer, while\ndeeper subnetworks, with higher latency, provide a more accurate answer.", "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "category": "cs.CV", "comment": "updated with ImageNet results; published as a conference paper at\n  ICLR 2017; project page at http:...", "img": "/static/thumbs/1605.07648v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1605.07648v4", "num_discussion": 0, "originally_published_time": "5/24/2016", "pid": "1605.07648v4", "published_time": "5/26/2017", "rawpid": "1605.07648", "tags": ["cs.CV"], "title": "FractalNet: Ultra-Deep Neural Networks without Residuals"}, {"abstract": "Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce \"deep compression\", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency.", "authors": ["Song Han", "Huizi Mao", "William J. Dally"], "category": "cs.CV", "comment": "Published as a conference paper at ICLR 2016 (oral)", "img": "/static/thumbs/1510.00149v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1510.00149v5", "num_discussion": 0, "originally_published_time": "10/1/2015", "pid": "1510.00149v5", "published_time": "2/15/2016", "rawpid": "1510.00149", "tags": ["cs.CV", "cs.NE"], "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained\n  Quantization and Huffman Coding"}, {"abstract": "We present an approach to automate the process of discovering optimization\nmethods, with a focus on deep learning architectures. We train a Recurrent\nNeural Network controller to generate a string in a domain specific language\nthat describes a mathematical update equation based on a list of primitive\nfunctions, such as the gradient, running average of the gradient, etc. The\ncontroller is trained with Reinforcement Learning to maximize the performance\nof a model after a few epochs. On CIFAR-10, our method discovers several update\nrules that are better than many commonly used optimizers, such as Adam,\nRMSProp, or SGD with and without Momentum on a ConvNet model. We introduce two\nnew optimizers, named PowerSign and AddSign, which we show transfer well and\nimprove training on a variety of different tasks and architectures, including\nImageNet classification and Google\u0027s neural machine translation system.", "authors": ["Irwan Bello", "Barret Zoph", "Vijay Vasudevan", "Quoc V. Le"], "category": "cs.AI", "comment": "ICML 2017 Conference paper", "img": "/static/thumbs/1709.07417v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.07417v2", "num_discussion": 0, "originally_published_time": "9/21/2017", "pid": "1709.07417v2", "published_time": "9/22/2017", "rawpid": "1709.07417", "tags": ["cs.AI", "cs.LG", "stat.ML"], "title": "Neural Optimizer Search with Reinforcement Learning"}, {"abstract": "With a goal of understanding what drives generalization in deep networks, we\nconsider several recently suggested explanations, including norm-based control,\nsharpness and robustness. We study how these measures can ensure\ngeneralization, highlighting the importance of scale normalization, and making\na connection between sharpness and PAC-Bayes theory. We then investigate how\nwell the measures explain different observed phenomena.", "authors": ["Behnam Neyshabur", "Srinadh Bhojanapalli", "David McAllester", "Nathan Srebro"], "category": "cs.LG", "comment": "19 pages, 8 figures", "img": "/static/thumbs/1706.08947v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.08947v2", "num_discussion": 0, "originally_published_time": "6/27/2017", "pid": "1706.08947v2", "published_time": "7/6/2017", "rawpid": "1706.08947", "tags": ["cs.LG"], "title": "Exploring Generalization in Deep Learning"}, {"abstract": "Attentional, RNN-based encoder-decoder models for abstractive summarization\nhave achieved good performance on short input and output sequences. For longer\ndocuments and summaries however these models often include repetitive and\nincoherent phrases. We introduce a neural network model with a novel\nintra-attention that attends over the input and continuously generated output\nseparately, and a new training method that combines standard supervised word\nprediction and reinforcement learning (RL). Models trained only with supervised\nlearning often exhibit \"exposure bias\" - they assume ground truth is provided\nat each step during training. However, when standard word prediction is\ncombined with the global sequence prediction training of RL the resulting\nsummaries become more readable. We evaluate this model on the CNN/Daily Mail\nand New York Times datasets. Our model obtains a 41.16 ROUGE-1 score on the\nCNN/Daily Mail dataset, an improvement over previous state-of-the-art models.\nHuman evaluation also shows that our model produces higher quality summaries.", "authors": ["Romain Paulus", "Caiming Xiong", "Richard Socher"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1705.04304v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.04304v3", "num_discussion": 0, "originally_published_time": "5/11/2017", "pid": "1705.04304v3", "published_time": "11/13/2017", "rawpid": "1705.04304", "tags": ["cs.CL"], "title": "A Deep Reinforced Model for Abstractive Summarization"}, {"abstract": "We explore the use of deep learning hierarchical models for problems in\nfinancial prediction and classification. Financial prediction problems -- such\nas those presented in designing and pricing securities, constructing\nportfolios, and risk management -- often involve large data sets with complex\ndata interactions that currently are difficult or impossible to specify in a\nfull economic model. Applying deep learning methods to these problems can\nproduce more useful results than standard methods in finance. In particular,\ndeep learning can detect and exploit interactions in the data that are, at\nleast currently, invisible to any existing financial economic theory.", "authors": ["J. B. Heaton", "N. G. Polson", "J. H. Witte"], "category": "cs.LG", "comment": "20 Pages, 5 Figures", "img": "/static/thumbs/1602.06561v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.06561v3", "num_discussion": 0, "originally_published_time": "2/21/2016", "pid": "1602.06561v3", "published_time": "1/14/2018", "rawpid": "1602.06561", "tags": ["cs.LG"], "title": "Deep Learning in Finance"}, {"abstract": "Deep learning tools have gained tremendous attention in applied machine\nlearning. However such tools for regression and classification do not capture\nmodel uncertainty. In comparison, Bayesian models offer a mathematically\ngrounded framework to reason about model uncertainty, but usually come with a\nprohibitive computational cost. In this paper we develop a new theoretical\nframework casting dropout training in deep neural networks (NNs) as approximate\nBayesian inference in deep Gaussian processes. A direct result of this theory\ngives us tools to model uncertainty with dropout NNs -- extracting information\nfrom existing models that has been thrown away so far. This mitigates the\nproblem of representing uncertainty in deep learning without sacrificing either\ncomputational complexity or test accuracy. We perform an extensive study of the\nproperties of dropout\u0027s uncertainty. Various network architectures and\nnon-linearities are assessed on tasks of regression and classification, using\nMNIST as an example. We show a considerable improvement in predictive\nlog-likelihood and RMSE compared to existing state-of-the-art methods, and\nfinish by using dropout\u0027s uncertainty in deep reinforcement learning.", "authors": ["Yarin Gal", "Zoubin Ghahramani"], "category": "stat.ML", "comment": "12 pages, 6 figures; fixed a mistake with standard error and added a\n  new table with updated result...", "img": "/static/thumbs/1506.02142v6.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1506.02142v6", "num_discussion": 0, "originally_published_time": "6/6/2015", "pid": "1506.02142v6", "published_time": "10/4/2016", "rawpid": "1506.02142", "tags": ["stat.ML", "cs.LG"], "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in\n  Deep Learning"}, {"abstract": "The Convolution Neural Network (CNN) has demonstrated the unique advantage in\naudio, image and text learning; recently it has also challenged Recurrent\nNeural Networks (RNNs) with long short-term memory cells (LSTM) in\nsequence-to-sequence learning, since the computations involved in CNN are\neasily parallelizable whereas those involved in RNN are mostly sequential,\nleading to a performance bottleneck. However, unlike RNN, the native CNN lacks\nthe history sensitivity required for sequence transformation; therefore\nenhancing the sequential order awareness, or position-sensitivity, becomes the\nkey to make CNN the general deep learning model. In this work we introduce an\nextended CNN model with strengthen position-sensitivity, called PoseNet. A\nnotable feature of PoseNet is the asymmetric treatment of position information\nin the encoder and the decoder. Experiments shows that PoseNet allows us to\nimprove the accuracy of CNN based sequence-to-sequence learning significantly,\nachieving around 33-36 BLEU scores on the WMT 2014 English-to-German\ntranslation task, and around 44-46 BLEU scores on the English-to-French\ntranslation task.", "authors": ["Qiming Chen", "Ren Wu"], "category": "cs.CL", "comment": "", "img": "/static/thumbs/1712.09662v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1712.09662v1", "num_discussion": 1, "originally_published_time": "12/27/2017", "pid": "1712.09662v1", "published_time": "12/27/2017", "rawpid": "1712.09662", "tags": ["cs.CL", "cs.LG", "cs.NE"], "title": "CNN Is All You Need"}, {"abstract": "For sophisticated reinforcement learning (RL) systems to interact usefully\nwith real-world environments, we need to communicate complex goals to these\nsystems. In this work, we explore goals defined in terms of (non-expert) human\npreferences between pairs of trajectory segments. We show that this approach\ncan effectively solve complex RL tasks without access to the reward function,\nincluding Atari games and simulated robot locomotion, while providing feedback\non less than one percent of our agent\u0027s interactions with the environment. This\nreduces the cost of human oversight far enough that it can be practically\napplied to state-of-the-art RL systems. To demonstrate the flexibility of our\napproach, we show that we can successfully train complex novel behaviors with\nabout an hour of human time. These behaviors and environments are considerably\nmore complex than any that have been previously learned from human feedback.", "authors": ["Paul Christiano", "Jan Leike", "Tom B. Brown", "Miljan Martic", "Shane Legg", "Dario Amodei"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1706.03741v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1706.03741v3", "num_discussion": 0, "originally_published_time": "6/12/2017", "pid": "1706.03741v3", "published_time": "7/13/2017", "rawpid": "1706.03741", "tags": ["stat.ML"], "title": "Deep reinforcement learning from human preferences"}, {"abstract": "We present a novel and practical deep fully convolutional neural network\narchitecture for semantic pixel-wise segmentation termed SegNet. This core\ntrainable segmentation engine consists of an encoder network, a corresponding\ndecoder network followed by a pixel-wise classification layer. The architecture\nof the encoder network is topologically identical to the 13 convolutional\nlayers in the VGG16 network. The role of the decoder network is to map the low\nresolution encoder feature maps to full input resolution feature maps for\npixel-wise classification. The novelty of SegNet lies is in the manner in which\nthe decoder upsamples its lower resolution input feature map(s). Specifically,\nthe decoder uses pooling indices computed in the max-pooling step of the\ncorresponding encoder to perform non-linear upsampling. This eliminates the\nneed for learning to upsample. The upsampled maps are sparse and are then\nconvolved with trainable filters to produce dense feature maps. We compare our\nproposed architecture with the widely adopted FCN and also with the well known\nDeepLab-LargeFOV, DeconvNet architectures. This comparison reveals the memory\nversus accuracy trade-off involved in achieving good segmentation performance.\n  SegNet was primarily motivated by scene understanding applications. Hence, it\nis designed to be efficient both in terms of memory and computational time\nduring inference. It is also significantly smaller in the number of trainable\nparameters than other competing architectures. We also performed a controlled\nbenchmark of SegNet and other architectures on both road scenes and SUN RGB-D\nindoor scene segmentation tasks. We show that SegNet provides good performance\nwith competitive inference time and more efficient inference memory-wise as\ncompared to other architectures. We also provide a Caffe implementation of\nSegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.", "authors": ["Vijay Badrinarayanan", "Alex Kendall", "Roberto Cipolla"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1511.00561v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.00561v3", "num_discussion": 0, "originally_published_time": "11/2/2015", "pid": "1511.00561v3", "published_time": "10/10/2016", "rawpid": "1511.00561", "tags": ["cs.CV", "cs.LG", "cs.NE"], "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image\n  Segmentation"}, {"abstract": "Several machine learning models, including neural networks, consistently\nmisclassify adversarial examples---inputs formed by applying small but\nintentionally worst-case perturbations to examples from the dataset, such that\nthe perturbed input results in the model outputting an incorrect answer with\nhigh confidence. Early attempts at explaining this phenomenon focused on\nnonlinearity and overfitting. We argue instead that the primary cause of neural\nnetworks\u0027 vulnerability to adversarial perturbation is their linear nature.\nThis explanation is supported by new quantitative results while giving the\nfirst explanation of the most intriguing fact about them: their generalization\nacross architectures and training sets. Moreover, this view yields a simple and\nfast method of generating adversarial examples. Using this approach to provide\nexamples for adversarial training, we reduce the test set error of a maxout\nnetwork on the MNIST dataset.", "authors": ["Ian J. Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "category": "stat.ML", "comment": "", "img": "/static/thumbs/1412.6572v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1412.6572v3", "num_discussion": 0, "originally_published_time": "12/20/2014", "pid": "1412.6572v3", "published_time": "3/20/2015", "rawpid": "1412.6572", "tags": ["stat.ML", "cs.LG"], "title": "Explaining and Harnessing Adversarial Examples"}, {"abstract": "Theory of mind (ToM; Premack \u0026 Woodruff, 1978) broadly refers to humans\u0027\nability to represent the mental states of others, including their desires,\nbeliefs, and intentions. We propose to train a machine to build such models\ntoo. We design a Theory of Mind neural network -- a ToMnet -- which uses\nmeta-learning to build models of the agents it encounters, from observations of\ntheir behaviour alone. Through this process, it acquires a strong prior model\nfor agents\u0027 behaviour, as well as the ability to bootstrap to richer\npredictions about agents\u0027 characteristics and mental states using only a small\nnumber of behavioural observations. We apply the ToMnet to agents behaving in\nsimple gridworld environments, showing that it learns to model random,\nalgorithmic, and deep reinforcement learning agents from varied populations,\nand that it passes classic ToM tasks such as the \"Sally-Anne\" test (Wimmer \u0026\nPerner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold\nfalse beliefs about the world. We argue that this system -- which autonomously\nlearns how to model other agents in its world -- is an important step forward\nfor developing multi-agent AI systems, for building intermediating technology\nfor machine-human interaction, and for advancing the progress on interpretable\nAI.", "authors": ["Neil C. Rabinowitz", "Frank Perbet", "H. Francis Song", "Chiyuan Zhang", "S. M. Ali Eslami", "Matthew Botvinick"], "category": "cs.AI", "comment": "21 pages, 15 figures", "img": "/static/thumbs/1802.07740v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.07740v2", "num_discussion": 0, "originally_published_time": "2/21/2018", "pid": "1802.07740v2", "published_time": "3/12/2018", "rawpid": "1802.07740", "tags": ["cs.AI"], "title": "Machine Theory of Mind"}, {"abstract": "Despite widespread adoption, machine learning models remain mostly black\nboxes. Understanding the reasons behind predictions is, however, quite\nimportant in assessing trust, which is fundamental if one plans to take action\nbased on a prediction, or when choosing whether to deploy a new model. Such\nunderstanding also provides insights into the model, which can be used to\ntransform an untrustworthy model or prediction into a trustworthy one. In this\nwork, we propose LIME, a novel explanation technique that explains the\npredictions of any classifier in an interpretable and faithful manner, by\nlearning an interpretable model locally around the prediction. We also propose\na method to explain models by presenting representative individual predictions\nand their explanations in a non-redundant way, framing the task as a submodular\noptimization problem. We demonstrate the flexibility of these methods by\nexplaining different models for text (e.g. random forests) and image\nclassification (e.g. neural networks). We show the utility of explanations via\nnovel experiments, both simulated and with human subjects, on various scenarios\nthat require trust: deciding if one should trust a prediction, choosing between\nmodels, improving an untrustworthy classifier, and identifying why a classifier\nshould not be trusted.", "authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1602.04938v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.04938v3", "num_discussion": 0, "originally_published_time": "2/16/2016", "pid": "1602.04938v3", "published_time": "8/9/2016", "rawpid": "1602.04938", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier"}, {"abstract": "We present some updates to YOLO! We made a bunch of little design changes to\nmake it better. We also trained this new network that\u0027s pretty swell. It\u0027s a\nlittle bigger than last time but more accurate. It\u0027s still fast though, don\u0027t\nworry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but\nthree times faster. When we look at the old .5 IOU mAP detection metric YOLOv3\nis quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5\nmAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always,\nall the code is online at https://pjreddie.com/yolo/", "authors": ["Joseph Redmon", "Ali Farhadi"], "category": "cs.CV", "comment": "Tech Report", "img": "/static/thumbs/1804.02767v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1804.02767v1", "num_discussion": 0, "originally_published_time": "4/8/2018", "pid": "1804.02767v1", "published_time": "4/8/2018", "rawpid": "1804.02767", "tags": ["cs.CV"], "title": "YOLOv3: An Incremental Improvement"}, {"abstract": "Learning to solve complex sequences of tasks--while both leveraging transfer\nand avoiding catastrophic forgetting--remains a key obstacle to achieving\nhuman-level intelligence. The progressive networks approach represents a step\nforward in this direction: they are immune to forgetting and can leverage prior\nknowledge via lateral connections to previously learned features. We evaluate\nthis architecture extensively on a wide variety of reinforcement learning tasks\n(Atari and 3D maze games), and show that it outperforms common baselines based\non pretraining and finetuning. Using a novel sensitivity measure, we\ndemonstrate that transfer occurs at both low-level sensory and high-level\ncontrol layers of the learned policy.", "authors": ["Andrei A. Rusu", "Neil C. Rabinowitz", "Guillaume Desjardins", "Hubert Soyer", "James Kirkpatrick", "Koray Kavukcuoglu", "Razvan Pascanu", "Raia Hadsell"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1606.04671v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.04671v3", "num_discussion": 0, "originally_published_time": "6/15/2016", "pid": "1606.04671v3", "published_time": "9/7/2016", "rawpid": "1606.04671", "tags": ["cs.LG"], "title": "Progressive Neural Networks"}, {"abstract": "Given a grayscale photograph as input, this paper attacks the problem of\nhallucinating a plausible color version of the photograph. This problem is\nclearly underconstrained, so previous approaches have either relied on\nsignificant user interaction or resulted in desaturated colorizations. We\npropose a fully automatic approach that produces vibrant and realistic\ncolorizations. We embrace the underlying uncertainty of the problem by posing\nit as a classification task and use class-rebalancing at training time to\nincrease the diversity of colors in the result. The system is implemented as a\nfeed-forward pass in a CNN at test time and is trained on over a million color\nimages. We evaluate our algorithm using a \"colorization Turing test,\" asking\nhuman participants to choose between a generated and ground truth color image.\nOur method successfully fools humans on 32% of the trials, significantly higher\nthan previous methods. Moreover, we show that colorization can be a powerful\npretext task for self-supervised feature learning, acting as a cross-channel\nencoder. This approach results in state-of-the-art performance on several\nfeature learning benchmarks.", "authors": ["Richard Zhang", "Phillip Isola", "Alexei A. Efros"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1603.08511v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.08511v5", "num_discussion": 0, "originally_published_time": "3/28/2016", "pid": "1603.08511v5", "published_time": "10/5/2016", "rawpid": "1603.08511", "tags": ["cs.CV"], "title": "Colorful Image Colorization"}, {"abstract": "We investigate a new method to augment recurrent neural networks with extra\nmemory without increasing the number of network parameters. The system has an\nassociative memory based on complex-valued vectors and is closely related to\nHolographic Reduced Representations and Long Short-Term Memory networks.\nHolographic Reduced Representations have limited capacity: as they store more\ninformation, each retrieval becomes noisier due to interference. Our system in\ncontrast creates redundant copies of stored information, which enables\nretrieval with reduced noise. Experiments demonstrate faster learning on\nmultiple memorization tasks.", "authors": ["Ivo Danihelka", "Greg Wayne", "Benigno Uria", "Nal Kalchbrenner", "Alex Graves"], "category": "cs.NE", "comment": "ICML-2016", "img": "/static/thumbs/1602.03032v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.03032v2", "num_discussion": 0, "originally_published_time": "2/9/2016", "pid": "1602.03032v2", "published_time": "5/19/2016", "rawpid": "1602.03032", "tags": ["cs.NE"], "title": "Associative Long Short-Term Memory"}, {"abstract": "Convolutional networks are at the core of most state-of-the-art computer\nvision solutions for a wide variety of tasks. Since 2014 very deep\nconvolutional networks started to become mainstream, yielding substantial gains\nin various benchmarks. Although increased model size and computational cost\ntend to translate to immediate quality gains for most tasks (as long as enough\nlabeled data is provided for training), computational efficiency and low\nparameter count are still enabling factors for various use cases such as mobile\nvision and big-data scenarios. Here we explore ways to scale up networks in\nways that aim at utilizing the added computation as efficiently as possible by\nsuitably factorized convolutions and aggressive regularization. We benchmark\nour methods on the ILSVRC 2012 classification challenge validation set\ndemonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%\ntop-5 error for single frame evaluation using a network with a computational\ncost of 5 billion multiply-adds per inference and with using less than 25\nmillion parameters. With an ensemble of 4 models and multi-crop evaluation, we\nreport 3.5% top-5 error on the validation set (3.6% error on the test set) and\n17.3% top-1 error on the validation set.", "authors": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1512.00567v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1512.00567v3", "num_discussion": 1, "originally_published_time": "12/2/2015", "pid": "1512.00567v3", "published_time": "12/11/2015", "rawpid": "1512.00567", "tags": ["cs.CV"], "title": "Rethinking the Inception Architecture for Computer Vision"}, {"abstract": "We introduce the dense captioning task, which requires a computer vision\nsystem to both localize and describe salient regions in images in natural\nlanguage. The dense captioning task generalizes object detection when the\ndescriptions consist of a single word, and Image Captioning when one predicted\nregion covers the full image. To address the localization and description task\njointly we propose a Fully Convolutional Localization Network (FCLN)\narchitecture that processes an image with a single, efficient forward pass,\nrequires no external regions proposals, and can be trained end-to-end with a\nsingle round of optimization. The architecture is composed of a Convolutional\nNetwork, a novel dense localization layer, and Recurrent Neural Network\nlanguage model that generates the label sequences. We evaluate our network on\nthe Visual Genome dataset, which comprises 94,000 images and 4,100,000\nregion-grounded captions. We observe both speed and accuracy improvements over\nbaselines based on current state of the art approaches in both generation and\nretrieval settings.", "authors": ["Justin Johnson", "Andrej Karpathy", "Li Fei-Fei"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1511.07571v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.07571v1", "num_discussion": 0, "originally_published_time": "11/24/2015", "pid": "1511.07571v1", "published_time": "11/24/2015", "rawpid": "1511.07571", "tags": ["cs.CV", "cs.LG"], "title": "DenseCap: Fully Convolutional Localization Networks for Dense Captioning"}, {"abstract": "Neural machine translation is a recently proposed approach to machine\ntranslation. Unlike the traditional statistical machine translation, the neural\nmachine translation aims at building a single neural network that can be\njointly tuned to maximize the translation performance. The models proposed\nrecently for neural machine translation often belong to a family of\nencoder-decoders and consists of an encoder that encodes a source sentence into\na fixed-length vector from which a decoder generates a translation. In this\npaper, we conjecture that the use of a fixed-length vector is a bottleneck in\nimproving the performance of this basic encoder-decoder architecture, and\npropose to extend this by allowing a model to automatically (soft-)search for\nparts of a source sentence that are relevant to predicting a target word,\nwithout having to form these parts as a hard segment explicitly. With this new\napproach, we achieve a translation performance comparable to the existing\nstate-of-the-art phrase-based system on the task of English-to-French\ntranslation. Furthermore, qualitative analysis reveals that the\n(soft-)alignments found by the model agree well with our intuition.", "authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "category": "cs.CL", "comment": "Accepted at ICLR 2015 as oral presentation", "img": "/static/thumbs/1409.0473v7.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1409.0473v7", "num_discussion": 0, "originally_published_time": "9/1/2014", "pid": "1409.0473v7", "published_time": "5/19/2016", "rawpid": "1409.0473", "tags": ["cs.CL", "cs.LG", "cs.NE", "stat.ML"], "title": "Neural Machine Translation by Jointly Learning to Align and Translate"}, {"abstract": "This work explores conditional image generation with a new image density\nmodel based on the PixelCNN architecture. The model can be conditioned on any\nvector, including descriptive labels or tags, or latent embeddings created by\nother networks. When conditioned on class labels from the ImageNet database,\nthe model is able to generate diverse, realistic scenes representing distinct\nanimals, objects, landscapes and structures. When conditioned on an embedding\nproduced by a convolutional network given a single image of an unseen face, it\ngenerates a variety of new portraits of the same person with different facial\nexpressions, poses and lighting conditions. We also show that conditional\nPixelCNN can serve as a powerful decoder in an image autoencoder. Additionally,\nthe gated convolutional layers in the proposed model improve the log-likelihood\nof PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet,\nwith greatly reduced computational cost.", "authors": ["Aaron van den Oord", "Nal Kalchbrenner", "Oriol Vinyals", "Lasse Espeholt", "Alex Graves", "Koray Kavukcuoglu"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1606.05328v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1606.05328v2", "num_discussion": 0, "originally_published_time": "6/16/2016", "pid": "1606.05328v2", "published_time": "6/18/2016", "rawpid": "1606.05328", "tags": ["cs.CV", "cs.LG"], "title": "Conditional Image Generation with PixelCNN Decoders"}, {"abstract": "This paper addresses the scalability challenge of architecture search by\nformulating the task in a differentiable manner. Unlike conventional approaches\nof applying evolution or reinforcement learning over a discrete and\nnon-differentiable search space, our method is based on the continuous\nrelaxation of the architecture representation, allowing efficient search of the\narchitecture using gradient descent. Extensive experiments on CIFAR-10,\nImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in\ndiscovering high-performance convolutional architectures for image\nclassification and recurrent architectures for language modeling, while being\norders of magnitude faster than state-of-the-art non-differentiable techniques.", "authors": ["Hanxiao Liu", "Karen Simonyan", "Yiming Yang"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1806.09055v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1806.09055v1", "num_discussion": 0, "originally_published_time": "6/24/2018", "pid": "1806.09055v1", "published_time": "6/24/2018", "rawpid": "1806.09055", "tags": ["cs.LG", "cs.CL", "cs.CV", "stat.ML"], "title": "DARTS: Differentiable Architecture Search"}, {"abstract": "Artificial intelligence (AI) has undergone a renaissance recently, making\nmajor progress in key domains such as vision, language, control, and\ndecision-making. This has been due, in part, to cheap data and cheap compute\nresources, which have fit the natural strengths of deep learning. However, many\ndefining characteristics of human intelligence, which developed under much\ndifferent pressures, remain out of reach for current approaches. In particular,\ngeneralizing beyond one\u0027s experiences--a hallmark of human intelligence from\ninfancy--remains a formidable challenge for modern AI.\n  The following is part position paper, part review, and part unification. We\nargue that combinatorial generalization must be a top priority for AI to\nachieve human-like abilities, and that structured representations and\ncomputations are key to realizing this objective. Just as biology uses nature\nand nurture cooperatively, we reject the false choice between\n\"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an\napproach which benefits from their complementary strengths. We explore how\nusing relational inductive biases within deep learning architectures can\nfacilitate learning about entities, relations, and rules for composing them. We\npresent a new building block for the AI toolkit with a strong relational\ninductive bias--the graph network--which generalizes and extends various\napproaches for neural networks that operate on graphs, and provides a\nstraightforward interface for manipulating structured knowledge and producing\nstructured behaviors. We discuss how graph networks can support relational\nreasoning and combinatorial generalization, laying the foundation for more\nsophisticated, interpretable, and flexible patterns of reasoning.", "authors": ["Peter W. Battaglia", "Jessica B. Hamrick", "Victor Bapst", "Alvaro Sanchez-Gonzalez", "Vinicius Zambaldi", "Mateusz Malinowski", "Andrea Tacchetti", "David Raposo", "Adam Santoro", "Ryan Faulkner", "Caglar Gulcehre", "Francis Song", "Andrew Ballard", "Justin Gilmer", "George Dahl", "Ashish Vaswani", "Kelsey Allen", "Charles Nash", "Victoria Langston", "Chris Dyer", "Nicolas Heess", "Daan Wierstra", "Pushmeet Kohli", "Matt Botvinick", "Oriol Vinyals", "Yujia Li", "Razvan Pascanu"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1806.01261v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1806.01261v2", "num_discussion": 1, "originally_published_time": "6/4/2018", "pid": "1806.01261v2", "published_time": "6/11/2018", "rawpid": "1806.01261", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "Relational inductive biases, deep learning, and graph networks"}, {"abstract": "We investigate omni-supervised learning, a special regime of semi-supervised\nlearning in which the learner exploits all available labeled data plus\ninternet-scale sources of unlabeled data. Omni-supervised learning is\nlower-bounded by performance on existing labeled datasets, offering the\npotential to surpass state-of-the-art fully supervised methods. To exploit the\nomni-supervised setting, we propose data distillation, a method that ensembles\npredictions from multiple transformations of unlabeled data, using a single\nmodel, to automatically generate new training annotations. We argue that visual\nrecognition models have recently become accurate enough that it is now possible\nto apply classic ideas about self-training to challenging real-world data. Our\nexperimental results show that in the cases of human keypoint detection and\ngeneral object detection, state-of-the-art models trained with data\ndistillation surpass the performance of using labeled data from the COCO\ndataset alone.", "authors": ["Ilija Radosavovic", "Piotr Doll\u00e1r", "Ross Girshick", "Georgia Gkioxari", "Kaiming He"], "category": "cs.CV", "comment": "tech report", "img": "/static/thumbs/1712.04440v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1712.04440v1", "num_discussion": 0, "originally_published_time": "12/12/2017", "pid": "1712.04440v1", "published_time": "12/12/2017", "rawpid": "1712.04440", "tags": ["cs.CV"], "title": "Data Distillation: Towards Omni-Supervised Learning"}, {"abstract": "In recent years there have been many successes of using deep representations\nin reinforcement learning. Still, many of these applications use conventional\narchitectures, such as convolutional networks, LSTMs, or auto-encoders. In this\npaper, we present a new neural network architecture for model-free\nreinforcement learning. Our dueling network represents two separate estimators:\none for the state value function and one for the state-dependent action\nadvantage function. The main benefit of this factoring is to generalize\nlearning across actions without imposing any change to the underlying\nreinforcement learning algorithm. Our results show that this architecture leads\nto better policy evaluation in the presence of many similar-valued actions.\nMoreover, the dueling architecture enables our RL agent to outperform the\nstate-of-the-art on the Atari 2600 domain.", "authors": ["Ziyu Wang", "Tom Schaul", "Matteo Hessel", "Hado van Hasselt", "Marc Lanctot", "Nando de Freitas"], "category": "cs.LG", "comment": "15 pages, 5 figures, and 5 tables", "img": "/static/thumbs/1511.06581v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1511.06581v3", "num_discussion": 0, "originally_published_time": "11/20/2015", "pid": "1511.06581v3", "published_time": "4/5/2016", "rawpid": "1511.06581", "tags": ["cs.LG"], "title": "Dueling Network Architectures for Deep Reinforcement Learning"}, {"abstract": "A neural network (NN) is a parameterised function that can be tuned via\ngradient descent to approximate a labelled collection of data with high\nprecision. A Gaussian process (GP), on the other hand, is a probabilistic model\nthat defines a distribution over possible functions, and is updated in light of\ndata via the rules of probabilistic inference. GPs are probabilistic,\ndata-efficient and flexible, however they are also computationally intensive\nand thus limited in their applicability. We introduce a class of neural latent\nvariable models which we call Neural Processes (NPs), combining the best of\nboth worlds. Like GPs, NPs define distributions over functions, are capable of\nrapid adaptation to new observations, and can estimate the uncertainty in their\npredictions. Like NNs, NPs are computationally efficient during training and\nevaluation but also learn to adapt their priors to data. We demonstrate the\nperformance of NPs on a range of learning tasks, including regression and\noptimisation, and compare and contrast with related models in the literature.", "authors": ["Marta Garnelo", "Jonathan Schwarz", "Dan Rosenbaum", "Fabio Viola", "Danilo J. Rezende", "S. M. Ali Eslami", "Yee Whye Teh"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1807.01622v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1807.01622v1", "num_discussion": 0, "originally_published_time": "7/4/2018", "pid": "1807.01622v1", "published_time": "7/4/2018", "rawpid": "1807.01622", "tags": ["cs.LG", "stat.ML"], "title": "Neural Processes"}, {"abstract": "We propose Efficient Neural Architecture Search (ENAS), a fast and\ninexpensive approach for automatic model design. In ENAS, a controller learns\nto discover neural network architectures by searching for an optimal subgraph\nwithin a large computational graph. The controller is trained with policy\ngradient to select a subgraph that maximizes the expected reward on the\nvalidation set. Meanwhile the model corresponding to the selected subgraph is\ntrained to minimize a canonical cross entropy loss. Thanks to parameter sharing\nbetween child models, ENAS is fast: it delivers strong empirical performances\nusing much fewer GPU-hours than all existing automatic model design approaches,\nand notably, 1000x less expensive than standard Neural Architecture Search. On\nthe Penn Treebank dataset, ENAS discovers a novel architecture that achieves a\ntest perplexity of 55.8, establishing a new state-of-the-art among all methods\nwithout post-training processing. On the CIFAR-10 dataset, ENAS designs novel\narchitectures that achieve a test error of 2.89%, which is on par with NASNet\n(Zoph et al., 2018), whose test error is 2.65%.", "authors": ["Hieu Pham", "Melody Y. Guan", "Barret Zoph", "Quoc V. Le", "Jeff Dean"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1802.03268v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1802.03268v2", "num_discussion": 0, "originally_published_time": "2/9/2018", "pid": "1802.03268v2", "published_time": "2/12/2018", "rawpid": "1802.03268", "tags": ["cs.LG", "cs.CL", "cs.CV", "cs.NE", "stat.ML"], "title": "Efficient Neural Architecture Search via Parameter Sharing"}, {"abstract": "In this paper, we propose a novel generative model named Stacked Generative\nAdversarial Networks (SGAN), which is trained to invert the hierarchical\nrepresentations of a bottom-up discriminative network. Our model consists of a\ntop-down stack of GANs, each learned to generate lower-level representations\nconditioned on higher-level representations. A representation discriminator is\nintroduced at each feature hierarchy to encourage the representation manifold\nof the generator to align with that of the bottom-up discriminative network,\nleveraging the powerful discriminative representations to guide the generative\nmodel. In addition, we introduce a conditional loss that encourages the use of\nconditional information from the layer above, and a novel entropy loss that\nmaximizes a variational lower bound on the conditional entropy of generator\noutputs. We first train each stack independently, and then train the whole\nmodel end-to-end. Unlike the original GAN that uses a single noise vector to\nrepresent all the variations, our SGAN decomposes variations into multiple\nlevels and gradually resolves uncertainties in the top-down generative process.\nBased on visual inspection, Inception scores and visual Turing test, we\ndemonstrate that SGAN is able to generate images of much higher quality than\nGANs without stacking.", "authors": ["Xun Huang", "Yixuan Li", "Omid Poursaeed", "John Hopcroft", "Serge Belongie"], "category": "cs.CV", "comment": "CVPR 2017, camera-ready version", "img": "/static/thumbs/1612.04357v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1612.04357v4", "num_discussion": 0, "originally_published_time": "12/13/2016", "pid": "1612.04357v4", "published_time": "4/12/2017", "rawpid": "1612.04357", "tags": ["cs.CV", "cs.LG", "cs.NE", "stat.ML"], "title": "Stacked Generative Adversarial Networks"}, {"abstract": "We present a simple, highly modularized network architecture for image\nclassification. Our network is constructed by repeating a building block that\naggregates a set of transformations with the same topology. Our simple design\nresults in a homogeneous, multi-branch architecture that has only a few\nhyper-parameters to set. This strategy exposes a new dimension, which we call\n\"cardinality\" (the size of the set of transformations), as an essential factor\nin addition to the dimensions of depth and width. On the ImageNet-1K dataset,\nwe empirically show that even under the restricted condition of maintaining\ncomplexity, increasing cardinality is able to improve classification accuracy.\nMoreover, increasing cardinality is more effective than going deeper or wider\nwhen we increase the capacity. Our models, named ResNeXt, are the foundations\nof our entry to the ILSVRC 2016 classification task in which we secured 2nd\nplace. We further investigate ResNeXt on an ImageNet-5K set and the COCO\ndetection set, also showing better results than its ResNet counterpart. The\ncode and models are publicly available online.", "authors": ["Saining Xie", "Ross Girshick", "Piotr Doll\u00e1r", "Zhuowen Tu", "Kaiming He"], "category": "cs.CV", "comment": "Accepted to CVPR 2017. Code and models:\n  https://github.com/facebookresearch/ResNeXt", "img": "/static/thumbs/1611.05431v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1611.05431v2", "num_discussion": 0, "originally_published_time": "11/16/2016", "pid": "1611.05431v2", "published_time": "4/11/2017", "rawpid": "1611.05431", "tags": ["cs.CV"], "title": "Aggregated Residual Transformations for Deep Neural Networks"}, {"abstract": "We introduce Equilibrium Propagation, a learning framework for energy-based\nmodels. It involves only one kind of neural computation, performed in both the\nfirst phase (when the prediction is made) and the second phase of training\n(after the target or prediction error is revealed). Although this algorithm\ncomputes the gradient of an objective function just like Backpropagation, it\ndoes not need a special computation or circuit for the second phase, where\nerrors are implicitly propagated. Equilibrium Propagation shares similarities\nwith Contrastive Hebbian Learning and Contrastive Divergence while solving the\ntheoretical issues of both algorithms: our algorithm computes the gradient of a\nwell defined objective function. Because the objective function is defined in\nterms of local perturbations, the second phase of Equilibrium Propagation\ncorresponds to only nudging the prediction (fixed point, or stationary\ndistribution) towards a configuration that reduces prediction error. In the\ncase of a recurrent multi-layer supervised network, the output units are\nslightly nudged towards their target in the second phase, and the perturbation\nintroduced at the output layer propagates backward in the hidden layers. We\nshow that the signal \u0027back-propagated\u0027 during this second phase corresponds to\nthe propagation of error derivatives and encodes the gradient of the objective\nfunction, when the synaptic update corresponds to a standard form of\nspike-timing dependent plasticity. This work makes it more plausible that a\nmechanism similar to Backpropagation could be implemented by brains, since\nleaky integrator neural computation performs both inference and error\nback-propagation in our model. The only local difference between the two phases\nis whether synaptic changes are allowed or not.", "authors": ["Benjamin Scellier", "Yoshua Bengio"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1602.05179v5.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1602.05179v5", "num_discussion": 0, "originally_published_time": "2/16/2016", "pid": "1602.05179v5", "published_time": "3/28/2017", "rawpid": "1602.05179", "tags": ["cs.LG"], "title": "Equilibrium Propagation: Bridging the Gap Between Energy-Based Models\n  and Backpropagation"}, {"abstract": "The framework of reinforcement learning or optimal control provides a\nmathematical formalization of intelligent decision making that is powerful and\nbroadly applicable. While the general form of the reinforcement learning\nproblem enables effective reasoning about uncertainty, the connection between\nreinforcement learning and inference in probabilistic models is not immediately\nobvious. However, such a connection has considerable value when it comes to\nalgorithm design: formalizing a problem as probabilistic inference in principle\nallows us to bring to bear a wide array of approximate inference tools, extend\nthe model in flexible and powerful ways, and reason about compositionality and\npartial observability. In this article, we will discuss how a generalization of\nthe reinforcement learning or optimal control problem, which is sometimes\ntermed maximum entropy reinforcement learning, is equivalent to exact\nprobabilistic inference in the case of deterministic dynamics, and variational\ninference in the case of stochastic dynamics. We will present a detailed\nderivation of this framework, overview prior work that has drawn on this and\nrelated ideas to propose new reinforcement learning and control algorithms, and\ndescribe perspectives on future research.", "authors": ["Sergey Levine"], "category": "cs.LG", "comment": "", "img": "/static/thumbs/1805.00909v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1805.00909v3", "num_discussion": 0, "originally_published_time": "5/2/2018", "pid": "1805.00909v3", "published_time": "5/20/2018", "rawpid": "1805.00909", "tags": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "title": "Reinforcement Learning and Control as Probabilistic Inference: Tutorial\n  and Review"}, {"abstract": "This monograph aims at providing an introduction to key concepts, algorithms,\nand theoretical results in machine learning. The treatment concentrates on\nprobabilistic models for supervised and unsupervised learning problems. It\nintroduces fundamental concepts and algorithms by building on first principles,\nwhile also exposing the reader to more advanced topics with extensive pointers\nto the literature, within a unified notation and mathematical framework. The\nmaterial is organized according to clearly defined categories, such as\ndiscriminative and generative models, frequentist and Bayesian approaches,\nexact and approximate inference, as well as directed and undirected models.\nThis monograph is meant as an entry point for researchers with a background in\nprobability and linear algebra.", "authors": ["Osvaldo Simeone"], "category": "cs.LG", "comment": "This is an expanded and improved version of the original posting.\n  Feedback is welcome", "img": "/static/thumbs/1709.02840v3.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.02840v3", "num_discussion": 0, "originally_published_time": "9/8/2017", "pid": "1709.02840v3", "published_time": "5/17/2018", "rawpid": "1709.02840", "tags": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "title": "A Brief Introduction to Machine Learning for Engineers"}, {"abstract": "In this paper we argue for the fundamental importance of the value\ndistribution: the distribution of the random return received by a reinforcement\nlearning agent. This is in contrast to the common approach to reinforcement\nlearning which models the expectation of this return, or value. Although there\nis an established body of literature studying the value distribution, thus far\nit has always been used for a specific purpose such as implementing risk-aware\nbehaviour. We begin with theoretical results in both the policy evaluation and\ncontrol settings, exposing a significant distributional instability in the\nlatter. We then use the distributional perspective to design a new algorithm\nwhich applies Bellman\u0027s equation to the learning of approximate value\ndistributions. We evaluate our algorithm using the suite of games from the\nArcade Learning Environment. We obtain both state-of-the-art results and\nanecdotal evidence demonstrating the importance of the value distribution in\napproximate reinforcement learning. Finally, we combine theoretical and\nempirical evidence to highlight the ways in which the value distribution\nimpacts learning in the approximate setting.", "authors": ["Marc G. Bellemare", "Will Dabney", "R\u00e9mi Munos"], "category": "cs.LG", "comment": "ICML 2017", "img": "/static/thumbs/1707.06887v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.06887v1", "num_discussion": 0, "originally_published_time": "7/21/2017", "pid": "1707.06887v1", "published_time": "7/21/2017", "rawpid": "1707.06887", "tags": ["cs.LG", "cs.AI", "stat.ML"], "title": "A Distributional Perspective on Reinforcement Learning"}, {"abstract": "Despite recent advances, memory-augmented deep neural networks are still\nlimited when it comes to life-long and one-shot learning, especially in\nremembering rare events. We present a large-scale life-long memory module for\nuse in deep learning. The module exploits fast nearest-neighbor algorithms for\nefficiency and thus scales to large memory sizes. Except for the\nnearest-neighbor query, the module is fully differentiable and trained\nend-to-end with no extra supervision. It operates in a life-long manner, i.e.,\nwithout the need to reset it during training.\n  Our memory module can be easily added to any part of a supervised neural\nnetwork. To show its versatility we add it to a number of networks, from simple\nconvolutional ones tested on image classification to deep sequence-to-sequence\nand recurrent-convolutional models. In all cases, the enhanced network gains\nthe ability to remember and do life-long one-shot learning. Our module\nremembers training examples shown many thousands of steps in the past and it\ncan successfully generalize from them. We set new state-of-the-art for one-shot\nlearning on the Omniglot dataset and demonstrate, for the first time, life-long\none-shot learning in recurrent neural networks on a large-scale machine\ntranslation task.", "authors": ["\u0141ukasz Kaiser", "Ofir Nachum", "Aurko Roy", "Samy Bengio"], "category": "cs.LG", "comment": "Conference paper accepted for ICLR\u002717", "img": "/static/thumbs/1703.03129v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1703.03129v1", "num_discussion": 0, "originally_published_time": "3/9/2017", "pid": "1703.03129v1", "published_time": "3/9/2017", "rawpid": "1703.03129", "tags": ["cs.LG"], "title": "Learning to Remember Rare Events"}, {"abstract": "Most existing machine learning classifiers are highly vulnerable to\nadversarial examples. An adversarial example is a sample of input data which\nhas been modified very slightly in a way that is intended to cause a machine\nlearning classifier to misclassify it. In many cases, these modifications can\nbe so subtle that a human observer does not even notice the modification at\nall, yet the classifier still makes a mistake. Adversarial examples pose\nsecurity concerns because they could be used to perform an attack on machine\nlearning systems, even if the adversary has no access to the underlying model.\nUp to now, all previous work have assumed a threat model in which the adversary\ncan feed data directly into the machine learning classifier. This is not always\nthe case for systems operating in the physical world, for example those which\nare using signals from cameras and other sensors as an input. This paper shows\nthat even in such physical world scenarios, machine learning systems are\nvulnerable to adversarial examples. We demonstrate this by feeding adversarial\nimages obtained from cell-phone camera to an ImageNet Inception classifier and\nmeasuring the classification accuracy of the system. We find that a large\nfraction of adversarial examples are classified incorrectly even when perceived\nthrough the camera.", "authors": ["Alexey Kurakin", "Ian Goodfellow", "Samy Bengio"], "category": "cs.CV", "comment": "14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk", "img": "/static/thumbs/1607.02533v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1607.02533v4", "num_discussion": 0, "originally_published_time": "7/8/2016", "pid": "1607.02533v4", "published_time": "2/11/2017", "rawpid": "1607.02533", "tags": ["cs.CV", "cs.CR", "cs.LG", "stat.ML"], "title": "Adversarial examples in the physical world"}, {"abstract": "We consider image transformation problems, where an input image is\ntransformed into an output image. Recent methods for such problems typically\ntrain feed-forward convolutional neural networks using a \\emph{per-pixel} loss\nbetween the output and ground-truth images. Parallel work has shown that\nhigh-quality images can be generated by defining and optimizing\n\\emph{perceptual} loss functions based on high-level features extracted from\npretrained networks. We combine the benefits of both approaches, and propose\nthe use of perceptual loss functions for training feed-forward networks for\nimage transformation tasks. We show results on image style transfer, where a\nfeed-forward network is trained to solve the optimization problem proposed by\nGatys et al in real-time. Compared to the optimization-based method, our\nnetwork gives similar qualitative results but is three orders of magnitude\nfaster. We also experiment with single-image super-resolution, where replacing\na per-pixel loss with a perceptual loss gives visually pleasing results.", "authors": ["Justin Johnson", "Alexandre Alahi", "Li Fei-Fei"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1603.08155v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.08155v1", "num_discussion": 0, "originally_published_time": "3/27/2016", "pid": "1603.08155v1", "published_time": "3/27/2016", "rawpid": "1603.08155", "tags": ["cs.CV", "cs.LG"], "title": "Perceptual Losses for Real-Time Style Transfer and Super-Resolution"}, {"abstract": "Many real-world applications can be described as large-scale games of\nimperfect information. To deal with these challenging domains, prior work has\nfocused on computing Nash equilibria in a handcrafted abstraction of the\ndomain. In this paper we introduce the first scalable end-to-end approach to\nlearning approximate Nash equilibria without prior domain knowledge. Our method\ncombines fictitious self-play with deep reinforcement learning. When applied to\nLeduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium,\nwhereas common reinforcement learning methods diverged. In Limit Texas Holdem,\na poker game of real-world scale, NFSP learnt a strategy that approached the\nperformance of state-of-the-art, superhuman algorithms based on significant\ndomain expertise.", "authors": ["Johannes Heinrich", "David Silver"], "category": "cs.LG", "comment": "updated version, incorporating conference feedback", "img": "/static/thumbs/1603.01121v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1603.01121v2", "num_discussion": 0, "originally_published_time": "3/3/2016", "pid": "1603.01121v2", "published_time": "6/28/2016", "rawpid": "1603.01121", "tags": ["cs.LG", "cs.AI", "cs.GT"], "title": "Deep Reinforcement Learning from Self-Play in Imperfect-Information\n  Games"}, {"abstract": "Developing neural network image classification models often requires\nsignificant architecture engineering. In this paper, we study a method to learn\nthe model architectures directly on the dataset of interest. As this approach\nis expensive when the dataset is large, we propose to search for an\narchitectural building block on a small dataset and then transfer the block to\na larger dataset. The key contribution of this work is the design of a new\nsearch space (the \"NASNet search space\") which enables transferability. In our\nexperiments, we search for the best convolutional layer (or \"cell\") on the\nCIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking\ntogether more copies of this cell, each with their own parameters to design a\nconvolutional architecture, named \"NASNet architecture\". We also introduce a\nnew regularization technique called ScheduledDropPath that significantly\nimproves generalization in the NASNet models. On CIFAR-10 itself, NASNet\nachieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet\nachieves, among the published works, state-of-the-art accuracy of 82.7% top-1\nand 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than\nthe best human-invented architectures while having 9 billion fewer FLOPS - a\nreduction of 28% in computational demand from the previous state-of-the-art\nmodel. When evaluated at different levels of computational cost, accuracies of\nNASNets exceed those of the state-of-the-art human-designed models. For\ninstance, a small version of NASNet also achieves 74% top-1 accuracy, which is\n3.1% better than equivalently-sized, state-of-the-art models for mobile\nplatforms. Finally, the learned features by NASNet used with the Faster-RCNN\nframework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO\ndataset.", "authors": ["Barret Zoph", "Vijay Vasudevan", "Jonathon Shlens", "Quoc V. Le"], "category": "cs.CV", "comment": "", "img": "/static/thumbs/1707.07012v4.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1707.07012v4", "num_discussion": 0, "originally_published_time": "7/21/2017", "pid": "1707.07012v4", "published_time": "4/11/2018", "rawpid": "1707.07012", "tags": ["cs.CV", "cs.LG", "stat.ML"], "title": "Learning Transferable Architectures for Scalable Image Recognition"}, {"abstract": "In recent years, significant progress has been made in solving challenging\nproblems across various domains using deep reinforcement learning (RL).\nReproducing existing work and accurately judging the improvements offered by\nnovel methods is vital to sustaining this progress. Unfortunately, reproducing\nresults for state-of-the-art deep RL methods is seldom straightforward. In\nparticular, non-determinism in standard benchmark environments, combined with\nvariance intrinsic to the methods, can make reported results tough to\ninterpret. Without significance metrics and tighter standardization of\nexperimental reporting, it is difficult to determine whether improvements over\nthe prior state-of-the-art are meaningful. In this paper, we investigate\nchallenges posed by reproducibility, proper experimental techniques, and\nreporting procedures. We illustrate the variability in reported metrics and\nresults when comparing against common baselines and suggest guidelines to make\nfuture results in deep RL more reproducible. We aim to spur discussion about\nhow to ensure continued progress in the field by minimizing wasted effort\nstemming from results that are non-reproducible and easily misinterpreted.", "authors": ["Peter Henderson", "Riashat Islam", "Philip Bachman", "Joelle Pineau", "Doina Precup", "David Meger"], "category": "cs.LG", "comment": "Accepted to the Thirthy-Second AAAI Conference On Artificial\n  Intelligence (AAAI), 2018", "img": "/static/thumbs/1709.06560v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1709.06560v2", "num_discussion": 0, "originally_published_time": "9/19/2017", "pid": "1709.06560v2", "published_time": "11/24/2017", "rawpid": "1709.06560", "tags": ["cs.LG", "stat.ML"], "title": "Deep Reinforcement Learning that Matters"}, {"abstract": "Generative Adversarial Networks (GANs) have gathered a lot of attention from\nthe computer vision community, yielding impressive results for image\ngeneration. Advances in the adversarial generation of natural language from\nnoise however are not commensurate with the progress made in generating images,\nand still lag far behind likelihood based methods. In this paper, we take a\nstep towards generating natural language with a GAN objective alone. We\nintroduce a simple baseline that addresses the discrete output space problem\nwithout relying on gradient estimators and show that it is able to achieve\nstate-of-the-art results on a Chinese poem generation dataset. We present\nquantitative results on generating sentences from context-free and\nprobabilistic context-free grammars, and qualitative language modeling results.\nA conditional version is also described that can generate sequences conditioned\non sentence characteristics.", "authors": ["Sai Rajeswar", "Sandeep Subramanian", "Francis Dutil", "Christopher Pal", "Aaron Courville"], "category": "cs.CL", "comment": "11 pages, 3 figures, 5 tables", "img": "/static/thumbs/1705.10929v1.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1705.10929v1", "num_discussion": 0, "originally_published_time": "5/31/2017", "pid": "1705.10929v1", "published_time": "5/31/2017", "rawpid": "1705.10929", "tags": ["cs.CL", "cs.AI", "cs.NE", "stat.ML"], "title": "Adversarial Generation of Natural Language"}, {"abstract": "A long-standing obstacle to progress in deep learning is the problem of\nvanishing and exploding gradients. Although, the problem has largely been\novercome via carefully constructed initializations and batch normalization,\narchitectures incorporating skip-connections such as highway and resnets\nperform much better than standard feedforward architectures despite well-chosen\ninitialization and batch normalization. In this paper, we identify the\nshattered gradients problem. Specifically, we show that the correlation between\ngradients in standard feedforward networks decays exponentially with depth\nresulting in gradients that resemble white noise whereas, in contrast, the\ngradients in architectures with skip-connections are far more resistant to\nshattering, decaying sublinearly. Detailed empirical evidence is presented in\nsupport of the analysis, on both fully-connected networks and convnets.\nFinally, we present a new \"looks linear\" (LL) initialization that prevents\nshattering, with preliminary experiments showing the new initialization allows\nto train very deep networks without the addition of skip-connections.", "authors": ["David Balduzzi", "Marcus Frean", "Lennox Leary", "JP Lewis", "Kurt Wan-Duo Ma", "Brian McWilliams"], "category": "cs.NE", "comment": "ICML 2017, final version", "img": "/static/thumbs/1702.08591v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.08591v2", "num_discussion": 2, "originally_published_time": "2/28/2017", "pid": "1702.08591v2", "published_time": "6/6/2018", "rawpid": "1702.08591", "tags": ["cs.NE", "cs.LG", "stat.ML"], "title": "The Shattered Gradients Problem: If resnets are the answer, then what is\n  the question?"}, {"abstract": "We present Deep Voice, a production-quality text-to-speech system constructed\nentirely from deep neural networks. Deep Voice lays the groundwork for truly\nend-to-end neural speech synthesis. The system comprises five major building\nblocks: a segmentation model for locating phoneme boundaries, a\ngrapheme-to-phoneme conversion model, a phoneme duration prediction model, a\nfundamental frequency prediction model, and an audio synthesis model. For the\nsegmentation model, we propose a novel way of performing phoneme boundary\ndetection with deep neural networks using connectionist temporal classification\n(CTC) loss. For the audio synthesis model, we implement a variant of WaveNet\nthat requires fewer parameters and trains faster than the original. By using a\nneural network for each component, our system is simpler and more flexible than\ntraditional text-to-speech systems, where each component requires laborious\nfeature engineering and extensive domain expertise. Finally, we show that\ninference with our system can be performed faster than real time and describe\noptimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x\nspeedups over existing implementations.", "authors": ["Sercan O. Arik", "Mike Chrzanowski", "Adam Coates", "Gregory Diamos", "Andrew Gibiansky", "Yongguo Kang", "Xian Li", "John Miller", "Andrew Ng", "Jonathan Raiman", "Shubho Sengupta", "Mohammad Shoeybi"], "category": "cs.CL", "comment": "Submitted to ICML 2017", "img": "/static/thumbs/1702.07825v2.pdf.jpg", "in_library": 0, "link": "http://arxiv.org/abs/1702.07825v2", "num_discussion": 0, "originally_published_time": "2/25/2017", "pid": "1702.07825v2", "published_time": "3/7/2017", "rawpid": "1702.07825", "tags": ["cs.CL", "cs.LG", "cs.NE", "cs.SD"], "title": "Deep Voice: Real-time Neural Text-to-Speech"}]